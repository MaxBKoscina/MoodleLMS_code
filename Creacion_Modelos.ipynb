{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCTs9mhd8EgA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Monta el Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ASmQW8mv8GMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Se carga los dataset de los semestre"
      ],
      "metadata": {
        "id": "rpULmhyJaNLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_1 = pd.read_csv('/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano -  Generación y Análisis de Secuencias de Actividad/logs/df_semestre_full_2023.csv')\n",
        "df_2024_2 = pd.read_csv('/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano -  Generación y Análisis de Secuencias de Actividad/logs/df_semestre_full_2024_S2.csv')\n",
        "df_2024_1 = pd.read_csv('/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano -  Generación y Análisis de Secuencias de Actividad/logs/df_semestre_full_2024_S1.csv')"
      ],
      "metadata": {
        "id": "m0WOX6e58lBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_1"
      ],
      "metadata": {
        "id": "DYdiItqr9qxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2024_2"
      ],
      "metadata": {
        "id": "GtIULSQp9tqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2024_1.columns"
      ],
      "metadata": {
        "id": "TFb6YW3p90E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n🔎 Valores nulos por columna:\")\n",
        "print(df_2023_1.isna().sum())\n"
      ],
      "metadata": {
        "id": "IK_PqW1IJqs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2024_1.dtypes"
      ],
      "metadata": {
        "id": "HprpXc4w-KJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2024_2.dtypes"
      ],
      "metadata": {
        "id": "C0M6NftoQ2z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_1.columns"
      ],
      "metadata": {
        "id": "OwH6UtgQ-OsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PeARdJq02Spc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_2023_1.info())"
      ],
      "metadata": {
        "id": "-h-P46ly3RSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_2024_1.info())"
      ],
      "metadata": {
        "id": "7J6mOPz93T4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_2024_2.info())"
      ],
      "metadata": {
        "id": "gMXJwLfH3VVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ajfcNKtc2Svs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#df_2023_1['Semestre'] = '2023-S1'\n",
        "#df_2024_1['Semestre'] = '2024-S1'\n",
        "#df_2024_2['Semestre'] = '2024-S2'\n",
        "\n",
        "# Paso 2: Crear una lista con los dataframes a unir\n",
        "lista_dataframes = [df_2023_1, df_2024_1, df_2024_2]\n",
        "\n",
        "# Paso 3: Concatenar los dataframes\n",
        "df_consolidado = pd.concat(lista_dataframes, ignore_index=True)\n",
        "\n",
        "# Paso 4: Verificar el resultado\n",
        "print(\"Dimensiones del dataframe consolidado (filas, columnas):\")\n",
        "print(df_consolidado.shape)\n",
        "\n",
        "print(\"\\nPrimeras filas del dataframe consolidado:\")\n",
        "print(df_consolidado.head())\n",
        "\n",
        "print(\"\\nÚltimas filas del dataframe consolidado (para ver si se unieron bien los diferentes Semestre):\")\n",
        "print(df_consolidado.tail())\n",
        "\n",
        "print(\"\\nConteo de valores en la columna 'Semestre' para verificar la unión:\")\n",
        "print(df_consolidado['Semestre'].value_counts())\n",
        "\n",
        "print(\"\\nInformación general del dataframe consolidado (para revisar tipos de datos y no nulos):\")\n",
        "df_consolidado.info()"
      ],
      "metadata": {
        "id": "lIkbF-nx40c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el DataFrame en un archivo CSV\n",
        "df_consolidado.to_csv('/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano -  Generación y Análisis de Secuencias de Actividad/logs/df_consolidado_full.csv', index=False)\n",
        "\n",
        "print(\"Archivo CSV guardado correctamente.\")"
      ],
      "metadata": {
        "id": "eKB4gRiYfnW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_consolidado = pd.read_csv('/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano -  Generación y Análisis de Secuencias de Actividad/logs/df_consolidado_full.csv')"
      ],
      "metadata": {
        "id": "rjrkSFd24EIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columnas_a_mantener = [\n",
        "    # --- Features calculadas de Logs ---\n",
        "    'max_days_with_access',         # Índice 7\n",
        "    'max_days_without_access',      # Índice 8\n",
        "    'first_last_log_diff',          # Índice 9\n",
        "    'logs',                         # Índice 10 (Total logs calculados para ese estudiante-semestre)\n",
        "    'week_logs',                    # Índice 11 (¿Es el total semanal o el de la última semana? Asegúrate que represente el estado final)\n",
        "    'daily_avg',                    # Índice 12 (Promedio diario calculado para el periodo)\n",
        "    'weekly_avg',                   # Índice 13 (Promedio semanal calculado para el periodo)\n",
        "    'days_with_logs',               # Índice 14 (Total días con logs)\n",
        "    'days_with_logs_avg',           # Índice 15 (Promedio días con logs)\n",
        "    'days_with_logs_week',          # Índice 16 (¿Días con logs en la última semana?)\n",
        "    'days_with_logs_avg_week',      # Índice 17 (¿Promedio días con logs en la última semana?)\n",
        "    'activity_total',               # Índice 19 (Total logs categoría 'activity')\n",
        "    'content_total',                # Índice 20 (Total logs categoría 'content')\n",
        "    'other_total',                  # Índice 21 (Total logs categoría 'other')\n",
        "    'report_total',\n",
        "    'system_total',\n",
        "    'activity_week',                # Índice 22 (¿Logs 'activity' última semana?)\n",
        "    'content_week',                 # Índice 23 (¿Logs 'content' última semana?)\n",
        "    'other_week',                   # Índice 24 (¿Logs 'other' última semana?)\n",
        "    'report_week',                  # Índice 25 (¿Logs 'report' última semana?)\n",
        "    'system_week',                  # Índice 26 (¿Logs 'system' última semana?)\n",
        "    'total_weeks',                  # Índice 27 (Duración total del curso, si es constante por semestre)\n",
        "    'course_progress',              # Índice 28 (Progreso en el punto de corte)\n",
        "    'longest_streak',   # Índice 29 (Racha más larga de acceso/actividad)\n",
        "\n",
        "\n",
        "    # --- Tu indicador binario (Feature) ---\n",
        "    'aprobando',                    # Índice 32 (La variable 0/1 que creaste, la renombraremos luego)\n",
        "\n",
        "    # --- Notas Individuales (Features) ---\n",
        "    'Control 1',                    # Índice 34\n",
        "    'Control 2',                    # Índice 35\n",
        "    'Sumativa 1',                   # Índice 36\n",
        "\n",
        "    # --- Variable Objetivo (Target) ---\n",
        "    'aprobo_semestre_real'          # Índice 33\n",
        "]"
      ],
      "metadata": {
        "id": "1bMNn_qV9HtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Se deja un dataframe con una fila unica por alumnos y su semestre para creacion de modelos"
      ],
      "metadata": {
        "id": "Qt3h5LmZamoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_agregado = df_consolidado.groupby(['Nombre completo del usuario', 'Semestre'])[columnas_a_mantener].last().reset_index()\n"
      ],
      "metadata": {
        "id": "tnlMyRvrC16o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el DataFrame en un archivo CSV\n",
        "df_agregado.to_csv('/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano -  Generación y Análisis de Secuencias de Actividad/logs/df_united_final.csv', index=False)\n",
        "\n",
        "print(\"Archivo CSV guardado correctamente.\")"
      ],
      "metadata": {
        "id": "auSomcX2fGfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agregado"
      ],
      "metadata": {
        "id": "nutvr8tvDbMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agregado.columns\n"
      ],
      "metadata": {
        "id": "cE33kxZ4F_JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_log_cols = [\n",
        "    'max_days_with_access', 'max_days_without_access', 'first_last_log_diff',\n",
        "    'logs', 'week_logs', 'daily_avg', 'weekly_avg', 'days_with_logs',\n",
        "    'days_with_logs_avg', 'days_with_logs_week', 'days_with_logs_avg_week',\n",
        "    'activity_total', 'content_total', 'other_total','report_total',\n",
        "    'system_total', 'activity_week',\n",
        "    'content_week', 'other_week', 'report_week', 'system_week',\n",
        "    'total_weeks', 'course_progress', 'longest_streak'\n",
        "\n",
        "]\n",
        "target_col = 'aprobo_semestre_real'\n"
      ],
      "metadata": {
        "id": "OpoGNAHxEt5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la matriz de correlación de Pearson\n",
        "correlation_matrix = df_agregado[features_log_cols].corr(method='pearson')\n",
        "\n",
        "# Ver la correlación de 'y' con todas las demás variables\n",
        "# Define X and y here if they haven't been defined yet\n",
        "X = df_agregado[features_log_cols]\n",
        "y = df_agregado[target_col]\n",
        "\n",
        "# Calculate correlation of each feature in X with y\n",
        "correlations_with_y = X.corrwith(y, method='pearson')\n",
        "\n",
        "print(correlations_with_y)"
      ],
      "metadata": {
        "id": "z_kIjSzdcyss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_agregado[features_log_cols]\n",
        "y = df_agregado[target_col]\n"
      ],
      "metadata": {
        "id": "4gxZn8CvDc8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlations_with_y = X.corrwith(y, method='pearson') # Puedes cambiar 'pearson' por 'spearman' o 'kendall'\n",
        "\n",
        "# Mostrar las correlaciones\n",
        "print(\"Correlación de cada característica en X con la variable y:\")\n",
        "print(correlations_with_y)"
      ],
      "metadata": {
        "id": "Q1LX8Pvodl62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ver la diferencia de aprobados vs reprobados"
      ],
      "metadata": {
        "id": "l9Z0G5YWa0V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_agregado['aprobo_semestre_real'].value_counts())\n",
        "print(df_agregado['aprobo_semestre_real'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "tzb6ezEgF_yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Forma de X (features):\", X.shape)\n",
        "print(\"Forma de y (target):\", y.shape)\n",
        "print(\"\\nPrimeras filas de X:\")\n",
        "print(X.head())"
      ],
      "metadata": {
        "id": "13mUw3zIFY76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separacions de datos"
      ],
      "metadata": {
        "id": "OxjnG30va5bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,  # Puedes ajustar entre 0.2 y 0.3\n",
        "                                                    stratify=y,      # Buena práctica mantener la distribución\n",
        "                                                    random_state=42) # Para que la división sea siempre la misma\n",
        "\n",
        "print(\"\\nForma de X_train:\", X_train.shape)\n",
        "print(\"Forma de X_test:\", X_test.shape)\n",
        "print(\"Forma de y_train:\", y_train.shape)\n",
        "print(\"Forma de y_test:\", y_test.shape)\n",
        "\n",
        "# Verificar distribución en y_train (debería ser similar a la original ~54/46)\n",
        "print(\"\\nDistribución del target en Entrenamiento (y_train):\")\n",
        "print(y_train.value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "eD_PNfo3I2a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Paso 2: Crear una instancia del StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Paso 3:\n",
        "# Ajustar el scaler SÓLO con los datos de entrenamiento (X_train)\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Usar el scaler YA AJUSTADO con X_train para transformar X_test.\n",
        "# ¡Es muy importante NO volver a hacer .fit() con X_test!\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Verificar las dimensiones (deberían ser las mismas que X_train y X_test originales)\n",
        "print(\"\\nDimensiones de X_train_scaled:\", X_train_scaled.shape)\n",
        "print(\"Dimensiones de X_test_scaled:\", X_test_scaled.shape)\n",
        "\n",
        "# Opcional: Ver cómo lucen algunas filas de los datos escalados\n",
        "# (Serán arrays de NumPy, no DataFrames de Pandas directamente, a menos que los conviertas)\n",
        "print(\"\\nPrimeras 5 filas de X_train_scaled:\")\n",
        "print(X_train_scaled[:5])"
      ],
      "metadata": {
        "id": "AUu1ok2YSlbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NO VA SMOTE - SE HACE POR CADA POR EL TEMA DE PRACTICA"
      ],
      "metadata": {
        "id": "iDxNRX1jRt0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------"
      ],
      "metadata": {
        "id": "ygceT0qfRzH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "idwryon4ovRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "KC0Og1iOR0-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metodo del codo"
      ],
      "metadata": {
        "id": "uEqAkDJsR2M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Rango de valores de k a probar\n",
        "k_range = range(1, 16)\n",
        "k_scores_f1 = []\n",
        "\n",
        "# Validación cruzada estratificada\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"🔁 Buscando mejor valor de k para KNN...\")\n",
        "\n",
        "for k_value in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k_value)\n",
        "    scores = cross_val_score(knn, X_train_scaled, y_train, cv=cv, scoring='f1_macro', n_jobs=-1)\n",
        "    k_scores_f1.append(scores.mean())\n",
        "    print(f\"  k={k_value}: F1 Macro promedio = {scores.mean():.4f}\")\n",
        "\n",
        "# Gráfico del codo\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, k_scores_f1, marker='o', linestyle='-')\n",
        "plt.title(\"Método del Codo: KNN (post-SMOTE) vs F1 Macro\")\n",
        "plt.xlabel(\"Valor de k (n_neighbors)\")\n",
        "plt.ylabel(\"F1 Macro Promedio (CV)\")\n",
        "plt.grid(True)\n",
        "plt.xticks(k_range)\n",
        "plt.show()\n",
        "\n",
        "# Mejor k encontrado\n",
        "best_k_index = np.argmax(k_scores_f1)\n",
        "best_k = list(k_range)[best_k_index]\n",
        "print(f\"\\n🎯 Mejor k según F1 Macro: {best_k}\")\n"
      ],
      "metadata": {
        "id": "ANFzZUWCloIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Diagnóstico: Verificar el tamaño de la clase minoritaria en y_train_fit ---\n",
        "print(\"\\n🔎 Verificando distribución de clases en y_train_fit:\")\n",
        "# print(y_train_res.value_counts()) # Original line that caused the error\n",
        "print(np.bincount(y_train.astype(int))) # Use np.bincount for NumPy arrays\n",
        "\n",
        "# Get the counts of each class from the bincount result\n",
        "counts = np.bincount(y_train.astype(int))\n",
        "\n",
        "# Find the minimum count among the classes\n",
        "minority_class_size = counts.min()\n",
        "\n",
        "print(f\"Tamaño de la clase minoritaria en y_train_fit: {minority_class_size}\")"
      ],
      "metadata": {
        "id": "tGjsvQw8BpoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creacion KNN"
      ],
      "metadata": {
        "id": "um0uYke4R4qB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-genetic-opt"
      ],
      "metadata": {
        "id": "NxwqlZ2vTO3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "1ljiAGGK9GLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "oPTUNA"
      ],
      "metadata": {
        "id": "AWLkTz36SOBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import recall_score, make_scorer # Necesitas make_scorer\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline # Es bueno ser explícito con ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "\n",
        "# --- Scorer Personalizado para Recall de Clase 0.0 ---\n",
        "# Es crucial definir que quieres el recall para la clase '0'\n",
        "scorer_recall_clase_0 = make_scorer(recall_score, pos_label=0, average='binary', zero_division=0)\n",
        "\n",
        "def objective_knn(trial):\n",
        "    # Hiperparámetros para KNN\n",
        "    n_neighbors = trial.suggest_int(\"knn_n_neighbors\", 3, 16, step=2) # k impares suelen ser preferidos\n",
        "    weights = trial.suggest_categorical(\"knn_weights\", [\"uniform\", \"distance\"])\n",
        "    metric = trial.suggest_categorical(\"knn_metric\", [\"euclidean\", \"manhattan\", \"minkowski\"])\n",
        "\n",
        "    p_val = 2 # Valor por defecto para p si no es minkowski o para minkowski con p=2\n",
        "    if metric == 'minkowski':\n",
        "        p_val = trial.suggest_categorical(\"knn_p\", [1, 2]) # p=1 (manhattan), p=2 (euclidean)\n",
        "\n",
        "    # Hiperparámetro para SMOTE\n",
        "    smote_k = trial.suggest_int(\"smote_k_neighbors\", 3, 7) # Renombré para consistencia con el pipeline\n",
        "\n",
        "    # Crear el Pipeline\n",
        "    # Es importante instanciar SMOTE y KNN dentro de la función objective\n",
        "    # para que cada trial tenga su propia configuración con los HPs sugeridos.\n",
        "    pipeline = ImbPipeline([\n",
        "        ('smote', SMOTE(k_neighbors=smote_k, random_state=42)),\n",
        "        ('knn', KNeighborsClassifier(\n",
        "            n_neighbors=n_neighbors,\n",
        "            weights=weights,\n",
        "            metric=metric,\n",
        "            p=p_val, # Añadir p aquí\n",
        "            n_jobs=-1 # Usar todos los procesadores para KNN\n",
        "            ))\n",
        "    ])\n",
        "\n",
        "    # Validación cruzada estratificada\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=) # Usar trial.number para diferente shuffle por trial\n",
        "                                                                            # o un random_state fijo si prefieres\n",
        "\n",
        "    # Obtener el score usando cross_val_score\n",
        "    # n_jobs=1 para cross_val_score para evitar problemas de paralelismo anidado,\n",
        "    # especialmente si Optuna ya paraleliza trials o si n_jobs=-1 en KNN causa problemas.\n",
        "    try:\n",
        "        scores = cross_val_score(pipeline, X_train_scaled, y_train.astype(int), # Asegurar y_train como int\n",
        "                                 scoring=scorer_recall_clase_0, # Usar el scorer personalizado\n",
        "                                 cv=cv,\n",
        "                                 n_jobs=1)\n",
        "        score_mean = np.mean(scores)\n",
        "    except ValueError as e_cv:\n",
        "        # Esto puede pasar si SMOTE falla en un fold (ej. k_neighbors > n_muestras_clase_minoritaria_en_fold)\n",
        "        print(f\"  Trial {trial.number} con params {trial.params}: Error en cross_val_score ({e_cv}). Score = 0.0\")\n",
        "        return 0.0 # Devolver un score muy malo para que Optuna no elija estos parámetros\n",
        "\n",
        "    # (Opcional) Pruning: Si Optuna detecta que un trial no es prometedor, puede detenerlo.\n",
        "    # Si tuvieras un bucle de folds manual, podrías usar trial.report() y trial.should_prune()\n",
        "    # Con cross_val_score, el pruning es menos directo de implementar aquí.\n",
        "\n",
        "    return score_mean\n",
        "\n",
        "# --- Crear y Ejecutar el Estudio Optuna ---\n",
        "# Asegúrate de que X_train_scaled e y_train estén definidos y preparados antes de esta línea\n",
        "\n",
        "study_knn = optuna.create_study(direction=\"maximize\", study_name=\"KNN_SMOTE_Optuna_Recall0\")\n",
        "\n",
        "    # n_trials controla cuántas combinaciones diferentes de hiperparámetros probar\n",
        "    # Empieza con un número menor (ej. 30-50) para ver si funciona y luego puedes aumentarlo.\n",
        "study_knn.optimize(objective_knn, n_trials=100, timeout=1800) # Ejemplo: 50 trials o 30 minutos\n",
        "\n",
        "print(\"\\n✅ Búsqueda de hiperparámetros para KNN completada.\")\n",
        "print(\"🎯 Mejores hiperparámetros KNN encontrados:\")\n",
        "print(study_knn.best_trial.params) # Muestra los nombres que usaste en trial.suggest_\n",
        "print(f\"📈 Mejor recall clase 0 (promedio CV): {study_knn.best_value:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CJqYFnTL9Ddf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "\n",
        "# --- Scorer Personalizado para Recall de Clase 0.0 ---\n",
        "# Es crucial definir que quieres el recall para la clase '0'\n",
        "scorer_recall_clase_0 = make_scorer(recall_score, pos_label=0, average='binary', zero_division=0)\n",
        "\n",
        "def objective_knn(trial):\n",
        "\n",
        "    n_neighbors = trial.suggest_int(\"n_neighbors\", 3, 16, step=2)\n",
        "\n",
        "    weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
        "\n",
        "    metric = trial.suggest_categorical(\"metric\", [\"euclidean\", \"manhattan\", \"minkowski\"])\n",
        "\n",
        "    smote_k = trial.suggest_int(\"smote_k\", 3, 7)\n",
        "\n",
        "\n",
        "\n",
        "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, metric=metric)\n",
        "\n",
        "    pipeline = ImbPipeline([('smote', smote), ('knn', knn)])\n",
        "\n",
        "\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    scores = cross_val_score(pipeline, X_train_scaled, y_train, scoring=scorer_recall_clase_0, cv=cv)\n",
        "\n",
        "\n",
        "\n",
        "    return scores.mean()\n",
        "\n",
        "\n",
        "\n",
        "study_knn = optuna.create_study(direction=\"maximize\", study_name=\"KNN_SMOTE_Optuna\")\n",
        "\n",
        "study_knn.optimize(objective_knn, n_trials=100)\n",
        "\n",
        "\n",
        "print(f\"✅ Mejor recall promedio: {study_knn.best_value:.4f}\")"
      ],
      "metadata": {
        "id": "7AkoKn9bHoIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 📌 Extraer mejores hiperparámetros\n",
        "best_params = study_knn.best_params\n",
        "print(\"⚙ Usando hiperparámetros:\", best_params)\n",
        "\n",
        "# 🔁 Aplicar SMOTE y entrenar el modelo\n",
        "smote_final = SMOTE(k_neighbors=best_params[\"smote_k\"], random_state=42)\n",
        "knn_final = KNeighborsClassifier(\n",
        "    n_neighbors=best_params[\"n_neighbors\"],\n",
        "    weights=best_params[\"weights\"],\n",
        "    metric=best_params[\"metric\"]\n",
        ")\n",
        "\n",
        "pipeline_knn_final = Pipeline([(\"smote\", smote_final), (\"knn\", knn_final)])\n",
        "pipeline_knn_final.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 🧪 Predicciones probabilísticas y etiquetas\n",
        "y_proba_knn = pipeline_knn_final.predict_proba(X_test_scaled)[:, 1]\n",
        "y_pred_knn = (y_proba_knn >= 0.5).astype(int)\n",
        "\n",
        "# 📊 Evaluación\n",
        "print(\"📊 Evaluación del Modelo KNN Final (umbral 0.5)\")\n",
        "print(classification_report(y_test, y_pred_knn, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"]))\n",
        "print(\"✔ AUC:\", roc_auc_score(y_test, y_proba_knn))\n",
        "\n",
        "print(\"📌 Matriz de Confusión:\")\n",
        "print(confusion_matrix(y_test, y_pred_knn))\n",
        "\n",
        "# 🧪 Visualización opcional de curva precision-recall\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_proba_knn)\n",
        "plt.plot(thresholds, precision[:-1], label=\"Precision\")\n",
        "plt.plot(thresholds, recall[:-1], label=\"Recall\")\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Precision & Recall vs Threshold (KNN)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZnAUdlsw9fny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 📌 Extraer mejores hiperparámetros\n",
        "best_params = study_knn.best_params\n",
        "print(\"⚙ Usando hiperparámetros:\", best_params)\n",
        "\n",
        "# Pipeline con SMOTE y KNN usando los mejores hiperparámetros\n",
        "smote = SMOTE(k_neighbors=best_params[\"smote_k\"], random_state=42)\n",
        "knn = KNeighborsClassifier(\n",
        "    n_neighbors=best_params[\"n_neighbors\"],\n",
        "    weights=best_params[\"weights\"],\n",
        "    metric=best_params[\"metric\"]\n",
        ")\n",
        "\n",
        "pipeline_knn = Pipeline([(\"smote\", smote), (\"knn\", knn)])\n",
        "pipeline_knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Probabilidades\n",
        "y_pred_probs_knn = pipeline_knn.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Evaluación por threshold (como antes, pero ahora sí con SMOTE)\n",
        "thresholds = np.arange(0.50, 0.81, 0.02)\n",
        "for thresh in thresholds:\n",
        "    print(f\"\\n🔸 Evaluación con umbral = {thresh:.2f}\")\n",
        "    y_pred = (y_pred_probs_knn >= thresh).astype(int)\n",
        "    print(classification_report(y_test, y_pred, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"]))\n",
        "    print(\"📌 Matriz de Confusión:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "8i2WHjFebGUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardar hiperparametros"
      ],
      "metadata": {
        "id": "EFZcLSXFbDLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Ruta al directorio en tu Google Drive\n",
        "ruta_carpeta_drive_knn = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano - Generación y Análisis de Secuencias de Actividad/logs/Modelos/KNN/'\n",
        "\n",
        "# Crear la carpeta si no existe\n",
        "os.makedirs(ruta_carpeta_drive_knn, exist_ok=True)\n",
        "\n",
        "# Nombre del archivo\n",
        "nombre_archivo = 'modelo_knn_opt.pkl'\n",
        "\n",
        "# Construir la ruta completa uniendo la carpeta con el nombre del archivo\n",
        "# NOTA: El primer argumento de os.path.join debe ser la ruta del directorio, no el modelo.\n",
        "ruta_modelo_completa = os.path.join(ruta_carpeta_drive_knn, nombre_archivo)\n",
        "\n",
        "# Guardar el pipeline completo (modelo entrenado con SMOTE)\n",
        "# La función joblib.dump toma el objeto a guardar como primer argumento\n",
        "joblib.dump(pipeline_knn_final, ruta_modelo_completa)\n",
        "\n",
        "print(f\"✅ Modelo KNN completo guardado en: {ruta_modelo_completa}\")"
      ],
      "metadata": {
        "id": "iKfcSEPQfNOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Catboost"
      ],
      "metadata": {
        "id": "mxrUmAzPVK87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import make_scorer, recall_score\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n"
      ],
      "metadata": {
        "id": "GJ1N6RioVLzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "Hr22YQbobJWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import make_scorer, recall_score\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Scorer personalizado para Recall de la clase 0\n",
        "scorer_recall_clase_0 = make_scorer(recall_score, pos_label=0,average='binary',zero_division=0)\n",
        "# Función objetivo para Optuna\n",
        "def objective_catboost(trial):\n",
        "    # Hiperparámetros para CatBoost\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True) # Rango ajustado, log scale puede ser mejor\n",
        "    depth = trial.suggest_int(\"depth\", 4, 10)\n",
        "    l2_leaf_reg = trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0, log=True) # log scale puede ser mejor\n",
        "    iterations = trial.suggest_int(\"iterations\", 100, 1000) # Aumentamos el máximo, early_stopping se encargará\n",
        "\n",
        "    # Nuevos parámetros\n",
        "    random_strength = trial.suggest_float(\"random_strength\", 0.1, 2.0, log=True)\n",
        "    border_count = trial.suggest_int(\"border_count\", 32, 255) # Comunes 128 o 254\n",
        "    subsample = trial.suggest_float(\"subsample\", 0.6, 1.0) # Fracción de filas para cada árbol\n",
        "    min_data_in_leaf = trial.suggest_int(\"min_data_in_leaf\", 1, 30)\n",
        "\n",
        "    # Parámetro para Early Stopping\n",
        "    early_stopping_rounds = trial.suggest_int(\"early_stopping_rounds\", 10, 100) # Cuántas rondas sin mejora\n",
        "\n",
        "\n",
        "    smote_k = trial.suggest_int(\"smote_k\", 3, 7)\n",
        "\n",
        "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "\n",
        "    catboost = CatBoostClassifier(\n",
        "        learning_rate=learning_rate,\n",
        "        depth=depth,\n",
        "        l2_leaf_reg=l2_leaf_reg,\n",
        "        iterations=iterations, #\n",
        "        random_strength=random_strength,\n",
        "        border_count=border_count,\n",
        "        subsample=subsample,\n",
        "        min_data_in_leaf=min_data_in_leaf,\n",
        "        verbose=0,\n",
        "        random_state=42,\n",
        "        # CatBoost permite la evaluación de métricas durante el entrenamiento para early stopping\n",
        "        # Es importante que la métrica de evaluación sea consistente con lo que quieres optimizar\n",
        "        eval_metric='Recall', # CatBoost usa Recall por defecto, si tu clase positiva es la 1.\n",
        "                              # Para Recall de Clase 0, CatBoost tiene 'Recall:class=0' o puedes usar 'F1' para balance\n",
        "        # Si 'Recall:class=0' no es directamente soportado, usa una métrica general como 'F1'\n",
        "        # o confía en la evaluación externa de cross_val_score con tu custom scorer.\n",
        "        # En este caso, como usamos cross_val_score con nuestro scorer, 'eval_metric' es menos crítico,\n",
        "        # pero es bueno para el early stopping interno de CatBoost. Si 'Recall:class=0' no funciona,\n",
        "        # puedes usar 'Logloss' o 'F1' para el early stopping.\n",
        "    )\n",
        "    pipeline  = ImbPipeline([\n",
        "        (\"smote\", smote),\n",
        "        (\"catboost\", catboost)\n",
        "    ])\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Lista para almacenar los scores de cada fold\n",
        "    fold_scores = []\n",
        "\n",
        "    # Iterar a través de los folds para la validación cruzada\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_scaled, y_train)):\n",
        "        X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        try:\n",
        "            # Para usar early_stopping_rounds, CatBoost necesita un eval_set durante el fit.\n",
        "            # Fit del pipeline (SMOTE se aplica solo al X_train_fold)\n",
        "            # Luego, el CatBoost dentro del pipeline se entrena con su propio eval_set\n",
        "            # Note: Para CatBoost en un pipeline, el eval_set debe pasarse al .fit() del pipeline,\n",
        "            # y el pipeline lo pasa al .fit() del estimador final si este lo soporta.\n",
        "            # En imblearn.pipeline, el fit del estimador final recibe el eval_set.\n",
        "\n",
        "            # Entrenamos el pipeline con el eval_set para CatBoost.\n",
        "            # Asegúrate que tu versión de imblearn y scikit-learn manejen bien eval_set en pipelines.\n",
        "            # Si hay problemas, una alternativa es entrenar el CatBoost fuera del pipeline para el early stopping\n",
        "            # y luego re-entrenar el pipeline completo con los parámetros finales.\n",
        "\n",
        "            # Método 1: Pasar eval_set directamente al pipeline (preferido si es compatible)\n",
        "            # Algunas versiones de imblearn.pipeline o sklearn.pipeline no pasan kwargs a sub-estimadores\n",
        "            # de forma directa. Si esto falla, usa el Método 2.\n",
        "\n",
        "            pipeline.fit(X_train_fold, y_train_fold,\n",
        "                         catboost__eval_set=(X_val_fold, y_val_fold),\n",
        "                         catboost__early_stopping_rounds=early_stopping_rounds)\n",
        "\n",
        "            # Predicción en el conjunto de validación del fold\n",
        "            y_pred_val = pipeline.predict(X_val_fold)\n",
        "\n",
        "            # Calcular el score con tu scorer personalizado\n",
        "            score = scorer_recall_clase_0(pipeline, X_val_fold, y_val_fold)\n",
        "            fold_scores.append(score)\n",
        "\n",
        "        except Exception as e: # Captura excepciones más generales para debug\n",
        "            print(f\"Trial fallido en fold {fold} por error: {e}\")\n",
        "            return 0.0 # O np.nan si prefieres que Optuna ignore estos trials\n",
        "\n",
        "    # Si todos los folds fallan, devuelve 0.0, de lo contrario, el promedio\n",
        "    if not fold_scores: # Si la lista está vacía (todos los trials fallaron)\n",
        "        return 0.0\n",
        "    return np.mean(fold_scores)\n",
        "\n",
        "\n",
        "\n",
        "# Ejecutar la búsqueda\n",
        "study_catboost = optuna.create_study(direction=\"maximize\", study_name=\"CatBoost_SMOTE_Optuna_Recall0\")\n",
        "study_catboost.optimize(objective_catboost, n_trials=100, timeout=1800)\n",
        "\n",
        "# Mostrar mejores resultados\n",
        "print(\"🎯 Mejores hiperparámetros CatBoost encontrados:\")\n",
        "print(study_catboost.best_trial.params)\n",
        "print(f\"📈 Mejor recall clase 0 (promedio CV): {study_catboost.best_value:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "WpSBO9eXQAfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
        "import numpy as np\n",
        "\n",
        "# Usar los mejores hiperparámetros encontrados\n",
        "best_params = study_catboost.best_trial.params\n",
        "\n",
        "#  Asegúrate de que `X_test_scaled` e `y_test` estén definidos correctamente\n",
        "\n",
        "# Reconstruir SMOTE y modelo con los mejores parámetros\n",
        "smote = SMOTE(k_neighbors=best_params['smote_k'], random_state=42)\n",
        "\n",
        "catboost_final = CatBoostClassifier(\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    depth=best_params['depth'],\n",
        "    l2_leaf_reg=best_params['l2_leaf_reg'],\n",
        "    iterations=best_params['iterations'],\n",
        "    random_strength=best_params['random_strength'],\n",
        "    border_count=best_params['border_count'],\n",
        "    subsample=best_params['subsample'],\n",
        "    min_data_in_leaf=best_params['min_data_in_leaf'],\n",
        "    verbose=0,\n",
        "    random_state=42,\n",
        "    eval_metric='Recall' # O 'F1', o 'Recall:class=0' si CatBoost lo soporta para la clase 0\n",
        ")\n",
        "\n",
        "# Construir el pipeline\n",
        "pipeline = ImbPipeline([\n",
        "    (\"smote\", smote),\n",
        "    (\"catboost\", catboost_final)\n",
        "])\n",
        "\n",
        "# Entrenar el pipeline completo con todos los datos de entrenamiento\n",
        "pipeline.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Obtener probabilidades de predicción\n",
        "y_proba = pipeline.predict_proba(X_test_scaled)[:, 1]  # Probabilidades clase 1\n",
        "\n",
        "# Bucle de evaluación con distintos umbrales\n",
        "for threshold in np.arange(0.50, 0.81, 0.02):\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "\n",
        "    print(f\"\\n Evaluación con umbral = {threshold:.2f}\")\n",
        "    print(classification_report(y_test, y_pred, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"]))\n",
        "    print(\" Matriz de Confusión:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# AUC general (sin importar el threshold)\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"\\n AUC: {auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "KllOsUIaVGhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold_final = 0.80\n",
        "y_pred_final = (y_proba >= threshold_final).astype(int)\n",
        "\n",
        "print(\"🔍 Evaluación Final del Modelo CatBoost con Threshold =\", threshold_final)\n",
        "print(classification_report(y_test, y_pred_final, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"], zero_division=0))\n",
        "print(\"📌 Matriz de Confusión:\")\n",
        "print(confusion_matrix(y_test, y_pred_final, labels=[0, 1]))"
      ],
      "metadata": {
        "id": "uPcCiwfuWSzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Ruta en tu Google Drive\n",
        "ruta_carpeta_drive_catboost = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano - Generación y Análisis de Secuencias de Actividad/logs/Modelos/CatBoost/'\n",
        "\n",
        "# Crear la carpeta si no existe\n",
        "os.makedirs(ruta_carpeta_drive_catboost, exist_ok=True)\n",
        "\n",
        "# Nombre de archivo\n",
        "nombre_archivo_catboost = 'modelo_catboost_opt.pkl'\n",
        "ruta_modelo_catboost = os.path.join(ruta_carpeta_drive_catboost, nombre_archivo_catboost)\n",
        "\n",
        "# Guardar el pipeline completo (incluye SMOTE y CatBoost ya entrenado)\n",
        "joblib.dump(pipeline, ruta_modelo_catboost)\n",
        "\n",
        "print(f\"✅ Modelo CatBoost guardado en: {ruta_modelo_catboost}\")\n"
      ],
      "metadata": {
        "id": "mYY09C36ndDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine - SVM"
      ],
      "metadata": {
        "id": "Fts295zvSd25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.svm import SVC\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score # Importa cross_val_score\n",
        "from sklearn.metrics import recall_score, make_scorer\n",
        "import numpy as np\n",
        "\n",
        "# Asegúrate de que X_train_scaled y y_train estén definidos y listos para usar\n",
        "\n",
        "# Scorer personalizado para Recall de la clase 0\n",
        "# Añadimos zero_division=0 para manejar casos donde un fold podría no tener instancias de la clase 0,\n",
        "# evitando errores de división por cero y devolviendo 0.0 en esos casos.\n",
        "scorer_recall_clase_0 = make_scorer(recall_score, pos_label=0, average='binary', zero_division=0)\n",
        "\n",
        "def objective_svm(trial):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    # Hiperparámetros para el clasificador SVM (SVC)\n",
        "    # Se utiliza el prefijo 'svc__' para indicar que son parámetros del paso 'svc' en el pipeline.\n",
        "    C = trial.suggest_float('svc__C', 1e-2, 100, log=True) # Parámetro de regularización\n",
        "    gamma = trial.suggest_categorical('svc__gamma', ['scale', 'auto', 0.01, 0.1, 1]) # Coeficiente del kernel\n",
        "    kernel = trial.suggest_categorical('svc__kernel', ['linear', 'rbf', 'poly']) # Tipo de kernel\n",
        "\n",
        "    # El parámetro 'degree' solo es relevante para el kernel 'poly'.\n",
        "    # Optuna permite sugerencias condicionales de hiperparámetros.\n",
        "    if kernel == 'poly':\n",
        "        degree = trial.suggest_int('svc__degree', 2, 5)\n",
        "    else:\n",
        "        # Si el kernel no es 'poly', el valor de 'degree' es irrelevante,\n",
        "        # pero debe ser proporcionado. SVC lo ignorará.\n",
        "        degree = 3\n",
        "\n",
        "    # Hiperparámetro para el oversampling SMOTE\n",
        "    # Se utiliza el prefijo 'smote__' para indicar que es un parámetro del paso 'smote' en el pipeline.\n",
        "    smote_k = trial.suggest_int('smote__k_neighbors', 3, 7) # Número de vecinos para SMOTE\n",
        "\n",
        "    # 1. Definir la instancia de SMOTE con el k_neighbors sugerido\n",
        "    smote_instance = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "\n",
        "    # 2. Definir la instancia de SVC con los hiperparámetros sugeridos\n",
        "    # probability=True es necesario para obtener probabilidades y luego ajustar umbrales.\n",
        "    svm_classifier = SVC(probability=True, C=C, gamma=gamma, kernel=kernel,\n",
        "                         degree=degree, random_state=42)\n",
        "\n",
        "    # 3. Construir el pipeline usando ImbPipeline (de imblearn)\n",
        "    # ImbPipeline asegura que SMOTE se aplica solo al conjunto de entrenamiento dentro de cada fold de CV.\n",
        "    pipeline = ImbPipeline([\n",
        "        ('smote', smote_instance), # Paso de oversampling\n",
        "        ('svc', svm_classifier)    # Paso del clasificador SVM\n",
        "    ])\n",
        "\n",
        "    # 4. Configurar la validación cruzada estratificada\n",
        "    # shuffle=True y random_state para reproducibilidad\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    try:\n",
        "        # 5. Evaluar el pipeline usando cross_val_score\n",
        "        # Esto automatiza el bucle de CV, el fitting y la puntuación de cada fold.\n",
        "        # scoring=scorer_recall_clase_0 asegura que se maximice el recall de la clase 0.\n",
        "        # n_jobs=-1 para usar todos los núcleos disponibles de la CPU, acelerando la CV.\n",
        "        scores = cross_val_score(pipeline, X_train_scaled, y_train,\n",
        "                                 scoring=scorer_recall_clase_0, cv=cv, n_jobs=-1)\n",
        "\n",
        "        # Devolver el promedio de los scores de los folds\n",
        "        return np.mean(scores)\n",
        "    except Exception as e:\n",
        "        # Capturar cualquier excepción y devolver 0.0 para penalizar el trial fallido.\n",
        "        print(f\"Trial fallido por error: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "# 6. Ejecutar la búsqueda de hiperparámetros con Optuna\n",
        "# direction=\"maximize\" porque queremos maximizar el recall.\n",
        "# n_trials: número de combinaciones de hiperparámetros a probar.\n",
        "# timeout: tiempo máximo en segundos para ejecutar la optimización.\n",
        "study_svm = optuna.create_study(direction=\"maximize\", study_name=\"SVM_SMOTE_Recall0\")\n",
        "study_svm.optimize(objective_svm, n_trials=100, timeout=1800)\n",
        "\n",
        "# 7. Mostrar los mejores resultados obtenidos\n",
        "print(\"🎯 Mejores hiperparámetros encontrados para SVM:\")\n",
        "print(study_svm.best_trial.params)\n",
        "print(f\"📈 Mejor recall clase 0 (promedio CV): {study_svm.best_value:.4f}\")\n"
      ],
      "metadata": {
        "id": "LdmqaQH3qXF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "#  Extraer mejores hiperparámetros\n",
        "best_params = study_svm.best_trial.params\n",
        "\n",
        "#  Configurar el modelo y SMOTE con los mejores parámetros\n",
        "smote = SMOTE(k_neighbors=best_params['smote__k_neighbors'], random_state=42)\n",
        "\n",
        "svm_final = SVC(\n",
        "    C=best_params['svc__C'],\n",
        "    gamma=best_params['svc__gamma'],\n",
        "    kernel=best_params['svc__kernel'],\n",
        "    degree=best_params.get('svc__degree', 3),  # solo importa si kernel = poly\n",
        "    probability=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "#  Construir pipeline final\n",
        "pipeline = ImbPipeline([\n",
        "    (\"smote\", smote),\n",
        "    (\"svc\", svm_final)\n",
        "])\n",
        "\n",
        "#  Entrenar el modelo final con todo el conjunto de entrenamiento\n",
        "pipeline.fit(X_train_scaled, y_train)\n",
        "\n",
        "#  Predicciones probabilísticas\n",
        "y_proba = pipeline.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "#  Evaluación con distintos umbrales\n",
        "print(\" Evaluación del Modelo SVM Final con distintos umbrales:\\n\")\n",
        "for threshold in np.arange(0.50, 0.81, 0.02):\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "    print(f\" Umbral = {threshold:.2f}\")\n",
        "    print(classification_report(y_test, y_pred, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"], zero_division=0))\n",
        "    print(\" Matriz de Confusión:\")\n",
        "    print(confusion_matrix(y_test, y_pred, labels=[0, 1]))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "#  AUC general\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"\\n✔ AUC: {auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "qD67Ww-E69z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Ruta destino\n",
        "ruta_guardado_svm = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano - Generación y Análisis de Secuencias de Actividad/logs/Modelos/SVM'\n",
        "os.makedirs(ruta_guardado_svm, exist_ok=True)\n",
        "\n",
        "# Nombre del archivo\n",
        "nombre_archivo = 'modelo_svm_opt.pkl'\n",
        "ruta_modelo_completa = os.path.join(ruta_guardado_svm, nombre_archivo)\n",
        "\n",
        "# Guardar el pipeline completo\n",
        "joblib.dump(pipeline, ruta_modelo_completa)\n",
        "print(f\"✅ Modelo SVM final guardado en: {ruta_modelo_completa}\")\n"
      ],
      "metadata": {
        "id": "yeezk9997oQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "_nCHqVTVscc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import make_scorer, recall_score\n",
        "import numpy as np\n",
        "\n",
        "# ⚙️ Scorer personalizado: Recall para clase 0\n",
        "scorer_recall_clase_0 = make_scorer(recall_score, pos_label=0, average='binary', zero_division=0)\n",
        "\n",
        "def objective_rf(trial):\n",
        "    # Hiperparámetros para Random Forest\n",
        "    n_estimators = trial.suggest_int(\"rf__n_estimators\", 100, 1000)\n",
        "    max_depth = trial.suggest_int(\"rf__max_depth\", 3, 50)\n",
        "    min_samples_split = trial.suggest_int(\"rf__min_samples_split\", 2, 22)\n",
        "    min_samples_leaf = trial.suggest_int(\"rf__min_samples_leaf\", 1, 22)\n",
        "    max_features = trial.suggest_categorical(\"rf__max_features\", ['sqrt', 'log2',None])\n",
        "    criterion = trial.suggest_categorical(\"rf__criterion\", ['gini', 'entropy'])\n",
        "    class_weight = trial.suggest_categorical(\"rf__class_weight\", [None, 'balanced', 'balanced_subsample'])\n",
        "    smote_k = trial.suggest_int(\"smote__k_neighbors\", 3, 7)\n",
        "\n",
        "    # Instanciar componentes\n",
        "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        criterion=criterion,\n",
        "        class_weight=class_weight,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Crear pipeline\n",
        "    pipeline = ImbPipeline([\n",
        "        (\"smote\", smote),\n",
        "        (\"rf\", rf)\n",
        "    ])\n",
        "\n",
        "    # Validación cruzada estratificada\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    try:\n",
        "        scores = cross_val_score(pipeline, X_train_scaled, y_train,\n",
        "                                 scoring=scorer_recall_clase_0, cv=cv, n_jobs=-1)\n",
        "        return np.mean(scores)\n",
        "    except Exception as e:\n",
        "        print(f\" Trial fallido: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "# Ejecutar optimización con Optuna\n",
        "study_rf = optuna.create_study(direction=\"maximize\", study_name=\"RandomForest_SMOTE_Recall0\")\n",
        "study_rf.optimize(objective_rf, n_trials=100, timeout=1800)\n",
        "\n",
        "# Resultados\n",
        "print(\" Mejores hiperparámetros encontrados:\")\n",
        "print(study_rf.best_trial.params)\n",
        "print(f\" Mejor recall clase 0 (promedio CV): {study_rf.best_value:.4f}\")\n"
      ],
      "metadata": {
        "id": "oxoanB7j5GT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisar dimensiones\n",
        "print(\" X_train_scaled shape:\", X_train_scaled.shape)\n",
        "print(\" y_train shape:\", y_train.shape)\n",
        "\n",
        "# 📊 Recuento por clase en y_train\n",
        "import numpy as np\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "print(\"Distribución de clases en y_train:\")\n",
        "for label, count in zip(unique, counts):\n",
        "    print(f\"Clase {label}: {count} ejemplos\")\n"
      ],
      "metadata": {
        "id": "9_VkYQ6I7xCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Reconstruir modelo con mejores hiperparámetros\n",
        "best_params = study_rf.best_trial.params\n",
        "\n",
        "# Instanciar SMOTE y RF con mejores parámetros\n",
        "smote = SMOTE(k_neighbors=best_params[\"smote__k_neighbors\"], random_state=42)\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=best_params[\"rf__n_estimators\"],\n",
        "    max_depth=best_params[\"rf__max_depth\"],\n",
        "    min_samples_split=best_params[\"rf__min_samples_split\"],\n",
        "    min_samples_leaf=best_params[\"rf__min_samples_leaf\"],\n",
        "    max_features=best_params[\"rf__max_features\"],\n",
        "    criterion=best_params[\"rf__criterion\"],\n",
        "    class_weight=best_params[\"rf__class_weight\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Crear pipeline final\n",
        "pipeline_rf_final = ImbPipeline([\n",
        "    (\"smote\", smote),\n",
        "    (\"rf\", rf_model)\n",
        "])\n",
        "\n",
        "# Entrenar modelo con todo el set de entrenamiento\n",
        "pipeline_rf_final.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Probabilidades para clase 0\n",
        "y_proba = pipeline_rf_final.predict_proba(X_test_scaled)[:, 0]  # Probabilidad de clase 0 (reprobar)\n",
        "\n",
        "# AUC general\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"✔ AUC: {auc:.4f}\\n\")\n",
        "\n",
        "# 📊 Evaluación con distintos thresholds\n",
        "thresholds = np.arange(0.50, 0.81, 0.02)\n",
        "\n",
        "for threshold in thresholds:\n",
        "    print(f\"🔸 Umbral = {threshold:.2f}\")\n",
        "\n",
        "    # Convertir probabilidades a clases según threshold\n",
        "    y_pred_thresh = (y_proba >= threshold).astype(int)\n",
        "\n",
        "    print(classification_report(y_test, y_pred_thresh, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"]))\n",
        "    print(\"📌 Matriz de Confusión:\")\n",
        "    print(confusion_matrix(y_test, y_pred_thresh))\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "ZTdb9S2dAdLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Ruta en tu Google Drive\n",
        "ruta_carpeta_rf = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano - Generación y Análisis de Secuencias de Actividad/logs/Modelos/RandomForest/'\n",
        "os.makedirs(ruta_carpeta_rf, exist_ok=True)\n",
        "\n",
        "# Ruta final del archivo\n",
        "ruta_guardado_rf = os.path.join(ruta_carpeta_rf, 'modelo_rf_optuna.pkl')\n",
        "\n",
        "# Guardar el pipeline entrenado\n",
        "joblib.dump(pipeline_rf_final, ruta_guardado_rf)\n",
        "\n",
        "print(f\"✅ Modelo RF (Optuna) guardado en: {ruta_guardado_rf}\")\n"
      ],
      "metadata": {
        "id": "HWNaMhBtgt1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "id": "L44mi_8mDLuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP"
      ],
      "metadata": {
        "id": "0yYihGvYD2XQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input # Importar Input explícitamente\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import recall_score, classification_report, confusion_matrix, roc_auc_score, accuracy_score, f1_score, make_scorer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "import traceback\n",
        "import os\n",
        "\n",
        "# --- Comprobaciones iniciales de datos y conversión a NumPy ---\n",
        "# Es preferible trabajar con arrays de NumPy para Keras, SMOTE y Sci-kit Learn\n",
        "# para evitar problemas de indexación que pueden ocurrir con Pandas DataFrames/Series\n",
        "# si tienen índices no secuenciales.\n",
        "\n",
        "\n",
        "# Convertir X_train_scaled y y_train a arrays de NumPy\n",
        "X_train_scaled_np = X_train_scaled.values if hasattr(X_train_scaled, 'values') else X_train_scaled\n",
        "y_train_np = y_train.values if hasattr(y_train, 'values') else y_train\n",
        "\n",
        "# Asegurarse de que los datos de prueba también estén en formato NumPy\n",
        "X_test_scaled_np = X_test_scaled.values if 'X_test_scaled' in locals() and hasattr(X_test_scaled, 'values') else X_test_scaled\n",
        "y_test_np = y_test.values if 'y_test' in locals() and hasattr(y_test, 'values') else y_test\n",
        "\n",
        "\n",
        "# --- PASO 1: Función para Crear el Modelo Keras ---\n",
        "def create_simple_keras_model(input_dim, optimizer_config=('adam', 0.001), layer_config=((64, 0.2), (32, 0.2)), activation_config='relu'):\n",
        "    \"\"\"\n",
        "    Crea un modelo Keras secuencial con capas Dense y Dropout.\n",
        "    Acepta configuración de optimizador, capas ocultas y activación.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Se añade la capa Input explícitamente para evitar la UserWarning de Keras.\n",
        "    model.add(Input(shape=(input_dim,)))\n",
        "\n",
        "    # Primera capa Dense (ya no necesita input_shape/input_dim aquí)\n",
        "    model.add(Dense(layer_config[0][0], activation=activation_config))\n",
        "    if layer_config[0][1] > 0: # Si hay dropout en la primera capa\n",
        "        model.add(Dropout(layer_config[0][1]))\n",
        "\n",
        "    # Añadir capas ocultas adicionales (a partir de la segunda)\n",
        "    for neurons, dropout in layer_config[1:]: # Iterar desde el segundo elemento en adelante\n",
        "        if neurons > 0:\n",
        "            model.add(Dense(neurons, activation=activation_config))\n",
        "            if dropout > 0:\n",
        "                model.add(Dropout(dropout))\n",
        "\n",
        "    # Capa de salida para clasificación binaria\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Configurar el optimizador\n",
        "    optimizer_name, learning_rate = optimizer_config\n",
        "    if optimizer_name.lower() == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name.lower() == 'rmsprop':\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "    else: # Por defecto a Adam\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # Definir métrica de recall para la clase 0\n",
        "    recall_metric_clase_0 = keras.metrics.Recall(class_id=0, name=\"recall_clase_0\")\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy', recall_metric_clase_0])\n",
        "    return model\n",
        "\n",
        "# --- PASO 2: Definir la Función Objetivo para Optuna ---\n",
        "def objective_simple_mlp(trial):\n",
        "    K.clear_session() # Limpiar la sesión de Keras para cada trial\n",
        "\n",
        "    # Hiperparámetros para SMOTE\n",
        "    smote_k_val = trial.suggest_categorical('smote_k_neighbors', [3, 5, 7])\n",
        "\n",
        "    # Hiperparámetros para Keras MLP\n",
        "    n_hidden_layers = trial.suggest_int('n_hidden_layers', 1, 2)\n",
        "    layer_config_list = []\n",
        "    for i in range(n_hidden_layers):\n",
        "        neurons = trial.suggest_categorical(f'neurons_l{i}', [32, 64, 128])\n",
        "        dropout = trial.suggest_categorical(f'dropout_l{i}', [0.2, 0.3, 0.4, 0.5])\n",
        "        layer_config_list.append((neurons, dropout))\n",
        "    layer_config_val = tuple(layer_config_list)\n",
        "\n",
        "    activation_val = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
        "    optimizer_name_val = trial.suggest_categorical('optimizer_name', ['adam', 'rmsprop'])\n",
        "    learning_rate_val = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "\n",
        "    epochs_val = trial.suggest_categorical('epochs', [25, 50,75,100])\n",
        "    batch_size_val = trial.suggest_categorical('batch_size', [32, 64])\n",
        "\n",
        "    # Configuración de Early Stopping\n",
        "    early_stopping_patience = trial.suggest_int('early_stopping_patience', 5, 15)\n",
        "\n",
        "    # Validación cruzada estratificada\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_recall_scores_clase_0 = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_scaled_np, y_train_np)):\n",
        "        X_train_fold_clean, X_val_fold_clean = X_train_scaled_np[train_idx], X_train_scaled_np[val_idx]\n",
        "        y_train_fold_clean, y_val_fold_clean = y_train_np[train_idx], y_train_np[val_idx]\n",
        "\n",
        "        # Aplicar SMOTE solo al conjunto de entrenamiento del fold\n",
        "        sm = SMOTE(random_state=42, k_neighbors=smote_k_val)\n",
        "        try:\n",
        "            X_train_fold_res, y_train_fold_res = sm.fit_resample(X_train_fold_clean, y_train_fold_clean)\n",
        "        except ValueError as smote_err:\n",
        "            print(f\"⚠️ SMOTE falló en fold {fold} con k_neighbors={smote_k_val}: {smote_err}. Penalizando.\")\n",
        "            fold_recall_scores_clase_0.append(0.0)\n",
        "            continue\n",
        "\n",
        "        # Crear el modelo Keras con los hiperparámetros del trial\n",
        "        model = create_simple_keras_model(\n",
        "            input_dim=X_train_fold_res.shape[1],\n",
        "            optimizer_config=(optimizer_name_val, learning_rate_val),\n",
        "            layer_config=layer_config_val,\n",
        "            activation_config=activation_val\n",
        "        )\n",
        "\n",
        "        # Configurar el callback de Early Stopping\n",
        "        early_stopping_cb = EarlyStopping(\n",
        "            monitor='val_recall_clase_0',\n",
        "            patience=early_stopping_patience,\n",
        "            mode='max',\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Entrenar el modelo\n",
        "        history = model.fit(X_train_fold_res, y_train_fold_res,\n",
        "                            epochs=epochs_val,\n",
        "                            batch_size=batch_size_val,\n",
        "                            validation_data=(X_val_fold_clean, y_val_fold_clean),\n",
        "                            callbacks=[early_stopping_cb],\n",
        "                            verbose=0)\n",
        "\n",
        "        # Evaluar el modelo en el conjunto de validación del fold\n",
        "        _, _, val_recall_clase_0_metric_value = model.evaluate(X_val_fold_clean, y_val_fold_clean, verbose=0)\n",
        "\n",
        "        fold_recall_scores_clase_0.append(val_recall_clase_0_metric_value)\n",
        "\n",
        "        # Reportar el score actual del fold a Optuna para poda\n",
        "        trial.report(val_recall_clase_0_metric_value, fold)\n",
        "\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    mean_recall_clase_0 = np.mean(fold_recall_scores_clase_0) if fold_recall_scores_clase_0 else 0.0\n",
        "    return mean_recall_clase_0\n",
        "\n",
        "# --- PASO 3: Crear un Estudio de Optuna y Ejecutar la Optimización ---\n",
        "study_name = \"MLP_Keras_Optuna_Recall0\"\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0, interval_steps=1)\n",
        "\n",
        "study_mlp_optuna = optuna.create_study(direction=\"maximize\", study_name=study_name, sampler=sampler, pruner=pruner)\n",
        "\n",
        "n_trials_optuna = 100\n",
        "timeout_seconds = 3600 # 1 hora de timeout\n",
        "\n",
        "print(f\" Iniciando optimización con Optuna para Keras MLP ({n_trials_optuna} trials, timeout: {timeout_seconds/60} min)...\")\n",
        "try:\n",
        "    study_mlp_optuna.optimize(objective_simple_mlp, n_trials=n_trials_optuna, timeout=timeout_seconds)\n",
        "    print(\" Optimización con Optuna para MLP Keras completada.\")\n",
        "\n",
        "    if study_mlp_optuna.best_trial:\n",
        "        print(f\"\\n Mejores hiperparámetros: {study_mlp_optuna.best_trial.params}\")\n",
        "        print(f\" Mejor Recall Clase 0.0 (promedio CV): {study_mlp_optuna.best_trial.value:.4f}\")\n",
        "    else:\n",
        "        print(\"⚠️ Optuna no encontró un 'best_trial' (posiblemente por timeout o todos los trials fallaron).\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Ocurrió un error INESPERADO durante la optimización con Optuna:\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Oasjet27Gu3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 4: Crear y Entrenar el Modelo Final con los Mejores Hiperparámetros ---\n",
        "\n",
        "\n",
        "best_params = study_mlp_optuna.best_trial.params # Acceso directo a los mejores parámetros\n",
        "\n",
        "# Preparar configuración del modelo final\n",
        "input_dim = X_train_scaled_np.shape[1]\n",
        "\n",
        "# Reconstruir la configuración de las capas dinámicamente a partir de best_params\n",
        "layer_config_list = []\n",
        "num_hidden_layers_optuna = best_params.get('n_hidden_layers', 1)\n",
        "for i in range(num_hidden_layers_optuna):\n",
        "    neurons = best_params.get(f'neurons_l{i}', 64)\n",
        "    dropout = best_params.get(f'dropout_l{i}', 0.5)\n",
        "    layer_config_list.append((neurons, dropout))\n",
        "layer_config_final = tuple(layer_config_list)\n",
        "\n",
        "# Crear la instancia del modelo Keras final\n",
        "final_model = create_simple_keras_model(\n",
        "    input_dim=input_dim,\n",
        "    optimizer_config=(best_params['optimizer_name'], best_params['learning_rate']),\n",
        "    layer_config=layer_config_final,\n",
        "    activation_config=best_params['activation']\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento completo (usando los k_neighbors óptimos)\n",
        "smote_final_k = best_params.get('smote_k_neighbors', 5)\n",
        "smote_final = SMOTE(k_neighbors=smote_final_k, random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote_final.fit_resample(X_train_scaled_np, y_train_np.astype(int))\n",
        "\n",
        "# Configurar Early Stopping para el entrenamiento final\n",
        "early_stopping_final_cb = EarlyStopping(\n",
        "    monitor='val_recall_clase_0', # Monitorea el recall de la clase 0 en el conjunto de validación\n",
        "    patience=6, #best_params['early_stopping_patience'], # Paciencia óptima\n",
        "    min_delta=0.02,\n",
        "    mode='max', # Buscamos maximizar el recall\n",
        "    restore_best_weights=True, # Restaurar los pesos del mejor epoch\n",
        "    verbose=1 # Mostrar el progreso del early stopping\n",
        ")\n",
        "\n",
        "print(f\"\\n Entrenando el modelo final con los mejores hiperparámetros: {best_params}\")\n",
        "# Entrenar el modelo final y CAPTURAR EL HISTORIAL en la variable 'history' (global)\n",
        "history = final_model.fit( # <-- LA VARIABLE 'history' GLOBAL SE DEFINE AQUÍ\n",
        "    X_train_resampled, y_train_resampled,\n",
        "    epochs=best_params['epochs'], # Épocas máximas óptimas\n",
        "    batch_size=best_params['batch_size'], # Batch size óptimo\n",
        "    validation_split=0.25, # Usar una parte de los datos remuestreados para validación\n",
        "    callbacks=[early_stopping_final_cb], # Aplicar Early Stopping\n",
        "    verbose=1 # Mostrar el progreso del entrenamiento\n",
        ")\n"
      ],
      "metadata": {
        "id": "enAZ-n0cmTSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 5: Evaluación en el conjunto de prueba ---\n",
        "\n",
        "print(\" Bloque 5: Modelo final entrenado y historial capturado.\")\n",
        "y_pred_probs = final_model.predict(X_test_scaled_np, verbose=0)\n",
        "y_pred_probs_flat = y_pred_probs.flatten() # Asegura que sea un array 1D\n",
        "\n",
        "print(\"\\n📊 Bloque 6: Evaluación del Modelo MLP Final con distintos umbrales en el conjunto de prueba:\\n\")\n",
        "for threshold in np.arange(0.50, 0.81, 0.02): # Rango de umbrales para explorar\n",
        "    y_pred_thresholded = (y_pred_probs_flat >= threshold).astype(int) # Predicciones binarizadas\n",
        "\n",
        "    print(f\" Umbral = {threshold:.2f}\")\n",
        "    print(classification_report(y_test_np, y_pred_thresholded, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"], zero_division=0))\n",
        "    print(\" Matriz de Confusión:\")\n",
        "    print(confusion_matrix(y_test_np, y_pred_thresholded, labels=[0, 1]))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Métricas Globales (independientes del umbral, evalúan la capacidad de ranking)\n",
        "print(\"\\n--- Métricas Globales del Modelo (independientes del umbral) ---\")\n",
        "auc = roc_auc_score(y_test_np, y_pred_probs_flat)\n",
        "print(f\"✔ ROC AUC: {auc:.4f}\")\n",
        "print(\" Bloque 6: Evaluación en el conjunto de prueba completada.\")"
      ],
      "metadata": {
        "id": "ScKIlt91l08Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 6: Graficar el Historial de Entrenamiento para Diagnóstico de Overfitting/Underfitting ---\n",
        "\n",
        "print(\"\\n Bloque 7: Generando gráficos del historial de entrenamiento para diagnosticar Overfitting/Underfitting\")\n",
        "\n",
        "# Define la ruta para guardar los gráficos\n",
        "#ruta_carpeta_graficos = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano - Generación y Análisis de Secuencias de Actividad/logs/Modelos/MLP/graficos/'\n",
        "#os.makedirs(ruta_carpeta_graficos, exist_ok=True) # Crea la carpeta si no existe\n",
        "\n",
        "# Gráfico de la pérdida (Loss)\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')\n",
        "if 'val_loss' in history.history:\n",
        "    plt.plot(history.history['val_loss'], label='Pérdida de Validación')\n",
        "plt.title('Pérdida (Loss) durante el Entrenamiento')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "#plt.savefig(os.path.join(ruta_carpeta_graficos, 'loss_plot.png')) # Guardar gráfico de pérdida\n",
        "\n",
        "# Gráfico del Recall de la Clase 0\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['recall_clase_0'], label='Recall Clase 0 (Entrenamiento)')\n",
        "if 'val_recall_clase_0' in history.history:\n",
        "    plt.plot(history.history['val_recall_clase_0'], label='Recall Clase 0 (Validación)')\n",
        "plt.title('Recall de la Clase 0 durante el Entrenamiento')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "#plt.savefig(os.path.join(ruta_carpeta_graficos, 'recall_clase0_plot.png')) # Guardar gráfico de recall\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show() # Muestra los gráficos en la salida del Notebook/Canvas\n",
        "\n",
        "print(\"✅ Bloque 7: Gráficos generados y guardados en Google Drive.\")"
      ],
      "metadata": {
        "id": "nCLKtKMBnTB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 7: Guardar el Modelo Final ---\n",
        "\n",
        "\n",
        "ruta_carpeta_modelo = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano - Generación y Análisis de Secuencias de Actividad/logs/Modelos/MLP/'\n",
        "os.makedirs(ruta_carpeta_modelo, exist_ok=True)\n",
        "\n",
        "nombre_archivo_modelo_mlp = 'modelo_optuna_final.keras' # Nombre que ya decidimos\n",
        "ruta_modelo_completa_mlp = os.path.join(ruta_carpeta_modelo, nombre_archivo_modelo_mlp)\n",
        "\n",
        "final_model.save(ruta_modelo_completa_mlp)\n",
        "print(f\"✅ Bloque 8: Modelo MLP entrenado y guardado en: {ruta_modelo_completa_mlp}\")"
      ],
      "metadata": {
        "id": "hq1s-21RnRmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNN OPTUNA"
      ],
      "metadata": {
        "id": "Cq0B_Q0nb9Nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Parámetros:\n",
        "    - input_dim: Dimensión de entrada (número de features).\n",
        "    - n_hidden_layers: Número de capas ocultas.\n",
        "    - neurons_list: Lista con la cantidad de neuronas por capa.\n",
        "    - dropout_list: Lista con valores de dropout por capa (opcional).\n",
        "    - activation: Función de activación (por defecto 'relu')."
      ],
      "metadata": {
        "id": "OvbwVyleconR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import recall_score,f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from optuna.exceptions import TrialPruned\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "QqpB1AyooqNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dnn_model(input_dim, optimizer_config=('adam', 0.001), layer_config=((128, 0.3),), activation_config='relu'):\n",
        "    \"\"\"\n",
        "    Crea y compila un modelo de red neuronal densa (DNN).\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))\n",
        "    for neurons, dr_rate in layer_config:\n",
        "        if neurons > 0:\n",
        "            model.add(Dense(neurons, activation=activation_config))\n",
        "            if dr_rate > 0:\n",
        "                model.add(Dropout(dr_rate))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    optimizer_name, learning_rate = optimizer_config\n",
        "    opt = Adam(learning_rate=learning_rate) if optimizer_name == 'adam' else RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    # Se mantiene el recall para poder graficarlo, pero no será el objetivo principal\n",
        "    recall_c0 = keras.metrics.Recall(class_id=0, name=\"recall_clase_0\")\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', recall_c0])\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# Funcion objetivo\n",
        "\n",
        "def objective_dnn_optuna_f1(trial):\n",
        "\n",
        "    K.clear_session()\n",
        "\n",
        "    # --- CAMBIO: Espacio de búsqueda ajustado para modelos más robustos ---\n",
        "    smote_k = trial.suggest_categorical('smote_k', [3, 5, 7])\n",
        "    n_layers = trial.suggest_int('n_hidden_layers', 1, 3) # Menos capas para evitar sobreajuste\n",
        "\n",
        "    layer_conf = []\n",
        "    for i in range(n_layers):\n",
        "        neurons = trial.suggest_categorical(f'n_units_l{i}', [32, 64, 96]) # Menos neuronas\n",
        "        dropout = trial.suggest_float(f'dropout_l{i}', 0.2, 0.6, step=0.1) # Mayor rango de dropout\n",
        "        layer_conf.append((neurons, dropout))\n",
        "\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])\n",
        "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
        "    epochs = trial.suggest_categorical('epochs', [25, 50, 75, 100])\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "    f1_scores = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
        "        X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        try:\n",
        "            sm = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "            X_train_res, y_train_res = sm.fit_resample(X_train_fold, y_train_fold)\n",
        "        except ValueError:\n",
        "            raise TrialPruned()\n",
        "\n",
        "        model = create_dnn_model(input_dim=X_train_res.shape[1], optimizer_config=(optimizer_name, lr), layer_config=tuple(layer_conf), activation_config=activation)\n",
        "\n",
        "        # El EarlyStopping ahora puede monitorear la pérdida de validación, que es más estable\n",
        "        cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True, verbose=0)\n",
        "\n",
        "        model.fit(X_train_res, y_train_res, epochs=epochs, batch_size=batch_size, validation_data=(X_val_fold, y_val_fold), callbacks=[cb], verbose=0)\n",
        "\n",
        "        # --- CAMBIO: Evaluación basada en F1-Score ---\n",
        "        y_pred_proba = model.predict(X_val_fold, verbose=0)\n",
        "        y_pred_classes = (y_pred_proba > 0.5).astype(\"int32\")\n",
        "\n",
        "        # Se calcula el F1 para la clase 0 (reprobados). zero_division=0 evita errores si no hay predicciones.\n",
        "        score = f1_score(y_val_fold, y_pred_classes, pos_label=0, zero_division=0)\n",
        "        f1_scores.append(score)\n",
        "\n",
        "    return np.mean(f1_scores)\n",
        "\n",
        "\n",
        "\n",
        "study_dnn = optuna.create_study(direction=\"maximize\", study_name=\"DNN_F1_Score_Optuna\")\n",
        "# CAMBIO: La función objetivo ahora es la que optimiza el F1\n",
        "study_dnn.optimize(objective_dnn_optuna_f1, n_trials=100, timeout=3600)\n",
        "\n",
        "print(\"\\n Mejor Trial (Optimizado para F1-Score):\")\n",
        "# CAMBIO: El valor mostrado ahora es el F1-Score\n",
        "print(f\"   Valor (F1-Score clase 0): {study_dnn.best_value:.4f}\")\n",
        "best_params = study_dnn.best_trial.params\n",
        "print(\"  Hiperparámetros:\")\n",
        "for key, value in best_params.items():\n",
        "    print(f\"    - {key}: {value}\")\n"
      ],
      "metadata": {
        "id": "cjuj7iHTtmLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import Callback # Import the Callback class\n",
        "from imblearn.over_sampling import SMOTE # Ensure SMOTE is imported\n",
        "\n",
        "\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(self, validation_data, pos_label=0):\n",
        "        super().__init__()\n",
        "        self.X_val, self.y_val = validation_data\n",
        "        self.pos_label = pos_label\n",
        "        self.train_f1s = [] # List to store F1-scores for training set\n",
        "        self.val_f1s = []   # List to store F1-scores for validation set\n",
        "        # Store the validation data to be used directly\n",
        "        self.val_data_for_pred = validation_data # Store explicitly for prediction\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # Predict on the validation data passed to the callback\n",
        "        y_val_pred_proba = self.model.predict(self.val_data_for_pred[0], verbose=0)[:, 0]\n",
        "\n",
        "        # Using the default 0.5 threshold for F1 calculation in the callback\n",
        "        # y_train_pred_classes = (y_train_pred_proba >= 0.5).astype(int) # This was the line causing error\n",
        "        y_val_pred_classes = (y_val_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "        val_f1 = f1_score(self.val_data_for_pred[1], y_val_pred_classes, pos_label=self.pos_label, zero_division=0)\n",
        "        self.val_f1s.append(val_f1)     # Append F1 of the explicit validation_data set\n",
        "\n",
        "        # Optionally print the F1 score at the end of the epoch\n",
        "        # logs['f1_class_0_explicit'] = val_f1 # You could add this to logs if needed, but logging requires specific Keras versions/methods\n",
        "        print(f\" - explicit_val_f1_class_0: {val_f1:.4f}\") # Print explicitly\n",
        "\n",
        "\n",
        "def plot_learning_curves(history, f1_callback):\n",
        "    \"\"\"\n",
        "    Genera gráficos para las curvas de aprendizaje (pérdida y F1-Score).\n",
        "    \"\"\"\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Gráfico de Pérdida (Loss)\n",
        "    ax1.plot(history.history['loss'], label='Pérdida de Entrenamiento', color='dodgerblue')\n",
        "    # Check if validation loss is available (it should be with validation_data)\n",
        "    if 'val_loss' in history.history:\n",
        "        ax1.plot(history.history['val_loss'], label='Pérdida de Validación', color='darkorange', linestyle='--')\n",
        "    ax1.set_title('Curva de Pérdida del Modelo', fontsize=16)\n",
        "    ax1.set_xlabel('Época'); ax1.set_ylabel('Loss'); ax1.legend()\n",
        "\n",
        "    # Gráfico de F1-Score\n",
        "    # Use the recorded F1 scores from the callback for the explicit validation set\n",
        "    # We are no longer calculating/plotting a separate 'training' F1 in the callback for simplicity/speed\n",
        "    ax2.plot(f1_callback.val_f1s, label='F1-Score Validación (Clase 0 - Explicit)', color='darkorange', linestyle='--') # Clarify label\n",
        "    ax2.set_title('Curva de F1-Score del Modelo (Clase 0) en Validación', fontsize=16)\n",
        "    ax2.set_xlabel('Época'); ax2.set_ylabel('F1-Score'); ax2.legend()\n",
        "\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- Entrenando modelo final con los mejores hiperparámetros para visualización ---\")\n",
        "\n",
        "# 1. Extraer los mejores hiperparámetros\n",
        "# Assume best_params, X_train_scaled, y_train are defined from previous cells\n",
        "\n",
        "best_params = study_dnn.best_trial.params\n",
        "final_layer_conf = []\n",
        "for i in range(best_params['n_hidden_layers']):\n",
        "    final_layer_conf.append((best_params[f'n_units_l{i}'], best_params[f'dropout_l{i}']))\n",
        "\n",
        "# 2. Aplicar SMOTE al conjunto de entrenamiento completo\n",
        "sm = SMOTE(k_neighbors=best_params['smote_k'], random_state=42)\n",
        "X_train_final_res, y_train_final_res = sm.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Crear y compilar el modelo final\n",
        "\n",
        "final_model = create_dnn_model(\n",
        "    input_dim=X_train_final_res.shape[1],\n",
        "    optimizer_config=(best_params['optimizer'], best_params['lr']),\n",
        "    layer_config=tuple(final_layer_conf),\n",
        "    activation_config=best_params['activation']\n",
        ")\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_fit, X_val_fit, y_train_fit, y_val_fit = train_test_split(\n",
        "    X_train_final_res, y_train_final_res,\n",
        "    test_size=0.2, # Use 20% of the resampled data for validation\n",
        "    random_state=42,\n",
        "    stratify=y_train_final_res # Maintain class distribution\n",
        ")\n",
        "\n",
        "f1_callback_instance = F1ScoreCallback(validation_data=(X_val_fit, y_val_fit), pos_label=0)\n",
        "\n",
        "\n",
        "\n",
        "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=10, mode='min',\n",
        "    restore_best_weights=True, verbose=1 # Set verbose to 1 to see when it stops\n",
        ")\n",
        "\n",
        "\n",
        "# 4. Entrenar el modelo final, passing the callbacks\n",
        "history_final = final_model.fit(\n",
        "    X_train_fit, # Use the training split of the resampled data\n",
        "    y_train_fit, # Use the training split of the resampled data\n",
        "    epochs= best_params['epochs'],\n",
        "    batch_size=best_params['batch_size'],\n",
        "    validation_data=(X_val_fit, y_val_fit), # Use the validation split of the resampled data\n",
        "    callbacks=[f1_callback_instance, early_stopping_callback], # Pass the callback instance\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 5. Generar y mostrar los gráficos, passing both history and the callback instance\n",
        "plot_learning_curves(history_final, f1_callback_instance)"
      ],
      "metadata": {
        "id": "qYL2yZx4qFvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Predecir probabilidades\n",
        "y_pred_proba_dnn = final_model.predict(X_test_scaled).ravel()\n",
        "\n",
        "# 2. Usar threshold estándar (0.5)\n",
        "y_pred_dnn = (y_pred_proba_dnn >= 0.5).astype(int)\n",
        "\n",
        "# 3. Reporte de clasificación\n",
        "print(\"📊 Reporte de Clasificación (Prueba):\")\n",
        "print(classification_report(y_test, y_pred_dnn, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"], zero_division=0))\n",
        "\n",
        "# 4. Métricas generales\n",
        "print(f\" Accuracy: {accuracy_score(y_test, y_pred_dnn):.4f}\")\n",
        "print(f\"F1 Macro: {f1_score(y_test, y_pred_dnn, average='macro'):.4f}\")\n",
        "print(f\" AUC-ROC: {roc_auc_score(y_test, y_pred_proba_dnn):.4f}\")\n",
        "\n",
        "# 5. Matriz de Confusión\n",
        "cm = confusion_matrix(y_test, y_pred_dnn)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Reprobar (0)\", \"Aprobar (1)\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Matriz de Confusión - DNN\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RTJcsCFrb92v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# 1. Obtener las probabilidades para la clase positiva (Aprobar)\n",
        "y_pred_proba_test_dnn = final_model.predict(X_test_scaled).ravel()  # aplanar por si devuelve (N, 1)\n",
        "\n",
        "# 2. Definir thresholds a evaluar\n",
        "thresholds = np.arange(0.50, 0.81, 0.02)\n",
        "\n",
        "# 3. Evaluar para cada threshold\n",
        "for thresh in thresholds:\n",
        "    print(f\"\\n--- Threshold: {thresh:.2f} ---\")\n",
        "\n",
        "    # Convertir probabilidades en clases según el threshold\n",
        "    y_pred_thresh = (y_pred_proba_test_dnn >= thresh).astype(int)\n",
        "\n",
        "    # Matriz de confusión\n",
        "    cm = confusion_matrix(y_test, y_pred_thresh)\n",
        "\n",
        "    # Reporte de clasificación\n",
        "    print(\"Matriz de Confusión:\")\n",
        "    print(cm)\n",
        "    print(\"Reporte de Clasificación:\")\n",
        "    print(classification_report(y_test, y_pred_thresh, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"]))\n",
        "\n",
        "    # Accuracy y F1 Macro\n",
        "    acc = accuracy_score(y_test, y_pred_thresh)\n",
        "    f1 = f1_score(y_test, y_pred_thresh, average='macro')\n",
        "\n",
        "    # AUC (aunque no depende del threshold)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba_test_dnn)\n",
        "\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1 Macro: {f1:.4f}\")\n",
        "    print(f\"AUC-ROC (fijo): {auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "CQ_TUJDE4oLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ruta donde guardar en tu Google Drive (ajusta si es necesario)\n",
        "ruta_modelo_dnn = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano - Generación y Análisis de Secuencias de Actividad/logs/Modelos/DNN'\n",
        "os.makedirs(ruta_modelo_dnn, exist_ok=True)\n",
        "\n",
        "# Guardar el modelo en formato .h5\n",
        "ruta_modelo_h5 = os.path.join(ruta_modelo_dnn, 'modelo_dnn_optuna_068.h5')\n",
        "final_model.save(ruta_modelo_h5)\n",
        "\n",
        "print(f\"✅ Modelo DNN guardado exitosamente en: {ruta_modelo_h5}\")\n"
      ],
      "metadata": {
        "id": "s1A7pRUt6W3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.plot_param_importances(study_dnn)\n"
      ],
      "metadata": {
        "id": "HjwjfavrhHzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "F4RDUNhIrPWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input, Conv1D, MaxPooling1D, Flatten, Reshape\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import recall_score, classification_report, confusion_matrix, roc_auc_score, accuracy_score, f1_score, make_scorer\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "import traceback # Para imprimir el stack trace de errores\n",
        "import os # Para guardar gráficos y modelos\n",
        "\n",
        "\n",
        "# Convertir X_train_scaled y y_train a arrays de NumPy\n",
        "# Esto es CRUCIAL para evitar KeyErrors con Pandas DataFrames/Series y asegurar compatibilidad\n",
        "X_train_scaled_np = X_train_scaled.values if hasattr(X_train_scaled, 'values') else X_train_scaled\n",
        "y_train_np = y_train.values if hasattr(y_train, 'values') else y_train\n",
        "\n",
        "# Asegurarse de que los datos de prueba también estén en formato NumPy\n",
        "X_test_scaled_np = X_test_scaled.values if 'X_test_scaled' in locals() and hasattr(X_test_scaled, 'values') else X_test_scaled\n",
        "y_test_np = y_test.values if 'y_test' in locals() and hasattr(y_test, 'values') else y_test\n",
        "\n",
        "\n",
        "# Validación de formas básicas\n",
        "if X_train_scaled_np.ndim != 2:\n",
        "    print(f\"ERROR: X_train_scaled debe ser 2D. Forma actual: {X_train_scaled_np.shape}\")\n",
        "    exit()\n",
        "if y_train_np.ndim != 1:\n",
        "    print(f\"ERROR: y_train debe ser 1D. Forma actual: {y_train_np.shape}\")\n",
        "    exit()\n",
        "\n",
        "print(\"Bloque 1: Importaciones y preparación de datos completados.\")"
      ],
      "metadata": {
        "id": "12CFqhwzsorf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 1: Función para Crear el Modelo CNN ---\n",
        "def create_cnn_model_v2(input_shape_features, layer_config_cnn, dense_units_post_cnn,\n",
        "                        activation_cnn, dropout_rate_dense, optimizer_config):\n",
        "    \"\"\"\n",
        "    Crea y compila un modelo de red neuronal convolucional (CNN) adaptado para datos tabulares.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Input(shape=(input_shape_features,)))\n",
        "    model.add(Reshape((input_shape_features, 1)))\n",
        "\n",
        "    for i, (filters, kernel_size, dropout_conv) in enumerate(layer_config_cnn):\n",
        "        if filters > 0:\n",
        "            model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation_cnn, padding='same'))\n",
        "            if dropout_conv > 0:\n",
        "                model.add(Dropout(dropout_conv))\n",
        "            model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    if dense_units_post_cnn > 0:\n",
        "        model.add(Dense(dense_units_post_cnn, activation=activation_cnn))\n",
        "        if dropout_rate_dense > 0:\n",
        "            model.add(Dropout(dropout_rate_dense))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer_name, learning_rate = optimizer_config\n",
        "    if optimizer_name.lower() == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name.lower() == 'rmsprop':\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "    else:\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # Para el entrenamiento de la CNN, Keras no tiene F1-Score nativo por class_id.\n",
        "    # Monitorearemos 'recall' o 'accuracy' o 'loss' y calcularemos F1 externamente.\n",
        "    # Si quieres monitorear recall_clase_0, sería:\n",
        "    # recall_clase_0_metric = keras.metrics.Recall(class_id=0, name=\"recall_clase_0\")\n",
        "    # metrics=['accuracy', recall_clase_0_metric]\n",
        "    # En este caso, para optimizar F1-Score en Optuna, lo calculamos manualmente en objective_cnn.\n",
        "    # Las métricas en compile son solo para el monitoreo interno del modelo durante fit().\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy']) # Solo accuracy para simplificar las métricas internas de Keras\n",
        "    return model\n",
        "\n",
        "# --- Callback personalizado para calcular F1-Score en cada época ---\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(self, X_val, y_val, pos_label=0):\n",
        "        super().__init__()\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.pos_label = pos_label\n",
        "        self.train_f1s = []\n",
        "        self.val_f1s = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Calcular F1-Score para el conjunto de validación\n",
        "        y_pred_val_proba = self.model.predict(self.X_val, verbose=0)\n",
        "        y_pred_val_classes = (y_pred_val_proba > 0.5).astype(\"int32\")\n",
        "        val_f1 = f1_score(self.y_val, y_pred_val_classes, pos_label=self.pos_label, zero_division=0)\n",
        "        self.val_f1s.append(val_f1)\n",
        "\n",
        "        # NOTE: Calculating training F1-score here is complex and slow if not passed through metrics.\n",
        "        # For simplicity in plotting, we'll append a placeholder or 0.0 for training F1.\n",
        "        self.train_f1s.append(0.0) # Placeholder, as it's hard to get true training F1 efficiently here\n",
        "\n",
        "        # Puedes imprimir para ver el progreso si verbose>0 en fit()\n",
        "        # print(f\" - epoch {epoch+1}: val_f1_clase_0: {val_f1:.4f}\")\n",
        "\n",
        "print(\"Bloque 2: Función 'create_cnn_model_v2' y 'F1ScoreCallback' definidas.\")"
      ],
      "metadata": {
        "id": "wItwr29g2z24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 2: Definir la Función Objetivo para Optuna (Optimizando para F1-Score Clase 0) ---\n",
        "def objective_cnn(trial):\n",
        "    K.clear_session()\n",
        "\n",
        "    smote_k = trial.suggest_categorical('smote_k', [3, 5, 7, 9])\n",
        "\n",
        "    n_conv_layers = trial.suggest_int('n_conv_layers', 1, 2)\n",
        "    # Lista para construir la configuración de las capas convolucionales\n",
        "    layer_config_cnn_list = []\n",
        "    for i in range(n_conv_layers):\n",
        "        filters = trial.suggest_categorical(f'filters_l{i}', [16, 32, 64, 128])\n",
        "        kernel_size = trial.suggest_categorical(f'kernel_size_l{i}', [2, 3])\n",
        "        dropout_conv = trial.suggest_float(f'dropout_conv_l{i}', 0.2, 0.5, step=0.1)\n",
        "        layer_config_cnn_list.append((filters, kernel_size, dropout_conv))\n",
        "\n",
        "    dense_units = trial.suggest_categorical('dense_units', [16, 32, 64])\n",
        "    dropout_rate_dense = trial.suggest_float('dropout_dense', 0.1, 0.3, step=0.1)\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop'])\n",
        "    lr = trial.suggest_float('lr', 1e-4, 5e-3, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
        "\n",
        "    epochs_max = trial.suggest_categorical('epochs_max', [50, 100, 150, 200])\n",
        "    early_stopping_patience = trial.suggest_int('early_stopping_patience', 10, 25)\n",
        "\n",
        "    sm = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "    X_res, y_res = sm.fit_resample(X_train_scaled_np, y_train_np)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores_fold = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_res, y_res)):\n",
        "        X_tr_fold, X_val_fold = X_res[train_idx], X_res[val_idx]\n",
        "        y_tr_fold, y_val_fold = y_res[train_idx], y_res[val_idx]\n",
        "\n",
        "        # --- ¡CORRECCIÓN AQUÍ! Pasar los parámetros directamente, no como un diccionario 'config' ---\n",
        "        model = create_cnn_model_v2(\n",
        "            input_shape_features=X_tr_fold.shape[1],\n",
        "            layer_config_cnn=tuple(layer_config_cnn_list), # Asegurar que sea tupla\n",
        "            dense_units_post_cnn=dense_units,\n",
        "            activation_cnn=activation,\n",
        "            dropout_rate_dense=dropout_rate_dense,\n",
        "            optimizer_config=(optimizer_name, lr)\n",
        "        )\n",
        "\n",
        "        early_stopping_cb_trial = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=early_stopping_patience,\n",
        "            mode='min',\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        model.fit(X_tr_fold, y_tr_fold,\n",
        "                  epochs=epochs_max,\n",
        "                  batch_size=batch_size,\n",
        "                  validation_data=(X_val_fold, y_val_fold),\n",
        "                  callbacks=[early_stopping_cb_trial],\n",
        "                  verbose=0)\n",
        "\n",
        "        y_val_pred_proba = model.predict(X_val_fold, verbose=0)\n",
        "        y_val_pred_classes = (y_val_pred_proba > 0.5).astype(\"int32\")\n",
        "\n",
        "        f1 = f1_score(y_val_fold, y_val_pred_classes, pos_label=0, zero_division=0)\n",
        "        f1_scores_fold.append(f1)\n",
        "\n",
        "        trial.report(f1, fold_idx)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(f1_scores_fold)\n",
        "\n",
        "print(\" Bloque 3: Función 'objective_cnn' definida.\")"
      ],
      "metadata": {
        "id": "qVbSsMdWzK75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 3: Crear un Estudio de Optuna y Ejecutar la Optimización ---\n",
        "study_name = \"CNN_F1_Score_Optuna\" # Nombre del estudio enfocado en F1-Score Clase 0\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0, interval_steps=1)\n",
        "\n",
        "study_cnn_optuna = optuna.create_study(direction=\"maximize\", study_name=study_name, sampler=sampler, pruner=pruner)\n",
        "\n",
        "n_trials_optuna = 100 # Número de trials\n",
        "timeout_seconds = 3600 # 2 horas de timeout (ajusta según tus recursos y paciencia)\n",
        "\n",
        "print(f\"🔁 Iniciando optimización con Optuna para Keras CNN ({n_trials_optuna} trials, timeout: {timeout_seconds/60} min)...\")\n",
        "try:\n",
        "    study_cnn_optuna.optimize(objective_cnn, n_trials=n_trials_optuna, timeout=timeout_seconds)\n",
        "    print(\"Optimización con Optuna para CNN Keras completada.\")\n",
        "\n",
        "    print(f\"\\n Mejores hiperparámetros CNN (Optimizados para F1-Score Clase 0): {study_cnn_optuna.best_trial.params}\")\n",
        "    print(f\" Mejor F1-Score clase 0 (promedio CV): {study_cnn_optuna.best_value:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Ocurrió un error INESPERADO durante la optimización con Optuna:\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n Bloque 4: Optimización Optuna completada.\")"
      ],
      "metadata": {
        "id": "sEAH9WjgzNmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 4: Crear y Entrenar el Modelo Final con los Mejores Hiperparámetros ---\n",
        "# Este bloque asume que 'study_cnn_optuna' está definido y que se encontró un best_trial.\n",
        "\n",
        "best_params = study_cnn_optuna.best_trial.params\n",
        "\n",
        "# Preparar configuración de capas CNN\n",
        "layer_config_cnn_final = []\n",
        "n_conv_layers_final = best_params.get('n_conv_layers', 1)\n",
        "for i in range(n_conv_layers_final):\n",
        "    filters = best_params.get(f'filters_l{i}', 32)\n",
        "    kernel_size = best_params.get(f'kernel_size_l{i}', 2)\n",
        "    dropout_conv = best_params.get(f'dropout_conv_l{i}', 0.2)\n",
        "    layer_config_cnn_final.append((filters, kernel_size, dropout_conv))\n",
        "\n",
        "# Crear la instancia del modelo CNN final\n",
        "final_model = create_cnn_model_v2(\n",
        "    input_shape_features=X_train_scaled_np.shape[1],\n",
        "    layer_config_cnn=tuple(layer_config_cnn_final),\n",
        "    dense_units_post_cnn=best_params['dense_units'],\n",
        "    activation_cnn=best_params['activation'],\n",
        "    dropout_rate_dense=best_params['dropout_dense'],\n",
        "    optimizer_config=(best_params['optimizer'], best_params['lr']) # Usar 'optimizer' y 'lr' de los HPs\n",
        ")\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento completo\n",
        "smote_final_k = best_params.get('smote_k', 5) # Usar 'smote_k' que es el nombre del parámetro\n",
        "smote_final = SMOTE(k_neighbors=smote_final_k, random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote_final.fit_resample(X_train_scaled_np, y_train_np.astype(int))\n",
        "\n",
        "# Configurar Early Stopping para el entrenamiento final\n",
        "early_stopping_final_cb = EarlyStopping(\n",
        "    monitor='val_loss', # Monitorear val_loss, ya que F1-Score no está directamente en metrics\n",
        "    patience=10, #best_params['early_stopping_patience'],\n",
        "    mode='min', # Minimizamos la pérdida\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Dividir para la validación interna del modelo final (para EarlyStopping y F1 callback)\n",
        "X_train_fit, X_val_fit, y_train_fit, y_val_fit = train_test_split(\n",
        "    X_train_resampled, y_train_resampled, test_size=0.2, random_state=42, stratify=y_train_resampled\n",
        ")\n",
        "\n",
        "# Instanciar el F1ScoreCallback para el entrenamiento final\n",
        "f1_callback_instance = F1ScoreCallback(X_val=X_val_fit, y_val=y_val_fit, pos_label=0)\n",
        "\n",
        "print(f\"\\nEntrenando el modelo final CNN con los mejores hiperparámetros (optimizados para F1-Score): {best_params}\")\n",
        "# Entrenar el modelo final y CAPTURAR EL HISTORIAL en la variable 'history'\n",
        "history = final_model.fit(\n",
        "    X_train_fit,\n",
        "    y_train_fit,\n",
        "    epochs=best_params['epochs_max'], # Usar el máximo de epochs\n",
        "    batch_size=best_params['batch_size'],\n",
        "    validation_data=(X_val_fit, y_val_fit), # Usar el conjunto de validación explícito\n",
        "    callbacks=[early_stopping_final_cb, f1_callback_instance], # Ambos callbacks\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Bloque 5: Modelo CNN final entrenado y historial capturado.\")"
      ],
      "metadata": {
        "id": "N-awFOwzBSd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 5: Evaluación en el conjunto de prueba ---\n",
        "# Este bloque asume que 'final_model' está definido y entrenado del Bloque 5.\n",
        "\n",
        "y_pred_probs = final_model.predict(X_test_scaled_np, verbose=0)\n",
        "y_pred_probs_flat = y_pred_probs.flatten()\n",
        "\n",
        "print(\"\\n📊 Bloque 6: Evaluación del Modelo CNN Final con distintos umbrales en el conjunto de prueba:\\n\")\n",
        "for threshold in np.arange(0.50, 0.81, 0.02):\n",
        "    y_pred_thresholded = (y_pred_probs_flat >= threshold).astype(int)\n",
        "\n",
        "    print(f\" Umbral = {threshold:.2f}\")\n",
        "    print(classification_report(y_test_np, y_pred_thresholded, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"], zero_division=0))\n",
        "    print(\" Matriz de Confusión:\")\n",
        "    print(confusion_matrix(y_test_np, y_pred_thresholded, labels=[0, 1]))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Métricas Globales (independientes del umbral, evalúan la capacidad de ranking)\n",
        "print(\"\\n--- Métricas Globales del Modelo (independientes del umbral) ---\")\n",
        "auc = roc_auc_score(y_test_np, y_pred_probs_flat)\n",
        "print(f\" ROC AUC: {auc:.4f}\")\n",
        "print(\" Bloque 6: Evaluación en el conjunto de prueba completada.\")"
      ],
      "metadata": {
        "id": "U2dt5ER5zPqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 6: Graficar el Historial de Entrenamiento para Diagnóstico de Overfitting/Underfitting ---\n",
        "# Este bloque asume que 'history' y 'f1_callback_instance' están definidos del Bloque 5.\n",
        "\n",
        "print(\"\\n Bloque 7: Generando gráficos del historial de entrenamiento para diagnosticar Overfitting/Underfitting...\")\n",
        "\n",
        "# Define la ruta para guardar los gráficos\n",
        "ruta_carpeta_graficos = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano - Generación y Análisis de Secuencias de Actividad/logs/Modelos/CNN/graficos/'\n",
        "os.makedirs(ruta_carpeta_graficos, exist_ok=True)\n",
        "\n",
        "# Función para graficar las curvas de aprendizaje (adaptada para F1-Score)\n",
        "def plot_learning_curves(history_obj, f1_callback_obj, best_params_for_title=None, save_path=None):\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    if best_params_for_title:\n",
        "        title_str = \"Curvas de Aprendizaje CNN (F1-Score Clase 0)\\n\"\n",
        "        title_str += f\"LR: {best_params_for_title.get('lr'):.1e}, Opt: {best_params_for_title.get('optimizer')}, \"\n",
        "        title_str += f\"ConvLayers: {best_params_for_title.get('n_conv_layers')}, SMOTE_k: {best_params_for_title.get('smote_k')}\"\n",
        "        fig.suptitle(title_str, fontsize=18, y=1.03)\n",
        "\n",
        "    # Gráfico de Pérdida (Loss)\n",
        "    ax1.plot(history_obj.history['loss'], label='Pérdida de Entrenamiento', color='dodgerblue')\n",
        "    if 'val_loss' in history_obj.history:\n",
        "        ax1.plot(history_obj.history['val_loss'], label='Pérdida de Validación', color='darkorange', linestyle='--')\n",
        "    ax1.set_title('Curva de Pérdida del Modelo', fontsize=16)\n",
        "    ax1.set_xlabel('Época'); ax1.set_ylabel('Loss'); ax1.legend()\n",
        "\n",
        "    # Gráfico de F1-Score (desde el callback personalizado)\n",
        "    # Nota: f1_callback_obj.train_f1s en este caso es un placeholder (0.0)\n",
        "    # Si quieres el F1 de entrenamiento real, necesitarías una implementación más compleja.\n",
        "    ax2.plot(f1_callback_obj.train_f1s, label='F1-Score Entrenamiento (Clase 0 - Placeholder)', color='lightgray', linestyle=':')\n",
        "    ax2.plot(f1_callback_obj.val_f1s, label='F1-Score Validación (Clase 0)', color='darkorange', linestyle='--')\n",
        "    ax2.set_title('Curva de F1-Score del Modelo (Clase 0)', fontsize=16)\n",
        "    ax2.set_xlabel('Época'); ax2.set_ylabel('F1-Score'); ax2.legend()\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]);\n",
        "    plt.show()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close(fig)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Llamada a la función de graficación\n",
        "plot_learning_curves(history, f1_callback_instance, best_params_for_title=best_params, save_path=os.path.join(ruta_carpeta_graficos, 'cnn_f1_learning_curves.png'))\n",
        "\n",
        "print(\"✅ Bloque 7: Gráficos generados y guardados en Google Drive.\")"
      ],
      "metadata": {
        "id": "4O0x3uDjCqmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 7: Guardar el Modelo Final ---\n",
        "# Este bloque asume que 'final_model' está definido y entrenado del Bloque 5.\n",
        "\n",
        "ruta_carpeta_modelo = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano - Generación y Análisis de Secuencias de Actividad/logs/Modelos/CNN/'\n",
        "os.makedirs(ruta_carpeta_modelo, exist_ok=True)\n",
        "\n",
        "nombre_archivo_modelo_cnn = 'modelo_cnn_f1_optuna_final_0.7.keras' # Nombre del modelo CNN optimizado para F1-Score\n",
        "ruta_modelo_completa_cnn = os.path.join(ruta_carpeta_modelo, nombre_archivo_modelo_cnn)\n",
        "\n",
        "final_model.save(ruta_modelo_completa_cnn)\n",
        "print(f\"Bloque 8: Modelo CNN entrenado y guardado en: {ruta_modelo_completa_cnn}\")"
      ],
      "metadata": {
        "id": "i0203iGwGv_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "OXdtcK3g_Hdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_consolidado2\n",
        "\n"
      ],
      "metadata": {
        "id": "nsB4OzoYG9zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_consolidado.columns"
      ],
      "metadata": {
        "id": "yBZesO0UHmD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_log_cols = [\n",
        "    'max_days_with_access', 'max_days_without_access', 'first_last_log_diff',\n",
        "    'logs', 'week_logs', 'daily_avg', 'weekly_avg', 'days_with_logs',\n",
        "    'days_with_logs_avg', 'days_with_logs_week', 'days_with_logs_avg_week',\n",
        "    'activity_total', 'content_total', 'other_total','report_total',\n",
        "    'system_total', 'activity_week', 'content_week', 'other_week',\n",
        "    'report_week', 'system_week', 'total_weeks', 'course_progress',\n",
        "    'longest_streak', 'proyeccion_nota_final', 'aprobando'\n",
        "]\n",
        "\n",
        "target_col = 'aprobo_semestre_real'\n"
      ],
      "metadata": {
        "id": "LlDMYyV7QWLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar columnas necesarias\n",
        "columnas_lstm = ['Nombre completo del usuario', 'semana_semestre', 'Semestre'] + features_log_cols + [target_col]\n",
        "df_lstm = df_consolidado[columnas_lstm].dropna(subset=[target_col])\n"
      ],
      "metadata": {
        "id": "IMhHnUynRt_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_lstm\n"
      ],
      "metadata": {
        "id": "1eiFzXUGVG46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the counts of unique values in the 'aprobo_semestre_real' column\n",
        "print(df_lstm['aprobo_semestre_real'].value_counts())\n",
        "\n",
        "# Optional: Check the normalized counts (proportions)\n",
        "print(\"\\nNormalized counts:\")\n",
        "print(df_lstm['aprobo_semestre_real'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "NLt_NVReViqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_lstm_grouped = df_lstm.groupby(['Nombre completo del usuario','Semestre','semana_semestre'])[features_log_cols].mean().reset_index()\n"
      ],
      "metadata": {
        "id": "y32UdxSdTM6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input, LSTM, Reshape # Agregada capa LSTM\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import recall_score, classification_report, confusion_matrix, roc_auc_score, accuracy_score, f1_score\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import tensorflow.keras.backend as K\n"
      ],
      "metadata": {
        "id": "T8JeS510do3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import StandardScaler # Importar StandardScaler\n",
        "\n",
        "# --- Definir las columnas clave para la creación de secuencias ---\n",
        "ID_COLS = ['Nombre completo del usuario', 'Semestre']\n",
        "TIME_COL = 'semana_semestre'\n",
        "TARGET_COL = 'aprobo_semestre_real'\n",
        "FEATURES_LOG_COLS = [\n",
        "    'max_days_with_access', 'max_days_without_access', 'first_last_log_diff',\n",
        "    'logs', 'week_logs', 'daily_avg', 'weekly_avg', 'days_with_logs',\n",
        "    'days_with_logs_avg', 'days_with_logs_week', 'days_with_logs_avg_week',\n",
        "    'activity_total', 'content_total', 'other_total','report_total',\n",
        "    'system_total', 'activity_week', 'content_week', 'other_week',\n",
        "    'report_week', 'system_week', 'total_weeks', 'course_progress',\n",
        "    'longest_streak', 'proyeccion_nota_final', 'aprobando'\n",
        "]\n",
        "\n",
        "# --- ASEGURAR QUE df_consolidado ESTÉ CARGADO Y DISPONIBLE ---\n",
        "\n",
        "\n",
        "print(f\"DEBUG: df_consolidado cargado. Filas iniciales: {len(df_consolidado)}\")\n",
        "print(f\"DEBUG: Columnas en df_consolidado: {df_consolidado.columns.tolist()}\")\n",
        "\n",
        "for col in ID_COLS + [TIME_COL] + [TARGET_COL]:\n",
        "    if col not in df_consolidado.columns:\n",
        "        print(f\" ERROR CRÍTICO: La columna '{col}' (ID/Tiempo/Target) NO se encontró en df_consolidado. ¡Revisa el nombre!\")\n",
        "        exit()\n",
        "\n",
        "all_cols_to_process_for_numeric = FEATURES_LOG_COLS + [TARGET_COL] + [TIME_COL]\n",
        "\n",
        "for col in all_cols_to_process_for_numeric:\n",
        "    if col in df_consolidado.columns:\n",
        "        original_dtype = df_consolidado[col].dtype\n",
        "        df_consolidado[col] = pd.to_numeric(df_consolidado[col], errors='coerce')\n",
        "        if df_consolidado[col].isnull().any() and not (pd.api.types.is_float_dtype(original_dtype) and df_consolidado[col].isnull().all()):\n",
        "            print(f\"DEBUG: Columna '{col}' ahora tiene {df_consolidado[col].isnull().sum()} NaNs después de la conversión a numérico. Posibles valores no numéricos originales.\")\n",
        "    elif col in FEATURES_LOG_COLS:\n",
        "        print(f\"Advertencia: La columna de característica '{col}' NO se encontró en df_consolidado. ¡Será ignorada!\")\n",
        "\n",
        "print(\" Bloque 1: Importaciones y definición de columnas completadas.\")\n",
        "print(f\"Número de filas en df_consolidado después de verificación y conversión numérica: {len(df_consolidado)}\")"
      ],
      "metadata": {
        "id": "qIZga007t8gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Ordenar y Manejar NaN's dentro de las Secuencias (¡CRÍTICO ANTES DEL ESCALADO!) ---\n",
        "print(\"DEBUG: Iniciando ordenamiento y manejo de NaNs...\")\n",
        "df_consolidado_original_len_b2 = len(df_consolidado)\n",
        "\n",
        "df_consolidado = df_consolidado.sort_values(by=ID_COLS + [TIME_COL]).reset_index(drop=True)\n",
        "\n",
        "cols_for_ffill = [\n",
        "    'max_days_with_access', 'max_days_without_access', 'longest_streak',\n",
        "    'promedio_ponderado', 'proyeccion_nota_final', 'aprobando'\n",
        "]\n",
        "\n",
        "for col in FEATURES_LOG_COLS:\n",
        "    if col not in df_consolidado.columns:\n",
        "        continue\n",
        "\n",
        "    if col in cols_for_ffill:\n",
        "        nan_before_ffill = df_consolidado[col].isnull().sum()\n",
        "        df_consolidado[col] = df_consolidado.groupby(ID_COLS)[col].ffill()\n",
        "        nan_after_ffill = df_consolidado[col].isnull().sum()\n",
        "        if nan_before_ffill > nan_after_ffill:\n",
        "            print(f\"DEBUG: ffill aplicado a '{col}'. NaNs restantes: {nan_after_ffill}\")\n",
        "\n",
        "    nan_before_fillna = df_consolidado[col].isnull().sum()\n",
        "    df_consolidado[col] = df_consolidado[col].fillna(0)\n",
        "    if nan_before_fillna > df_consolidado[col].isnull().sum():\n",
        "        print(f\"DEBUG: '{col}' rellenado con 0. NaNs restantes: {df_consolidado[col].isnull().sum()}\")\n",
        "\n",
        "target_nan_count = df_consolidado[TARGET_COL].isnull().sum()\n",
        "df_consolidado[TARGET_COL] = df_consolidado[TARGET_COL].fillna(0).astype(int)\n",
        "if target_nan_count > 0:\n",
        "    print(f\"DEBUG: {target_nan_count} NaNs en columna target '{TARGET_COL}' rellenados con 0 y convertidos a int.\")\n",
        "\n",
        "print(\" Bloque 2: Preprocesamiento de datos temporales y manejo de NaN's completado.\")\n",
        "print(f\"Número de filas en df_consolidado después de preprocesamiento: {len(df_consolidado)}\")\n",
        "\n",
        "if df_consolidado.empty:\n",
        "    print(\" ERROR CRÍTICO: df_consolidado está VACÍO después del preprocesamiento. No se pueden crear secuencias.\")"
      ],
      "metadata": {
        "id": "QlYEiOOjt-6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Aplicación de Escalado a las Características ---\n",
        "print(\"DEBUG: Iniciando escalado de FEATURES_LOG_COLS...\")\n",
        "\n",
        "# Seleccionar solo las columnas de features_log_cols que realmente existen en df_consolidado\n",
        "actual_features_to_scale = [f for f in FEATURES_LOG_COLS if f in df_consolidado.columns]\n",
        "\n",
        "if not actual_features_to_scale:\n",
        "    print(\" ERROR: No hay características válidas para escalar en df_consolidado.\")\n",
        "    # No se puede escalar, y las variables escaladas estarán vacías.\n",
        "else:\n",
        "    scaler_lstm = StandardScaler() # Se crea el escalador\n",
        "\n",
        "    # Aplicar fit_transform SOLO a las filas y columnas relevantes para escalado\n",
        "    # Asegúrate de no escalar la columna del target o las IDs\n",
        "    df_consolidado[actual_features_to_scale] = scaler_lstm.fit_transform(df_consolidado[actual_features_to_scale])\n",
        "\n",
        "    print(f\"DEBUG: Características escaladas: {actual_features_to_scale}\")\n",
        "    print(\"Bloque 3: Escalado de características completado.\")\n",
        "\n",
        "print(f\"Número de filas en df_consolidado después de escalado: {len(df_consolidado)}\")"
      ],
      "metadata": {
        "id": "T6_JPtZPuA2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Creación de Secuencias y Aplicación de Padding ---\n",
        "print(\"DEBUG: Iniciando creación de secuencias con agrupación por usuario Y semestre...\")\n",
        "X_sequences = []\n",
        "y_labels = []\n",
        "sequence_ids_list = [] # Opcional: para depuración o trazabilidad\n",
        "\n",
        "# Puedes definir una longitud máxima fija si sabes que tus semestres tienen una duración máxima\n",
        "# De lo contrario, pad_sequences usará la longitud de la secuencia más larga encontrada.\n",
        "MAX_WEEKS_FOR_LSTM = 15 # Ejemplo: Puedes ajustar esto a 15, 20 o el máximo real de semanas por semestre.\n",
        "\n",
        "# Reinicializar variables para el caso de no encontrar secuencias\n",
        "LSTM_MAX_SEQUENCE_LENGTH = MAX_WEEKS_FOR_LSTM # Usamos la longitud fija\n",
        "LSTM_NUM_FEATURES_PER_TIMESTEP = 0\n",
        "X_padded_sequences = np.array([])\n",
        "y_labels_np = np.array([])\n",
        "\n",
        "if df_consolidado.empty:\n",
        "    print(\"ERROR CRÍTICO: df_consolidado está VACÍO. No se pueden crear secuencias.\")\n",
        "else:\n",
        "    # Usar la lista de features que realmente se escalaron y existen\n",
        "    current_features_for_lstm = [f for f in FEATURES_LOG_COLS if f in df_consolidado.columns]\n",
        "    if not current_features_for_lstm:\n",
        "        print(\" ERROR CRÍTICO: Ninguna de las columnas en FEATURES_LOG_COLS se encontró en df_consolidado. No se pueden crear secuencias.\")\n",
        "    else:\n",
        "        # ¡¡¡CORRECCIÓN CLAVE: Agrupar por ID_COLS (Nombre completo del usuario, Semestre)!!!\n",
        "        grouped_sequences = df_consolidado.groupby(ID_COLS, dropna=False)\n",
        "\n",
        "        print(f\"DEBUG: Número de grupos (secuencias únicas por usuario-semestre) identificados: {len(grouped_sequences)}\")\n",
        "\n",
        "        if len(grouped_sequences) == 0:\n",
        "            print(\" ERROR CRÍTICO: No se encontraron grupos válidos al agrupar df_consolidado. Revisa ID_COLS y los datos.\")\n",
        "        else:\n",
        "            for name, group in grouped_sequences:\n",
        "                if group.empty or TIME_COL not in group.columns or not current_features_for_lstm:\n",
        "                    print(f\"DEBUG: Grupo vacío o sin columnas esenciales para la secuencia: {name}. Saltando.\")\n",
        "                    continue\n",
        "\n",
        "                group_sorted = group.sort_values(by=TIME_COL)\n",
        "                features_data = group_sorted[current_features_for_lstm].values\n",
        "\n",
        "                if len(features_data) >= 3:\n",
        "                    final_label = group_sorted[TARGET_COL].iloc[-1]\n",
        "                    X_sequences.append(features_data)\n",
        "                    y_labels.append(final_label)\n",
        "                    sequence_ids_list.append(name)\n",
        "\n",
        "            if not X_sequences:\n",
        "                print(\"ERROR CRÍTICO: X_sequences está VACÍA después de procesar los grupos y aplicar el filtro de longitud. No se pudo crear ninguna secuencia válida.\")\n",
        "            else:\n",
        "                # --- Padding de las Secuencias (Usando tu MAX_WEEKS_FOR_LSTM fijo) ---\n",
        "                print(f\"DEBUG: Aplicando padding a una longitud fija de {MAX_WEEKS_FOR_LSTM} semanas.\")\n",
        "\n",
        "                X_padded_sequences = pad_sequences(X_sequences,\n",
        "                                                   maxlen=MAX_WEEKS_FOR_LSTM,\n",
        "                                                   dtype='float32',\n",
        "                                                   padding='post',\n",
        "                                                   truncating='post',\n",
        "                                                   value=0.0)\n",
        "\n",
        "                y_labels_np = np.array(y_labels)\n",
        "\n",
        "                # El número de características por paso de tiempo se toma de la secuencia ya rellenada\n",
        "                LSTM_NUM_FEATURES_PER_TIMESTEP = X_padded_sequences.shape[2]\n",
        "\n",
        "                print(f\"Forma de X_padded_sequences para LSTM: {X_padded_sequences.shape}\")\n",
        "                print(f\"Forma de y_labels (NumPy array) para LSTM: {y_labels_np.shape}\")\n",
        "\n",
        "\n",
        "print(\"Bloque 4: Creación de secuencias y aplicación de padding completados.\")"
      ],
      "metadata": {
        "id": "4vhbqs1buFoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. División del Dataset para LSTM ---\n",
        "if X_padded_sequences.size == 0 or y_labels_np.size == 0:\n",
        "    print(\" ERROR CRÍTICO: Los datos para LSTM están vacíos. No se puede realizar la división.\")\n",
        "    X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = (np.array([]), np.array([]), np.array([]), np.array([]))\n",
        "else:\n",
        "    if len(np.unique(y_labels_np)) < 2:\n",
        "        print(\" ERROR: y_labels_np tiene menos de 2 clases únicas después del preprocesamiento. No se puede estratificar. Dividiendo sin estratificar.\")\n",
        "        X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(\n",
        "            X_padded_sequences, y_labels_np, test_size=0.2, random_state=42\n",
        "        )\n",
        "    else:\n",
        "        X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(\n",
        "            X_padded_sequences, y_labels_np, test_size=0.2, random_state=42, stratify=y_labels_np\n",
        "        )\n",
        "\n",
        "    print(f\"\\nForma de X_train_lstm: {X_train_lstm.shape}\")\n",
        "    print(f\"Forma de X_test_lstm: {X_test_lstm.shape}\")\n",
        "    print(f\"Forma de y_train_lstm: {y_train_lstm.shape}\")\n",
        "    print(f\"Forma de y_test_lstm: {y_test_lstm.shape}\")\n",
        "\n",
        "print(\"Bloque 5: División del dataset para LSTM completada.\")"
      ],
      "metadata": {
        "id": "ukYTC5Ok8IO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 1: Función para Crear el Modelo LSTM ---\n",
        "def create_lstm_model(input_shape, config):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Input(shape=input_shape))\n",
        "\n",
        "    for i in range(config['n_lstm_layers']):\n",
        "        model.add(LSTM(config[f'lstm_units_l{i}'], return_sequences=(i < config['n_lstm_layers'] - 1)))\n",
        "        if config[f'dropout_lstm_l{i}'] > 0:\n",
        "            model.add(Dropout(config[f'dropout_lstm_l{i}']))\n",
        "\n",
        "    if config['n_dense_layers'] > 0:\n",
        "        for i in range(config['n_dense_layers']):\n",
        "            model.add(Dense(config[f'dense_units_l{i}'], activation=config['activation']))\n",
        "            if config[f'dropout_dense_l{i}'] > 0:\n",
        "                model.add(Dropout(config[f'dropout_dense_l{i}']))\n",
        "    else:\n",
        "        model.add(Dense(32, activation=config['activation']))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer_name, learning_rate = config['optimizer_name'], config['lr']\n",
        "    if optimizer_name.lower() == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name.lower() == 'rmsprop':\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "    else:\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# --- Callback personalizado para calcular F1-Score en cada época ---\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(self, X_val, y_val, pos_label=0):\n",
        "        super().__init__()\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.pos_label = pos_label\n",
        "        self.train_f1s = []\n",
        "        self.val_f1s = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        y_pred_val_proba = self.model.predict(self.X_val, verbose=0)\n",
        "        y_pred_val_classes = (y_pred_val_proba > 0.5).astype(\"int32\")\n",
        "        val_f1 = f1_score(self.y_val, y_pred_val_classes, pos_label=self.pos_label, zero_division=0)\n",
        "        self.val_f1s.append(val_f1)\n",
        "        self.train_f1s.append(0.0)\n",
        "\n",
        "print(\"Bloque 6: Función 'create_lstm_model' y 'F1ScoreCallback' definidas.\")"
      ],
      "metadata": {
        "id": "hJy7tBOdum2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 2: Definir la Función Objetivo para Optuna (Optimizando para F1-Score Clase 0) ---\n",
        "# Esta función ahora recibe X_train_data y y_train_data como argumentos.\n",
        "def objective_lstm(trial, X_train_data, y_train_data):\n",
        "    K.clear_session()\n",
        "\n",
        "    smote_k = trial.suggest_categorical('smote_k', [3, 5, 7, 9])\n",
        "\n",
        "    n_lstm_layers = trial.suggest_int('n_lstm_layers', 1, 2)\n",
        "    lstm_layer_configs = {}\n",
        "    for i in range(n_lstm_layers):\n",
        "        lstm_layer_configs[f'lstm_units_l{i}'] = trial.suggest_categorical(f'lstm_units_l{i}', [32, 64, 96, 128])\n",
        "        lstm_layer_configs[f'dropout_lstm_l{i}'] = trial.suggest_float(f'dropout_lstm_l{i}', 0.2, 0.5, step=0.1)\n",
        "\n",
        "    n_dense_layers = trial.suggest_int('n_dense_layers', 0, 1)\n",
        "    dense_layer_configs = {}\n",
        "    if n_dense_layers > 0:\n",
        "        for i in range(n_dense_layers):\n",
        "            dense_layer_configs[f'dense_units_l{i}'] = trial.suggest_categorical(f'dense_units_l{i}', [32, 64, 128])\n",
        "            dense_layer_configs[f'dropout_dense_l{i}'] = trial.suggest_float(f'dropout_dense_l{i}', 0.2, 0.5, step=0.1)\n",
        "\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
        "    optimizer_name = trial.suggest_categorical('optimizer_name', ['Adam', 'RMSprop'])\n",
        "    lr = trial.suggest_float('lr', 1e-4, 5e-3, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
        "\n",
        "    epochs_max = trial.suggest_categorical('epochs_max', [50, 100, 150])\n",
        "    early_stopping_patience = trial.suggest_int('early_stopping_patience', 10, 30)\n",
        "\n",
        "    sm = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores_fold = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train_data, y_train_data)):\n",
        "        X_tr_fold, X_val_fold = X_train_data[train_idx], X_train_data[val_idx]\n",
        "        y_tr_fold, y_val_fold = y_train_data[train_idx], y_train_data[val_idx]\n",
        "\n",
        "        # Aplicar SMOTE al conjunto de entrenamiento del fold (aplanando temporalmente)\n",
        "        original_shape_X_tr_fold = X_tr_fold.shape\n",
        "        X_tr_fold_flattened = X_tr_fold.reshape(original_shape_X_tr_fold[0], -1)\n",
        "\n",
        "        try:\n",
        "            X_tr_fold_res_flat, y_tr_fold_res = sm.fit_resample(X_tr_fold_flattened, y_tr_fold)\n",
        "        except ValueError as e:\n",
        "            print(f\"Trial {trial.number}, Fold {fold_idx}: SMOTE falló ({e}). Retornando 0.0.\")\n",
        "            return 0.0\n",
        "\n",
        "        # Volver a dar forma 3D a los datos después de SMOTE para el modelo LSTM\n",
        "        X_tr_fold_res = X_tr_fold_res_flat.reshape(y_tr_fold_res.shape[0], original_shape_X_tr_fold[1], original_shape_X_tr_fold[2])\n",
        "\n",
        "        model_config = {\n",
        "            'n_lstm_layers': n_lstm_layers,\n",
        "            **lstm_layer_configs,\n",
        "            'n_dense_layers': n_dense_layers,\n",
        "            **dense_layer_configs,\n",
        "            'activation': activation,\n",
        "            'optimizer_name': optimizer_name,\n",
        "            'lr': lr\n",
        "        }\n",
        "\n",
        "        # Crear la instancia del modelo LSTM\n",
        "        # input_shape para LSTM es (max_sequence_length, num_features_per_timestep)\n",
        "        model = create_lstm_model(input_shape=(X_train_data.shape[1], X_train_data.shape[2]), config=model_config)\n",
        "\n",
        "        early_stopping_cb_trial = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=early_stopping_patience,\n",
        "            mode='min',\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        model.fit(X_tr_fold_res, y_tr_fold_res,\n",
        "                  epochs=epochs_max,\n",
        "                  batch_size=batch_size,\n",
        "                  validation_data=(X_val_fold, y_val_fold),\n",
        "                  callbacks=[early_stopping_cb_trial],\n",
        "                  verbose=0)\n",
        "\n",
        "        y_val_pred_proba = model.predict(X_val_fold, verbose=0)\n",
        "        y_val_pred_classes = (y_val_pred_proba > 0.5).astype(\"int32\")\n",
        "\n",
        "        f1 = f1_score(y_val_fold, y_val_pred_classes, pos_label=0, zero_division=0)\n",
        "        f1_scores_fold.append(f1)\n",
        "\n",
        "        trial.report(f1, fold_idx)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(f1_scores_fold)\n",
        "\n",
        "print(\"Bloque 7: Función 'objective_lstm' definida.\")"
      ],
      "metadata": {
        "id": "IRjdwD0lvJzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 3: Crear un Estudio de Optuna y Ejecutar la Optimización ---\n",
        "study_name = \"LSTM_F1_Score_Optuna\"\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0, interval_steps=1)\n",
        "\n",
        "study_lstm_optuna = optuna.create_study(direction=\"maximize\", study_name=study_name, sampler=sampler, pruner=pruner)\n",
        "\n",
        "n_trials_optuna = 60\n",
        "timeout_seconds = 7200\n",
        "\n",
        "print(f\"🔁 Iniciando optimización con Optuna para Keras LSTM ({n_trials_optuna} trials, timeout: {timeout_seconds/60} min)...\")\n",
        "try:\n",
        "    # ¡¡¡CORRECCIÓN AQUÍ!!! Se pasan X_train_lstm y y_train_lstm como argumentos\n",
        "    # Esto asume que X_train_lstm y y_train_lstm se definieron en el Bloque 5.\n",
        "    study_lstm_optuna.optimize(lambda trial: objective_lstm(trial, X_train_lstm, y_train_lstm), n_trials=n_trials_optuna, timeout=timeout_seconds)\n",
        "    print(\"✅ Optimización con Optuna para LSTM Keras completada.\")\n",
        "\n",
        "    print(f\"\\nMejores hiperparámetros LSTM (Optimizados para F1-Score Clase 0): {study_lstm_optuna.best_trial.params}\")\n",
        "    print(f\" Mejor F1-Score clase 0 (promedio CV): {study_lstm_optuna.best_value:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Ocurrió un error INESPERADO durante la optimización con Optuna:\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n Bloque 8: Optimización Optuna completada.\")"
      ],
      "metadata": {
        "id": "Y3WsHbBxvM6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 4: Crear y Entrenar el Modelo Final con los Mejores Hiperparámetros ---\n",
        "# Este bloque asume que 'study_lstm_optuna' está definido y que se encontró un best_trial.\n",
        "\n",
        "best_params = study_lstm_optuna.best_trial.params\n",
        "\n",
        "# Reconstruir la configuración de las capas LSTM\n",
        "lstm_layer_configs_final = []\n",
        "n_lstm_layers_final = best_params.get('n_lstm_layers', 1)\n",
        "for i in range(n_lstm_layers_final):\n",
        "    lstm_layer_configs_final.append({\n",
        "        'units': best_params[f'lstm_units_l{i}'],\n",
        "        'dropout': best_params[f'dropout_lstm_l{i}']\n",
        "    })\n",
        "\n",
        "# Reconstruir la configuración de las capas Densas\n",
        "dense_layer_configs_final = []\n",
        "n_dense_layers_final = best_params.get('n_dense_layers', 0)\n",
        "if n_dense_layers_final > 0:\n",
        "    for i in range(n_dense_layers_final):\n",
        "        dense_layer_configs_final.append({\n",
        "            'units': best_params[f'dense_units_l{i}'],\n",
        "            'dropout': best_params[f'dropout_dense_l{i}']\n",
        "        })\n",
        "\n",
        "# Crear la instancia del modelo LSTM final\n",
        "final_model = Sequential()\n",
        "# Input shape para LSTM es (max_sequence_length, num_features_per_timestep)\n",
        "# Lo obtenemos de X_train_lstm (que es de la división del Bloque 5).\n",
        "final_model.add(Masking(mask_value=0.0, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]))) # Capa Masking para ceros de padding\n",
        "\n",
        "for i, layer_config in enumerate(lstm_layer_configs_final):\n",
        "    final_model.add(LSTM(layer_config['units'], return_sequences=(i < n_lstm_layers_final - 1)))\n",
        "    if layer_config['dropout'] > 0:\n",
        "        final_model.add(Dropout(layer_config['dropout']))\n",
        "\n",
        "if n_dense_layers_final > 0:\n",
        "    for i, layer_config in enumerate(dense_layer_configs_final):\n",
        "        final_model.add(Dense(layer_config['units'], activation=best_params['activation']))\n",
        "        if layer_config['dropout'] > 0:\n",
        "            final_model.add(Dropout(layer_config['dropout']))\n",
        "else:\n",
        "    final_model.add(Dense(32, activation=best_params['activation']))\n",
        "    final_model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "final_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer_cls_final = getattr(keras.optimizers, best_params['optimizer_name'])\n",
        "optimizer_final = optimizer_cls_final(learning_rate=best_params['lr'])\n",
        "\n",
        "final_model.compile(loss='binary_crossentropy',\n",
        "                    optimizer=optimizer_final,\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento completo (X_train_lstm)\n",
        "smote_final_k = best_params.get('smote_k', 5)\n",
        "smote_final = SMOTE(k_neighbors=smote_final_k, random_state=42)\n",
        "\n",
        "# SMOTE necesita un array 2D. Aplanamos X_train_lstm.\n",
        "original_shape_X_train_lstm = X_train_lstm.shape\n",
        "X_train_lstm_flattened = X_train_lstm.reshape(original_shape_X_train_lstm[0], -1)\n",
        "\n",
        "X_train_resampled_flat, y_train_resampled = smote_final.fit_resample(X_train_lstm_flattened, y_train_lstm.astype(int))\n",
        "\n",
        "# Volver a dar forma 3D a los datos remuestreados para el entrenamiento del modelo LSTM\n",
        "X_train_resampled = X_train_resampled_flat.reshape(y_train_resampled.shape[0], original_shape_X_train_lstm[1], original_shape_X_train_lstm[2])\n",
        "\n",
        "\n",
        "# Configurar Early Stopping para el entrenamiento final\n",
        "early_stopping_final_cb = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=best_params['early_stopping_patience'],\n",
        "    mode='min',\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Dividir para la validación interna del modelo final (para EarlyStopping y F1 callback)\n",
        "X_train_fit, X_val_fit, y_train_fit, y_val_fit = train_test_split(\n",
        "    X_train_resampled, y_train_resampled, test_size=0.2, random_state=42, stratify=y_train_resampled\n",
        ")\n",
        "\n",
        "# Instanciar el F1ScoreCallback para el entrenamiento final\n",
        "f1_callback_instance = F1ScoreCallback(X_val=X_val_fit, y_val=y_val_fit, pos_label=0)\n",
        "\n",
        "print(f\"\\n Entrenando el modelo final LSTM con los mejores hiperparámetros (optimizados para F1-Score): {best_params}\")\n",
        "# Entrenar el modelo final y CAPTURAR EL HISTORIAL en la variable 'history'\n",
        "history = final_model.fit(\n",
        "    X_train_fit,\n",
        "    y_train_fit,\n",
        "    epochs=best_params['epochs_max'],\n",
        "    batch_size=best_params['batch_size'],\n",
        "    validation_data=(X_val_fit, y_val_fit),\n",
        "    callbacks=[early_stopping_final_cb, f1_callback_instance],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Bloque 9: Modelo LSTM final entrenado y historial capturado.\")"
      ],
      "metadata": {
        "id": "nKAcw2lu8T0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 5: Evaluación en el conjunto de prueba ---\n",
        "# Este bloque asume que 'final_model' está definido y entrenado del Bloque 9.\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
        "# X_test_lstm y y_test_lstm son los conjuntos de prueba definidos en el Bloque 5.\n",
        "y_pred_probs = final_model.predict(X_test_lstm, verbose=0)\n",
        "y_pred_probs_flat = y_pred_probs.flatten()\n",
        "\n",
        "print(\"\\n Bloque 10: Evaluación del Modelo LSTM Final con distintos umbrales en el conjunto de prueba:\\n\")\n",
        "for threshold in np.arange(0.50, 0.81, 0.02):\n",
        "    y_pred_thresholded = (y_pred_probs_flat >= threshold).astype(int)\n",
        "\n",
        "    print(f\" Umbral = {threshold:.2f}\")\n",
        "    print(classification_report(y_test_lstm, y_pred_thresholded, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"], zero_division=0))\n",
        "    print(\" Matriz de Confusión:\")\n",
        "    print(confusion_matrix(y_test_lstm, y_pred_thresholded, labels=[0, 1]))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Métricas Globales (independientes del umbral, evalúan la capacidad de ranking)\n",
        "print(\"\\n--- Métricas Globales del Modelo (independientes del umbral) ---\")\n",
        "auc = roc_auc_score(y_test_lstm, y_pred_probs_flat)\n",
        "print(f\" ROC AUC: {auc:.4f}\")\n",
        "print(\"Bloque 10: Evaluación en el conjunto de prueba completada.\")"
      ],
      "metadata": {
        "id": "NqqBNKY-J0ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 6: Graficar el Historial de Entrenamiento para Diagnóstico de Overfitting/Underfitting ---\n",
        "# Este bloque asume que 'history' y 'f1_callback_instance' están definidos del Bloque 9.\n",
        "import os\n",
        "print(\"\\n📈 Bloque 11: Generando gráficos del historial de entrenamiento para diagnosticar Overfitting/Underfitting...\")\n",
        "\n",
        "# Define la ruta para guardar los gráficos\n",
        "ruta_carpeta_graficos = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano - Generación y Análisis de Secuencias de Actividad/logs/Modelos/LSTM/graficos/'\n",
        "os.makedirs(ruta_carpeta_graficos, exist_ok=True)\n",
        "\n",
        "# Función para graficar las curvas de aprendizaje (adaptada para F1-Score)\n",
        "def plot_learning_curves(history_obj, f1_callback_obj, best_params_for_title=None, save_path=None):\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    if best_params_for_title:\n",
        "        title_str = \"Curvas de Aprendizaje LSTM (F1-Score Clase 0)\\n\"\n",
        "        title_str += f\"LR: {best_params_for_title.get('lr'):.1e}, Opt: {best_params_for_title.get('optimizer_name')}, \"\n",
        "        title_str += f\"LSTM_Layers: {best_params_for_title.get('n_lstm_layers')}, SMOTE_k: {best_params_for_title.get('smote_k')}\"\n",
        "        fig.suptitle(title_str, fontsize=18, y=1.03)\n",
        "\n",
        "    # Gráfico de Pérdida (Loss)\n",
        "    ax1.plot(history_obj.history['loss'], label='Pérdida de Entrenamiento', color='dodgerblue')\n",
        "    if 'val_loss' in history_obj.history:\n",
        "        ax1.plot(history_obj.history['val_loss'], label='Pérdida de Validación', color='darkorange', linestyle='--')\n",
        "    ax1.set_title('Curva de Pérdida del Modelo', fontsize=16)\n",
        "    ax1.set_xlabel('Época'); ax1.set_ylabel('Loss'); ax1.legend()\n",
        "\n",
        "    # Gráfico de F1-Score (desde el callback personalizado)\n",
        "    ax2.plot(f1_callback_obj.train_f1s, label='F1-Score Entrenamiento (Clase 0 - Placeholder)', color='lightgray', linestyle=':')\n",
        "    ax2.plot(f1_callback_obj.val_f1s, label='F1-Score Validación (Clase 0)', color='darkorange', linestyle='--')\n",
        "    ax2.set_title('Curva de F1-Score del Modelo (Clase 0)', fontsize=16)\n",
        "    ax2.set_xlabel('Época'); ax2.set_ylabel('F1-Score'); ax2.legend()\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]);\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close(fig)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Llamada a la función de graficación\n",
        "plot_learning_curves(history, f1_callback_instance, best_params_for_title=best_params, save_path=os.path.join(ruta_carpeta_graficos, 'lstm_f1_learning_curves.png'))\n",
        "\n",
        "print(\"✅ Bloque 11: Gráficos generados y guardados en Google Drive.\")"
      ],
      "metadata": {
        "id": "466MnEgmL4zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 7: Guardar el Modelo Final ---\n",
        "# Este bloque asume que 'final_model' está definido y entrenado del Bloque 9.\n",
        "\n",
        "ruta_carpeta_modelo = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano - Generación y Análisis de Secuencias de Actividad/logs/Modelos/LSTM/'\n",
        "os.makedirs(ruta_carpeta_modelo, exist_ok=True)\n",
        "\n",
        "nombre_archivo_modelo_lstm = 'modelo_lstm_f1_optuna_final.keras' # Nombre del modelo LSTM\n",
        "ruta_modelo_completa_lstm = os.path.join(ruta_carpeta_modelo, nombre_archivo_modelo_lstm)\n",
        "\n",
        "final_model.save(ruta_modelo_completa_lstm)\n",
        "print(f\"Bloque 12: Modelo LSTM entrenado y guardado en: {ruta_modelo_completa_lstm}\")"
      ],
      "metadata": {
        "id": "9-NcYpasMbUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hibrido CNN - LSTM"
      ],
      "metadata": {
        "id": "JJXJG6ynZQjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define las columnas clave (asegúrate que existan en df_consolidado)\n",
        "ID_COLS = ['Nombre completo del usuario', 'Semestre']\n",
        "TIME_COL = 'semana_semestre'\n",
        "TARGET_COL = 'aprobo_semestre_real'\n",
        "FEATURES_LOG_COLS = [\n",
        "    'max_days_with_access', 'max_days_without_access', 'first_last_log_diff',\n",
        "    'logs', 'week_logs', 'daily_avg', 'weekly_avg', 'days_with_logs',\n",
        "    'days_with_logs_avg', 'days_with_logs_week', 'days_with_logs_avg_week', #\n",
        "    'activity_total', 'content_total', 'other_total', 'report_total',\n",
        "    'system_total', 'activity_week', 'content_week', 'other_week',\n",
        "    'report_week', 'system_week', 'total_weeks', 'course_progress',\n",
        "    'longest_streak', 'proyeccion_nota_final', 'aprobando'\n",
        "]\n",
        "\n",
        "# Convertir df_consolidado a NumPy arrays y manejar NaNs (replicando la lógica de preparación de LSTM)\n",
        "# Esto es para asegurar que los datos X_train_lstm, y_train_lstm estén bien preparados.\n",
        "\n",
        "# Validar que las columnas CRÍTICAS existan\n",
        "for col in ID_COLS + [TIME_COL] + [TARGET_COL]:\n",
        "    if col not in df_consolidado.columns:\n",
        "        print(f\" ERROR CRÍTICO: La columna '{col}' (ID/Tiempo/Target) NO se encontró en df_consolidado. ¡Revisa el nombre!\")\n",
        "        exit()\n",
        "\n",
        "# Convertir a numérico (manejar posibles cadenas vacías o no numéricas)\n",
        "all_cols_to_process_for_numeric = FEATURES_LOG_COLS + [TARGET_COL] + [TIME_COL]\n",
        "for col in all_cols_to_process_for_numeric:\n",
        "    if col in df_consolidado.columns:\n",
        "        df_consolidado[col] = pd.to_numeric(df_consolidado[col], errors='coerce')\n",
        "    elif col in FEATURES_LOG_COLS:\n",
        "        print(f\"⚠️ Advertencia: La columna de característica '{col}' NO se encontró en df_consolidado. ¡Será ignorada!\")\n",
        "\n",
        "# Ordenar y Manejar NaN's (ffill y fillna(0))\n",
        "df_consolidado = df_consolidado.sort_values(by=ID_COLS + [TIME_COL]).reset_index(drop=True)\n",
        "cols_for_ffill = ['max_days_with_access', 'max_days_without_access', 'longest_streak', 'promedio_ponderado', 'proyeccion_nota_final', 'aprobando']\n",
        "for col in FEATURES_LOG_COLS:\n",
        "    if col not in df_consolidado.columns: continue\n",
        "    if col in cols_for_ffill: df_consolidado[col] = df_consolidado.groupby(ID_COLS)[col].ffill()\n",
        "    df_consolidado[col] = df_consolidado[col].fillna(0)\n",
        "df_consolidado[TARGET_COL] = df_consolidado[TARGET_COL].fillna(0).astype(int)\n",
        "\n",
        "# Aplicar Escalado (StandardScaler)\n",
        "actual_features_to_scale = [f for f in FEATURES_LOG_COLS if f in df_consolidado.columns]\n",
        "if not actual_features_to_scale:\n",
        "    print(\" ERROR: No hay características válidas para escalar en df_consolidado.\")\n",
        "    exit()\n",
        "scaler_lstm_cnn = StandardScaler()\n",
        "df_consolidado[actual_features_to_scale] = scaler_lstm_cnn.fit_transform(df_consolidado[actual_features_to_scale])\n",
        "\n",
        "# Crear Secuencias y Padding (usando tu MAX_WEEKS_FOR_LSTM fijo)\n",
        "X_sequences = [] ; y_labels = [] ; sequence_ids_list = []\n",
        "MAX_WEEKS_FOR_LSTM = 30 # Longitud fija\n",
        "current_features_for_lstm_cnn = [f for f in FEATURES_LOG_COLS if f in df_consolidado.columns]\n",
        "\n",
        "if df_consolidado.empty: print(\" ERROR: df_consolidado está VACÍO.\") ; exit()\n",
        "if not current_features_for_lstm_cnn: print(\" ERROR: No hay features para LSTM-CNN.\") ; exit()\n",
        "\n",
        "grouped_sequences = df_consolidado.groupby(ID_COLS, dropna=False)\n",
        "for name, group in grouped_sequences:\n",
        "    if group.empty or TIME_COL not in group.columns or not current_features_for_lstm_cnn: continue\n",
        "    group_sorted = group.sort_values(by=TIME_COL)\n",
        "    features_data = group_sorted[current_features_for_lstm_cnn].values\n",
        "    if len(features_data) >= 3: # Filtrar por longitud mínima\n",
        "        X_sequences.append(features_data)\n",
        "        y_labels.append(group_sorted[TARGET_COL].iloc[-1])\n",
        "\n",
        "if not X_sequences: print(\" ERROR: X_sequences está VACÍA.\") ; exit()\n",
        "\n",
        "X_padded_sequences_lstm_cnn = pad_sequences(X_sequences, maxlen=MAX_WEEKS_FOR_LSTM, dtype='float32', padding='post', truncating='post', value=0.0)\n",
        "y_labels_np_lstm_cnn = np.array(y_labels)\n",
        "\n",
        "# Division del Dataset para LSTM-CNN\n",
        "if X_padded_sequences_lstm_cnn.size == 0 or y_labels_np_lstm_cnn.size == 0:\n",
        "    print(\" ERROR: Los datos para LSTM-CNN están vacíos.\") ; exit()\n",
        "\n",
        "if len(np.unique(y_labels_np_lstm_cnn)) < 2:\n",
        "    print(\" ERROR: y_labels_np_lstm_cnn tiene menos de 2 clases. No se puede estratificar.\")\n",
        "    X_train_lstm_cnn, X_test_lstm_cnn, y_train_lstm_cnn, y_test_lstm_cnn = train_test_split(X_padded_sequences_lstm_cnn, y_labels_np_lstm_cnn, test_size=0.2, random_state=42)\n",
        "else:\n",
        "    X_train_lstm_cnn, X_test_lstm_cnn, y_train_lstm_cnn, y_test_lstm_cnn = train_test_split(X_padded_sequences_lstm_cnn, y_labels_np_lstm_cnn, test_size=0.2, random_state=42, stratify=y_labels_np_lstm_cnn)\n",
        "\n",
        "print(\" Bloque 1: Importaciones y preparación de datos para LSTM-CNN completados.\")\n",
        "print(f\"Forma de X_train_lstm_cnn: {X_train_lstm_cnn.shape}\")\n",
        "print(f\"Forma de y_train_lstm_cnn: {y_train_lstm_cnn.shape}\")"
      ],
      "metadata": {
        "id": "B4i9uRMCZRkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "# ¡¡¡CORRECCIÓN AQUI!!! Importar layers explícitamente para usar layers.Conv1D, etc.\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Dropout, Input, LSTM, Conv1D, MaxPooling1D, Flatten, Masking # Todas estas son parte de layers\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "\n",
        "from sklearn.metrics import f1_score # f1_score ya importado\n",
        "\n",
        "# --- PASO 1: Función para Crear el Modelo LSTM-CNN ---\n",
        "def create_lstm_cnn_model(input_shape, config):\n",
        "    \"\"\"\n",
        "    Crea y compila un modelo de red neuronal híbrida CNN-LSTM.\n",
        "    input_shape debe ser (max_sequence_length, num_features_per_timestep).\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Capa de entrada que recibe las secuencias, incluyendo Masking para los ceros de padding\n",
        "    model.add(layers.Masking(mask_value=0.0, input_shape=input_shape)) # Usar layers.Masking\n",
        "\n",
        "    # --- Parte CNN (extraer patrones locales a lo largo del tiempo) ---\n",
        "    for i in range(config['n_conv_layers']):\n",
        "        model.add(layers.Conv1D(filters=config[f'filters_l{i}'], # Usar layers.Conv1D\n",
        "                                kernel_size=config[f'kernel_size_l{i}'],\n",
        "                                activation=config['activation'],\n",
        "                                padding='same'))\n",
        "        if config[f'dropout_conv_l{i}'] > 0:\n",
        "            model.add(layers.Dropout(config[f'dropout_conv_l{i}'])) # Usar layers.Dropout\n",
        "        model.add(layers.MaxPooling1D(pool_size=2)) # Usar layers.MaxPooling1D\n",
        "\n",
        "    # --- Parte LSTM (aprender dependencias de las secuencias reducidas por CNN) ---\n",
        "    for i in range(config['n_lstm_layers']):\n",
        "        model.add(layers.LSTM(config[f'lstm_units_l{i}'], return_sequences=(i < config['n_lstm_layers'] - 1))) # Usar layers.LSTM\n",
        "        if config[f'dropout_lstm_l{i}'] > 0:\n",
        "            model.add(layers.Dropout(config[f'dropout_lstm_l{i}']))\n",
        "\n",
        "    # --- Capas densas post-recurrente ---\n",
        "    if config['n_dense_layers'] > 0:\n",
        "        for i in range(config['n_dense_layers']):\n",
        "            model.add(layers.Dense(config[f'dense_units_l{i}'], activation=config['activation'])) # Usar layers.Dense\n",
        "            if config[f'dropout_dense_l{i}'] > 0:\n",
        "                model.add(layers.Dropout(config[f'dropout_dense_l{i}']))\n",
        "    else:\n",
        "        model.add(layers.Dense(32, activation=config['activation'])) # Usar layers.Dense\n",
        "        model.add(layers.Dropout(0.2)) # Usar layers.Dropout\n",
        "\n",
        "    # Capa de salida para clasificación binaria\n",
        "    model.add(layers.Dense(1, activation='sigmoid')) # Usar layers.Dense\n",
        "\n",
        "    # Configurar el optimizador\n",
        "    optimizer_cls = getattr(tf.keras.optimizers, config['optimizer_name']) # tf.keras.optimizers es correcto\n",
        "    optimizer = optimizer_cls(learning_rate=config['lr'])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# --- Callback personalizado para calcular F1-Score en cada época ---\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(self, X_val, y_val, pos_label=0):\n",
        "        super().__init__()\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.pos_label = pos_label\n",
        "        self.train_f1s = []\n",
        "        self.val_f1s = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        y_pred_val_proba = self.model.predict(self.X_val, verbose=0)\n",
        "        y_pred_val_classes = (y_pred_val_proba > 0.5).astype(\"int32\")\n",
        "        val_f1 = f1_score(self.y_val, y_pred_val_classes, pos_label=self.pos_label, zero_division=0)\n",
        "        self.val_f1s.append(val_f1)\n",
        "        self.train_f1s.append(0.0) # Placeholder\n",
        "\n",
        "print(\" Bloque 2: Función 'create_lstm_cnn_model' y 'F1ScoreCallback' definidas.\")"
      ],
      "metadata": {
        "id": "-PexigSlaUHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K # Para K.clear_session()\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import optuna\n",
        "import traceback\n",
        "\n",
        "# --- PASO 2: Definir la Función Objetivo para Optuna (Optimizando para Clase 0) ---\n",
        "def objective_lstm_cnn(trial, X_train_data, y_train_data):\n",
        "    K.clear_session()\n",
        "\n",
        "    # --- Hiperparámetros para SMOTE ---\n",
        "    smote_k = trial.suggest_categorical('smote_k', [3, 5, 7, 9])\n",
        "\n",
        "    # --- Hiperparámetros para Capas CONVOLUCIONALES ---\n",
        "    n_conv_layers = trial.suggest_int('n_conv_layers', 1, 2)\n",
        "    conv_layer_configs = {}\n",
        "    for i in range(n_conv_layers):\n",
        "        conv_layer_configs[f'filters_l{i}'] = trial.suggest_categorical(f'filters_l{i}', [16, 32, 64, 128])\n",
        "        conv_layer_configs[f'kernel_size_l{i}'] = trial.suggest_categorical(f'kernel_size_l{i}', [2, 3])\n",
        "        conv_layer_configs[f'dropout_conv_l{i}'] = trial.suggest_float(f'dropout_conv_l{i}', 0.2, 0.5, step=0.1)\n",
        "\n",
        "    # --- Hiperparámetros para Capas LSTM ---\n",
        "    n_lstm_layers = trial.suggest_int('n_lstm_layers', 1, 2)\n",
        "    lstm_layer_configs = {}\n",
        "    for i in range(n_lstm_layers):\n",
        "        lstm_layer_configs[f'lstm_units_l{i}'] = trial.suggest_categorical(f'lstm_units_l{i}', [32, 64, 96, 128])\n",
        "        lstm_layer_configs[f'dropout_lstm_l{i}'] = trial.suggest_float(f'dropout_lstm_l{i}', 0.2, 0.5, step=0.1)\n",
        "\n",
        "    # --- Hiperparámetros para Capas DENSAS post-recurrente ---\n",
        "    n_dense_layers = trial.suggest_int('n_dense_layers', 0, 1) # 0 o 1 capa densa oculta\n",
        "    dense_layer_configs = {}\n",
        "    if n_dense_layers > 0:\n",
        "        for i in range(n_dense_layers):\n",
        "            dense_layer_configs[f'dense_units_l{i}'] = trial.suggest_categorical(f'dense_units_l{i}', [32, 64, 128])\n",
        "            dense_layer_configs[f'dropout_dense_l{i}'] = trial.suggest_float(f'dropout_dense_l{i}', 0.2, 0.5, step=0.1)\n",
        "\n",
        "    # --- Hiperparámetros Generales ---\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
        "    optimizer_name = trial.suggest_categorical('optimizer_name', ['Adam', 'RMSprop'])\n",
        "    lr = trial.suggest_float('lr', 1e-4, 5e-3, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
        "\n",
        "    epochs_max = trial.suggest_categorical('epochs_max', [50, 100, 150])\n",
        "    early_stopping_patience = trial.suggest_int('early_stopping_patience', 10, 30)\n",
        "\n",
        "    # --- Balanceo con SMOTE ---\n",
        "    sm = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "\n",
        "    # --- Validación Cruzada Estratificada (5 splits) ---\n",
        "    # Usar X_train_data y y_train_data que se pasaron como argumentos\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores_fold = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train_data, y_train_data)):\n",
        "        X_tr_fold, X_val_fold = X_train_data[train_idx], X_train_data[val_idx]\n",
        "        y_tr_fold, y_val_fold = y_train_data[train_idx], y_train_data[val_idx]\n",
        "\n",
        "        # Aplicar SMOTE al conjunto de entrenamiento del fold (aplanando temporalmente)\n",
        "        original_shape_X_tr_fold = X_tr_fold.shape\n",
        "        X_tr_fold_flattened = X_tr_fold.reshape(original_shape_X_tr_fold[0], -1)\n",
        "\n",
        "        try:\n",
        "            X_tr_fold_res_flat, y_tr_fold_res = sm.fit_resample(X_tr_fold_flattened, y_tr_fold)\n",
        "        except ValueError as e:\n",
        "            print(f\"Trial {trial.number}, Fold {fold_idx}: SMOTE falló ({e}). Retornando 0.0.\")\n",
        "            return 0.0\n",
        "\n",
        "        # Volver a dar forma 3D a los datos después de SMOTE para el modelo LSTM-CNN\n",
        "        X_tr_fold_res = X_tr_fold_res_flat.reshape(y_tr_fold_res.shape[0], original_shape_X_tr_fold[1], original_shape_X_tr_fold[2])\n",
        "\n",
        "        # --- Configuración del Modelo para este Trial ---\n",
        "        model_config = {\n",
        "            'n_conv_layers': n_conv_layers,\n",
        "            **conv_layer_configs, # Desempaquetar las configs de CNN\n",
        "            'n_lstm_layers': n_lstm_layers,\n",
        "            **lstm_layer_configs, # Desempaquetar las configs de LSTM\n",
        "            'n_dense_layers': n_dense_layers,\n",
        "            **dense_layer_configs, # Desempaquetar las configs de Dense\n",
        "            'activation': activation,\n",
        "            'optimizer_name': optimizer_name,\n",
        "            'lr': lr\n",
        "        }\n",
        "\n",
        "        # Crear la instancia del modelo LSTM-CNN\n",
        "        # input_shape es (max_sequence_length, num_features_per_timestep)\n",
        "        model = create_lstm_cnn_model(input_shape=(X_train_data.shape[1], X_train_data.shape[2]), config=model_config) !\n",
        "\n",
        "        # Callbacks de EarlyStopping\n",
        "        early_stopping_cb_trial = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=early_stopping_patience,\n",
        "            mode='min',\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        model.fit(X_tr_fold_res, y_tr_fold_res,\n",
        "                  epochs=epochs_max,\n",
        "                  batch_size=batch_size,\n",
        "                  validation_data=(X_val_fold, y_val_fold),\n",
        "                  callbacks=[early_stopping_cb_trial],\n",
        "                  verbose=0)\n",
        "\n",
        "        y_val_pred_proba = model.predict(X_val_fold, verbose=0)\n",
        "        y_val_pred_classes = (y_val_pred_proba > 0.5).astype(\"int32\")\n",
        "\n",
        "        f1 = f1_score(y_val_fold, y_val_pred_classes, pos_label=0, zero_division=0)\n",
        "        f1_scores_fold.append(f1)\n",
        "\n",
        "        trial.report(f1, fold_idx)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(f1_scores_fold)\n",
        "\n",
        "print(\"Bloque 3: Función 'objective_lstm_cnn' definida.\")"
      ],
      "metadata": {
        "id": "GldjaxedZy6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna # Necesario si este bloque se ejecuta solo\n",
        "\n",
        "# --- PASO 3: Crear un Estudio de Optuna y Ejecutar la Optimización ---\n",
        "study_name = \"LSTM_CNN_F1_Score_Optuna\" # Nuevo nombre para el estudio\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0, interval_steps=1)\n",
        "\n",
        "study_lstm_cnn_optuna = optuna.create_study(direction=\"maximize\", study_name=study_name, sampler=sampler, pruner=pruner)\n",
        "\n",
        "n_trials_optuna = 60 # Número de trials\n",
        "timeout_seconds = 7200 # 2 horas de timeout (ajusta según tus recursos y paciencia)\n",
        "\n",
        "print(f\" Iniciando optimización con Optuna para Keras LSTM-CNN ({n_trials_optuna} trials, timeout: {timeout_seconds/60} min)...\")\n",
        "try:\n",
        "    # Se pasan X_train_lstm_cnn y y_train_lstm_cnn como argumentos a la función objective\n",
        "    study_lstm_cnn_optuna.optimize(lambda trial: objective_lstm_cnn(trial, X_train_lstm_cnn, y_train_lstm_cnn), n_trials=n_trials_optuna, timeout=timeout_seconds)\n",
        "    print(\" Optimización con Optuna para LSTM-CNN Keras completada.\")\n",
        "\n",
        "    print(f\"\\n Mejores hiperparámetros LSTM-CNN (Optimizados para F1-Score Clase 0): {study_lstm_cnn_optuna.best_trial.params}\")\n",
        "    print(f\" Mejor F1-Score clase 0 (promedio CV): {study_lstm_cnn_optuna.best_value:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Ocurrió un error INESPERADO durante la optimización con Optuna:\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n✅ Bloque 4: Optimización Optuna completada.\")"
      ],
      "metadata": {
        "id": "w3JXu776Z0DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 4: Crear y Entrenar el Modelo Final con los Mejores Hiperparámetros ---\n",
        "# Este bloque asume que 'study_lstm_cnn_optuna' está definido y que se encontró un best_trial.\n",
        "\n",
        "best_params = study_lstm_cnn_optuna.best_trial.params\n",
        "\n",
        "# Reconstruir la configuración de las capas CNN\n",
        "conv_layer_configs_final = []\n",
        "n_conv_layers_final = best_params.get('n_conv_layers', 1)\n",
        "for i in range(n_conv_layers_final):\n",
        "    conv_layer_configs_final.append({\n",
        "        'filters': best_params[f'filters_l{i}'],\n",
        "        'kernel_size': best_params[f'kernel_size_l{i}'],\n",
        "        'dropout_conv': best_params[f'dropout_conv_l{i}']\n",
        "    })\n",
        "\n",
        "# Reconstruir la configuración de las capas LSTM\n",
        "lstm_layer_configs_final = []\n",
        "n_lstm_layers_final = best_params.get('n_lstm_layers', 1)\n",
        "for i in range(n_lstm_layers_final):\n",
        "    lstm_layer_configs_final.append({\n",
        "        'units': best_params[f'lstm_units_l{i}'],\n",
        "        'dropout': best_params[f'dropout_lstm_l{i}']\n",
        "    })\n",
        "\n",
        "# Reconstruir la configuración de las capas Densas\n",
        "dense_layer_configs_final = []\n",
        "n_dense_layers_final = best_params.get('n_dense_layers', 0)\n",
        "if n_dense_layers_final > 0:\n",
        "    for i in range(n_dense_layers_final):\n",
        "        dense_layer_configs_final.append({\n",
        "            'units': best_params[f'dense_units_l{i}'],\n",
        "            'dropout': best_params[f'dropout_dense_l{i}']\n",
        "        })\n",
        "\n",
        "# Crear la instancia del modelo LSTM-CNN final\n",
        "final_model_lstm_cnn = Sequential()\n",
        "final_model_lstm_cnn.add(Input(shape=(X_train_lstm_cnn.shape[1], X_train_lstm_cnn.shape[2]))) # Input shape para LSTM-CNN\n",
        "\n",
        "# Capa Masking para ceros de padding\n",
        "final_model_lstm_cnn.add(Masking(mask_value=0.0, input_shape=(X_train_lstm_cnn.shape[1], X_train_lstm_cnn.shape[2])))\n",
        "\n",
        "# Parte CNN\n",
        "for i, layer_config in enumerate(conv_layer_configs_final):\n",
        "    final_model_lstm_cnn.add(Conv1D(filters=layer_config['filters'],\n",
        "                                    kernel_size=layer_config['kernel_size'],\n",
        "                                    activation=best_params['activation'],\n",
        "                                    padding='same'))\n",
        "    if layer_config['dropout_conv'] > 0:\n",
        "        final_model_lstm_cnn.add(Dropout(layer_config['dropout_conv']))\n",
        "    final_model_lstm_cnn.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Parte LSTM\n",
        "for i, layer_config in enumerate(lstm_layer_configs_final):\n",
        "    final_model_lstm_cnn.add(LSTM(layer_config['units'], return_sequences=(i < n_lstm_layers_final - 1)))\n",
        "    if layer_config['dropout'] > 0:\n",
        "        final_model_lstm_cnn.add(Dropout(layer_config['dropout']))\n",
        "\n",
        "# Capas densas post-recurrente\n",
        "if n_dense_layers_final > 0:\n",
        "    for i, layer_config in enumerate(dense_layer_configs_final):\n",
        "        final_model_lstm_cnn.add(Dense(layer_config['units'], activation=best_params['activation']))\n",
        "        if layer_config['dropout'] > 0:\n",
        "            final_model_lstm_cnn.add(Dropout(layer_config['dropout']))\n",
        "else: # Si n_dense_layers fue 0, añadir una capa densa por defecto\n",
        "    final_model_lstm_cnn.add(Dense(32, activation=best_params['activation']))\n",
        "    final_model_lstm_cnn.add(Dropout(0.2))\n",
        "\n",
        "final_model_lstm_cnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer_cls_final = getattr(tf.keras.optimizers, best_params['optimizer_name'])\n",
        "optimizer_final = optimizer_cls_final(learning_rate=best_params['lr'])\n",
        "\n",
        "final_model_lstm_cnn.compile(loss='binary_crossentropy',\n",
        "                    optimizer=optimizer_final,\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento completo (X_train_lstm_cnn)\n",
        "smote_final_k = best_params.get('smote_k', 5)\n",
        "smote_final = SMOTE(k_neighbors=smote_final_k, random_state=42)\n",
        "\n",
        "# SMOTE necesita un array 2D. Aplanamos X_train_lstm_cnn.\n",
        "original_shape_X_train_lstm_cnn = X_train_lstm_cnn.shape\n",
        "X_train_lstm_cnn_flattened = X_train_lstm_cnn.reshape(original_shape_X_train_lstm_cnn[0], -1)\n",
        "\n",
        "X_train_resampled_flat, y_train_resampled = smote_final.fit_resample(X_train_lstm_cnn_flattened, y_train_lstm_cnn.astype(int))\n",
        "\n",
        "# Volver a dar forma 3D a los datos remuestreados\n",
        "X_train_resampled = X_train_resampled_flat.reshape(y_train_resampled.shape[0], original_shape_X_train_lstm_cnn[1], original_shape_X_train_lstm_cnn[2])\n",
        "\n",
        "# Configurar Early Stopping para el entrenamiento final\n",
        "early_stopping_final_cb = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=best_params['early_stopping_patience'],\n",
        "    mode='min',\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Dividir para la validación interna del modelo final\n",
        "X_train_fit, X_val_fit, y_train_fit, y_val_fit = train_test_split(\n",
        "    X_train_resampled, y_train_resampled, test_size=0.2, random_state=42, stratify=y_train_resampled\n",
        ")\n",
        "\n",
        "# Instanciar el F1ScoreCallback para el entrenamiento final\n",
        "f1_callback_instance_lstm_cnn = F1ScoreCallback(X_val=X_val_fit, y_val=y_val_fit, pos_label=0)\n",
        "\n",
        "print(f\"\\n Entrenando el modelo final LSTM-CNN con los mejores hiperparámetros (optimizados para F1-Score): {best_params}\")\n",
        "# Entrenar el modelo final y CAPTURAR EL HISTORIAL\n",
        "history_lstm_cnn = final_model_lstm_cnn.fit(\n",
        "    X_train_fit,\n",
        "    y_train_fit,\n",
        "    epochs=best_params['epochs_max'],\n",
        "    batch_size=best_params['batch_size'],\n",
        "    validation_data=(X_val_fit, y_val_fit),\n",
        "    callbacks=[early_stopping_final_cb, f1_callback_instance_lstm_cnn],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Bloque 5: Modelo LSTM-CNN final entrenado y historial capturado.\")"
      ],
      "metadata": {
        "id": "JIsw5Qlpan5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 5: Evaluación en el conjunto de prueba ---\n",
        "# Este bloque asume que 'final_model_lstm_cnn' está definido y entrenado del Bloque 5.\n",
        "\n",
        "y_pred_probs_lstm_cnn = final_model_lstm_cnn.predict(X_test_lstm_cnn, verbose=0)\n",
        "y_pred_probs_flat_lstm_cnn = y_pred_probs_lstm_cnn.flatten()\n",
        "\n",
        "print(\"\\nBloque 6: Evaluación del Modelo LSTM-CNN Final con distintos umbrales en el conjunto de prueba:\\n\")\n",
        "for threshold in np.arange(0.50, 0.81, 0.02):\n",
        "    y_pred_thresholded_lstm_cnn = (y_pred_probs_flat_lstm_cnn >= threshold).astype(int)\n",
        "\n",
        "    print(f\" Umbral = {threshold:.2f}\")\n",
        "    print(classification_report(y_test_lstm_cnn, y_pred_thresholded_lstm_cnn, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"], zero_division=0))\n",
        "    print(\" Matriz de Confusión:\")\n",
        "    print(confusion_matrix(y_test_lstm_cnn, y_pred_thresholded_lstm_cnn, labels=[0, 1]))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Métricas Globales (independientes del umbral, evalúan la capacidad de ranking)\n",
        "print(\"\\n--- Métricas Globales del Modelo (independientes del umbral) ---\")\n",
        "auc_lstm_cnn = roc_auc_score(y_test_lstm_cnn, y_pred_probs_flat_lstm_cnn)\n",
        "print(f\" ROC AUC: {auc_lstm_cnn:.4f}\")\n",
        "print(\"Bloque 6: Evaluación en el conjunto de prueba completada.\")"
      ],
      "metadata": {
        "id": "C7ykeoj6qWT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 6: Graficar el Historial de Entrenamiento para Diagnóstico de Overfitting/Underfitting ---\n",
        "# Este bloque asume que 'history_lstm_cnn' y 'f1_callback_instance_lstm_cnn' están definidos del Bloque 5.\n",
        "\n",
        "print(\"\\n📈 Bloque 7: Generando gráficos del historial de entrenamiento para diagnosticar Overfitting/Underfitting...\")\n",
        "\n",
        "# Define la ruta para guardar los gráficos\n",
        "ruta_carpeta_graficos_lstm_cnn = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano - Generación y Análisis de Secuencias de Actividad/logs/Modelos/LSTM_CNN/graficos/'\n",
        "os.makedirs(ruta_carpeta_graficos_lstm_cnn, exist_ok=True)\n",
        "\n",
        "# Función para graficar las curvas de aprendizaje (adaptada para F1-Score)\n",
        "def plot_learning_curves(history_obj, f1_callback_obj, best_params_for_title=None, save_path=None):\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    if best_params_for_title:\n",
        "        title_str = \"Curvas de Aprendizaje LSTM-CNN (F1-Score Clase 0)\\n\"\n",
        "        title_str += f\"LR: {best_params_for_title.get('lr'):.1e}, Opt: {best_params_for_title.get('optimizer_name')}, \"\n",
        "        title_str += f\"Conv_L: {best_params_for_title.get('n_conv_layers')}, LSTM_L: {best_params_for_title.get('n_lstm_layers')}, SMOTE_k: {best_params_for_title.get('smote_k')}\"\n",
        "        fig.suptitle(title_str, fontsize=18, y=1.03)\n",
        "\n",
        "    # Gráfico de Pérdida (Loss)\n",
        "    ax1.plot(history_obj.history['loss'], label='Pérdida de Entrenamiento', color='dodgerblue')\n",
        "    if 'val_loss' in history_obj.history:\n",
        "        ax1.plot(history_obj.history['val_loss'], label='Pérdida de Validación', color='darkorange', linestyle='--')\n",
        "    ax1.set_title('Curva de Pérdida del Modelo', fontsize=16)\n",
        "    ax1.set_xlabel('Época'); ax1.set_ylabel('Loss'); ax1.legend()\n",
        "\n",
        "    # Gráfico de F1-Score (desde el callback personalizado)\n",
        "    ax2.plot(f1_callback_obj.train_f1s, label='F1-Score Entrenamiento (Clase 0 - Placeholder)', color='lightgray', linestyle=':')\n",
        "    ax2.plot(f1_callback_obj.val_f1s, label='F1-Score Validación (Clase 0)', color='darkorange', linestyle='--')\n",
        "    ax2.set_title('Curva de F1-Score del Modelo (Clase 0)', fontsize=16)\n",
        "    ax2.set_xlabel('Época'); ax2.set_ylabel('F1-Score'); ax2.legend()\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]);\n",
        "    plt.show()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close(fig)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Llamada a la función de graficación\n",
        "plot_learning_curves(history_lstm_cnn, f1_callback_instance_lstm_cnn, best_params_for_title=best_params, save_path=os.path.join(ruta_carpeta_graficos_lstm_cnn, 'lstm_cnn_f1_learning_curves.png'))\n",
        "\n",
        "print(\"Bloque 7: Gráficos generados y guardados en Google Drive.\")"
      ],
      "metadata": {
        "id": "JjbYFBW1qbQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 7: Guardar el Modelo Final ---\n",
        "# Este bloque asume que 'final_model_lstm_cnn' está definido y entrenado del Bloque 5.\n",
        "\n",
        "ruta_carpeta_modelo_lstm_cnn = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Muñoz/Maximiliano - Generación y Análisis de Secuencias de Actividad/logs/Modelos/LSTM_CNN/'\n",
        "os.makedirs(ruta_carpeta_modelo_lstm_cnn, exist_ok=True)\n",
        "\n",
        "nombre_archivo_modelo_lstm_cnn = 'modelo_lstm_cnn_f1_optuna_final.keras' # Nombre del modelo LSTM-CNN\n",
        "ruta_modelo_completa_lstm_cnn = os.path.join(ruta_carpeta_modelo_lstm_cnn, nombre_archivo_modelo_lstm_cnn)\n",
        "\n",
        "final_model_lstm_cnn.save(ruta_modelo_completa_lstm_cnn)\n",
        "print(f\" Bloque 8: Modelo LSTM-CNN entrenado y guardado en: {ruta_modelo_completa_lstm_cnn}\")"
      ],
      "metadata": {
        "id": "AUxNx7O-wpZV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}