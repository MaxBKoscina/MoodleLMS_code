{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCTs9mhd8EgA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Monta el Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ASmQW8mv8GMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Se carga los dataset de los semestre"
      ],
      "metadata": {
        "id": "rpULmhyJaNLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_1 = pd.read_csv('/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano -  Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/df_semestre_full_2023.csv')\n",
        "df_2024_2 = pd.read_csv('/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano -  Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/df_semestre_full_2024_S2.csv')\n",
        "df_2024_1 = pd.read_csv('/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano -  Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/df_semestre_full_2024_S1.csv')"
      ],
      "metadata": {
        "id": "m0WOX6e58lBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_1"
      ],
      "metadata": {
        "id": "DYdiItqr9qxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2024_2"
      ],
      "metadata": {
        "id": "GtIULSQp9tqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2024_1.columns"
      ],
      "metadata": {
        "id": "TFb6YW3p90E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nüîé Valores nulos por columna:\")\n",
        "print(df_2023_1.isna().sum())\n"
      ],
      "metadata": {
        "id": "IK_PqW1IJqs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2024_1.dtypes"
      ],
      "metadata": {
        "id": "HprpXc4w-KJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2024_2.dtypes"
      ],
      "metadata": {
        "id": "C0M6NftoQ2z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_1.columns"
      ],
      "metadata": {
        "id": "OwH6UtgQ-OsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PeARdJq02Spc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_2023_1.info())"
      ],
      "metadata": {
        "id": "-h-P46ly3RSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_2024_1.info())"
      ],
      "metadata": {
        "id": "7J6mOPz93T4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_2024_2.info())"
      ],
      "metadata": {
        "id": "gMXJwLfH3VVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ajfcNKtc2Svs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#df_2023_1['Semestre'] = '2023-S1'\n",
        "#df_2024_1['Semestre'] = '2024-S1'\n",
        "#df_2024_2['Semestre'] = '2024-S2'\n",
        "\n",
        "# Paso 2: Crear una lista con los dataframes a unir\n",
        "lista_dataframes = [df_2023_1, df_2024_1, df_2024_2]\n",
        "\n",
        "# Paso 3: Concatenar los dataframes\n",
        "df_consolidado = pd.concat(lista_dataframes, ignore_index=True)\n",
        "\n",
        "# Paso 4: Verificar el resultado\n",
        "print(\"Dimensiones del dataframe consolidado (filas, columnas):\")\n",
        "print(df_consolidado.shape)\n",
        "\n",
        "print(\"\\nPrimeras filas del dataframe consolidado:\")\n",
        "print(df_consolidado.head())\n",
        "\n",
        "print(\"\\n√öltimas filas del dataframe consolidado (para ver si se unieron bien los diferentes Semestre):\")\n",
        "print(df_consolidado.tail())\n",
        "\n",
        "print(\"\\nConteo de valores en la columna 'Semestre' para verificar la uni√≥n:\")\n",
        "print(df_consolidado['Semestre'].value_counts())\n",
        "\n",
        "print(\"\\nInformaci√≥n general del dataframe consolidado (para revisar tipos de datos y no nulos):\")\n",
        "df_consolidado.info()"
      ],
      "metadata": {
        "id": "lIkbF-nx40c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el DataFrame en un archivo CSV\n",
        "df_consolidado.to_csv('/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano -  Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/df_consolidado_full.csv', index=False)\n",
        "\n",
        "print(\"Archivo CSV guardado correctamente.\")"
      ],
      "metadata": {
        "id": "eKB4gRiYfnW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_consolidado = pd.read_csv('/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano -  Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/df_consolidado_full.csv')"
      ],
      "metadata": {
        "id": "rjrkSFd24EIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columnas_a_mantener = [\n",
        "    # --- Features calculadas de Logs ---\n",
        "    'max_days_with_access',         # √çndice 7\n",
        "    'max_days_without_access',      # √çndice 8\n",
        "    'first_last_log_diff',          # √çndice 9\n",
        "    'logs',                         # √çndice 10 (Total logs calculados para ese estudiante-semestre)\n",
        "    'week_logs',                    # √çndice 11 (¬øEs el total semanal o el de la √∫ltima semana? Aseg√∫rate que represente el estado final)\n",
        "    'daily_avg',                    # √çndice 12 (Promedio diario calculado para el periodo)\n",
        "    'weekly_avg',                   # √çndice 13 (Promedio semanal calculado para el periodo)\n",
        "    'days_with_logs',               # √çndice 14 (Total d√≠as con logs)\n",
        "    'days_with_logs_avg',           # √çndice 15 (Promedio d√≠as con logs)\n",
        "    'days_with_logs_week',          # √çndice 16 (¬øD√≠as con logs en la √∫ltima semana?)\n",
        "    'days_with_logs_avg_week',      # √çndice 17 (¬øPromedio d√≠as con logs en la √∫ltima semana?)\n",
        "    'activity_total',               # √çndice 19 (Total logs categor√≠a 'activity')\n",
        "    'content_total',                # √çndice 20 (Total logs categor√≠a 'content')\n",
        "    'other_total',                  # √çndice 21 (Total logs categor√≠a 'other')\n",
        "    'report_total',\n",
        "    'system_total',\n",
        "    'activity_week',                # √çndice 22 (¬øLogs 'activity' √∫ltima semana?)\n",
        "    'content_week',                 # √çndice 23 (¬øLogs 'content' √∫ltima semana?)\n",
        "    'other_week',                   # √çndice 24 (¬øLogs 'other' √∫ltima semana?)\n",
        "    'report_week',                  # √çndice 25 (¬øLogs 'report' √∫ltima semana?)\n",
        "    'system_week',                  # √çndice 26 (¬øLogs 'system' √∫ltima semana?)\n",
        "    'total_weeks',                  # √çndice 27 (Duraci√≥n total del curso, si es constante por semestre)\n",
        "    'course_progress',              # √çndice 28 (Progreso en el punto de corte)\n",
        "    'longest_streak',   # √çndice 29 (Racha m√°s larga de acceso/actividad)\n",
        "\n",
        "\n",
        "    # --- Tu indicador binario (Feature) ---\n",
        "    'aprobando',                    # √çndice 32 (La variable 0/1 que creaste, la renombraremos luego)\n",
        "\n",
        "    # --- Notas Individuales (Features) ---\n",
        "    'Control 1',                    # √çndice 34\n",
        "    'Control 2',                    # √çndice 35\n",
        "    'Sumativa 1',                   # √çndice 36\n",
        "\n",
        "    # --- Variable Objetivo (Target) ---\n",
        "    'aprobo_semestre_real'          # √çndice 33\n",
        "]"
      ],
      "metadata": {
        "id": "1bMNn_qV9HtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Se deja un dataframe con una fila unica por alumnos y su semestre para creacion de modelos"
      ],
      "metadata": {
        "id": "Qt3h5LmZamoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_agregado = df_consolidado.groupby(['Nombre completo del usuario', 'Semestre'])[columnas_a_mantener].last().reset_index()\n"
      ],
      "metadata": {
        "id": "tnlMyRvrC16o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el DataFrame en un archivo CSV\n",
        "df_agregado.to_csv('/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano -  Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/df_united_final.csv', index=False)\n",
        "\n",
        "print(\"Archivo CSV guardado correctamente.\")"
      ],
      "metadata": {
        "id": "auSomcX2fGfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agregado"
      ],
      "metadata": {
        "id": "nutvr8tvDbMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agregado.columns\n"
      ],
      "metadata": {
        "id": "cE33kxZ4F_JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_log_cols = [\n",
        "    'max_days_with_access', 'max_days_without_access', 'first_last_log_diff',\n",
        "    'logs', 'week_logs', 'daily_avg', 'weekly_avg', 'days_with_logs',\n",
        "    'days_with_logs_avg', 'days_with_logs_week', 'days_with_logs_avg_week',\n",
        "    'activity_total', 'content_total', 'other_total','report_total',\n",
        "    'system_total', 'activity_week',\n",
        "    'content_week', 'other_week', 'report_week', 'system_week',\n",
        "    'total_weeks', 'course_progress', 'longest_streak'\n",
        "\n",
        "]\n",
        "target_col = 'aprobo_semestre_real'\n"
      ],
      "metadata": {
        "id": "OpoGNAHxEt5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la matriz de correlaci√≥n de Pearson\n",
        "correlation_matrix = df_agregado[features_log_cols].corr(method='pearson')\n",
        "\n",
        "# Ver la correlaci√≥n de 'y' con todas las dem√°s variables\n",
        "# Define X and y here if they haven't been defined yet\n",
        "X = df_agregado[features_log_cols]\n",
        "y = df_agregado[target_col]\n",
        "\n",
        "# Calculate correlation of each feature in X with y\n",
        "correlations_with_y = X.corrwith(y, method='pearson')\n",
        "\n",
        "print(correlations_with_y)"
      ],
      "metadata": {
        "id": "z_kIjSzdcyss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_agregado[features_log_cols]\n",
        "y = df_agregado[target_col]\n"
      ],
      "metadata": {
        "id": "4gxZn8CvDc8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlations_with_y = X.corrwith(y, method='pearson') # Puedes cambiar 'pearson' por 'spearman' o 'kendall'\n",
        "\n",
        "# Mostrar las correlaciones\n",
        "print(\"Correlaci√≥n de cada caracter√≠stica en X con la variable y:\")\n",
        "print(correlations_with_y)"
      ],
      "metadata": {
        "id": "Q1LX8Pvodl62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ver la diferencia de aprobados vs reprobados"
      ],
      "metadata": {
        "id": "l9Z0G5YWa0V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_agregado['aprobo_semestre_real'].value_counts())\n",
        "print(df_agregado['aprobo_semestre_real'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "tzb6ezEgF_yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Forma de X (features):\", X.shape)\n",
        "print(\"Forma de y (target):\", y.shape)\n",
        "print(\"\\nPrimeras filas de X:\")\n",
        "print(X.head())"
      ],
      "metadata": {
        "id": "13mUw3zIFY76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separacions de datos"
      ],
      "metadata": {
        "id": "OxjnG30va5bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,  # Puedes ajustar entre 0.2 y 0.3\n",
        "                                                    stratify=y,      # Buena pr√°ctica mantener la distribuci√≥n\n",
        "                                                    random_state=42) # Para que la divisi√≥n sea siempre la misma\n",
        "\n",
        "print(\"\\nForma de X_train:\", X_train.shape)\n",
        "print(\"Forma de X_test:\", X_test.shape)\n",
        "print(\"Forma de y_train:\", y_train.shape)\n",
        "print(\"Forma de y_test:\", y_test.shape)\n",
        "\n",
        "# Verificar distribuci√≥n en y_train (deber√≠a ser similar a la original ~54/46)\n",
        "print(\"\\nDistribuci√≥n del target en Entrenamiento (y_train):\")\n",
        "print(y_train.value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "eD_PNfo3I2a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Paso 2: Crear una instancia del StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Paso 3:\n",
        "# Ajustar el scaler S√ìLO con los datos de entrenamiento (X_train)\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Usar el scaler YA AJUSTADO con X_train para transformar X_test.\n",
        "# ¬°Es muy importante NO volver a hacer .fit() con X_test!\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Verificar las dimensiones (deber√≠an ser las mismas que X_train y X_test originales)\n",
        "print(\"\\nDimensiones de X_train_scaled:\", X_train_scaled.shape)\n",
        "print(\"Dimensiones de X_test_scaled:\", X_test_scaled.shape)\n",
        "\n",
        "# Opcional: Ver c√≥mo lucen algunas filas de los datos escalados\n",
        "# (Ser√°n arrays de NumPy, no DataFrames de Pandas directamente, a menos que los conviertas)\n",
        "print(\"\\nPrimeras 5 filas de X_train_scaled:\")\n",
        "print(X_train_scaled[:5])"
      ],
      "metadata": {
        "id": "AUu1ok2YSlbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NO VA SMOTE - SE HACE POR CADA POR EL TEMA DE PRACTICA"
      ],
      "metadata": {
        "id": "iDxNRX1jRt0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------"
      ],
      "metadata": {
        "id": "ygceT0qfRzH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "idwryon4ovRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "KC0Og1iOR0-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metodo del codo"
      ],
      "metadata": {
        "id": "uEqAkDJsR2M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Rango de valores de k a probar\n",
        "k_range = range(1, 16)\n",
        "k_scores_f1 = []\n",
        "\n",
        "# Validaci√≥n cruzada estratificada\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"üîÅ Buscando mejor valor de k para KNN...\")\n",
        "\n",
        "for k_value in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k_value)\n",
        "    scores = cross_val_score(knn, X_train_scaled, y_train, cv=cv, scoring='f1_macro', n_jobs=-1)\n",
        "    k_scores_f1.append(scores.mean())\n",
        "    print(f\"  k={k_value}: F1 Macro promedio = {scores.mean():.4f}\")\n",
        "\n",
        "# Gr√°fico del codo\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, k_scores_f1, marker='o', linestyle='-')\n",
        "plt.title(\"M√©todo del Codo: KNN (post-SMOTE) vs F1 Macro\")\n",
        "plt.xlabel(\"Valor de k (n_neighbors)\")\n",
        "plt.ylabel(\"F1 Macro Promedio (CV)\")\n",
        "plt.grid(True)\n",
        "plt.xticks(k_range)\n",
        "plt.show()\n",
        "\n",
        "# Mejor k encontrado\n",
        "best_k_index = np.argmax(k_scores_f1)\n",
        "best_k = list(k_range)[best_k_index]\n",
        "print(f\"\\nüéØ Mejor k seg√∫n F1 Macro: {best_k}\")\n"
      ],
      "metadata": {
        "id": "ANFzZUWCloIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Diagn√≥stico: Verificar el tama√±o de la clase minoritaria en y_train_fit ---\n",
        "print(\"\\nüîé Verificando distribuci√≥n de clases en y_train_fit:\")\n",
        "# print(y_train_res.value_counts()) # Original line that caused the error\n",
        "print(np.bincount(y_train.astype(int))) # Use np.bincount for NumPy arrays\n",
        "\n",
        "# Get the counts of each class from the bincount result\n",
        "counts = np.bincount(y_train.astype(int))\n",
        "\n",
        "# Find the minimum count among the classes\n",
        "minority_class_size = counts.min()\n",
        "\n",
        "print(f\"Tama√±o de la clase minoritaria en y_train_fit: {minority_class_size}\")"
      ],
      "metadata": {
        "id": "tGjsvQw8BpoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creacion KNN"
      ],
      "metadata": {
        "id": "um0uYke4R4qB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-genetic-opt"
      ],
      "metadata": {
        "id": "NxwqlZ2vTO3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "1ljiAGGK9GLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "oPTUNA"
      ],
      "metadata": {
        "id": "AWLkTz36SOBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import recall_score, make_scorer # Necesitas make_scorer\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline # Es bueno ser expl√≠cito con ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "\n",
        "# --- Scorer Personalizado para Recall de Clase 0.0 ---\n",
        "# Es crucial definir que quieres el recall para la clase '0'\n",
        "scorer_recall_clase_0 = make_scorer(recall_score, pos_label=0, average='binary', zero_division=0)\n",
        "\n",
        "def objective_knn(trial):\n",
        "    # Hiperpar√°metros para KNN\n",
        "    n_neighbors = trial.suggest_int(\"knn_n_neighbors\", 3, 16, step=2) # k impares suelen ser preferidos\n",
        "    weights = trial.suggest_categorical(\"knn_weights\", [\"uniform\", \"distance\"])\n",
        "    metric = trial.suggest_categorical(\"knn_metric\", [\"euclidean\", \"manhattan\", \"minkowski\"])\n",
        "\n",
        "    p_val = 2 # Valor por defecto para p si no es minkowski o para minkowski con p=2\n",
        "    if metric == 'minkowski':\n",
        "        p_val = trial.suggest_categorical(\"knn_p\", [1, 2]) # p=1 (manhattan), p=2 (euclidean)\n",
        "\n",
        "    # Hiperpar√°metro para SMOTE\n",
        "    smote_k = trial.suggest_int(\"smote_k_neighbors\", 3, 7) # Renombr√© para consistencia con el pipeline\n",
        "\n",
        "    # Crear el Pipeline\n",
        "    # Es importante instanciar SMOTE y KNN dentro de la funci√≥n objective\n",
        "    # para que cada trial tenga su propia configuraci√≥n con los HPs sugeridos.\n",
        "    pipeline = ImbPipeline([\n",
        "        ('smote', SMOTE(k_neighbors=smote_k, random_state=42)),\n",
        "        ('knn', KNeighborsClassifier(\n",
        "            n_neighbors=n_neighbors,\n",
        "            weights=weights,\n",
        "            metric=metric,\n",
        "            p=p_val, # A√±adir p aqu√≠\n",
        "            n_jobs=-1 # Usar todos los procesadores para KNN\n",
        "            ))\n",
        "    ])\n",
        "\n",
        "    # Validaci√≥n cruzada estratificada\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=) # Usar trial.number para diferente shuffle por trial\n",
        "                                                                            # o un random_state fijo si prefieres\n",
        "\n",
        "    # Obtener el score usando cross_val_score\n",
        "    # n_jobs=1 para cross_val_score para evitar problemas de paralelismo anidado,\n",
        "    # especialmente si Optuna ya paraleliza trials o si n_jobs=-1 en KNN causa problemas.\n",
        "    try:\n",
        "        scores = cross_val_score(pipeline, X_train_scaled, y_train.astype(int), # Asegurar y_train como int\n",
        "                                 scoring=scorer_recall_clase_0, # Usar el scorer personalizado\n",
        "                                 cv=cv,\n",
        "                                 n_jobs=1)\n",
        "        score_mean = np.mean(scores)\n",
        "    except ValueError as e_cv:\n",
        "        # Esto puede pasar si SMOTE falla en un fold (ej. k_neighbors > n_muestras_clase_minoritaria_en_fold)\n",
        "        print(f\"  Trial {trial.number} con params {trial.params}: Error en cross_val_score ({e_cv}). Score = 0.0\")\n",
        "        return 0.0 # Devolver un score muy malo para que Optuna no elija estos par√°metros\n",
        "\n",
        "    # (Opcional) Pruning: Si Optuna detecta que un trial no es prometedor, puede detenerlo.\n",
        "    # Si tuvieras un bucle de folds manual, podr√≠as usar trial.report() y trial.should_prune()\n",
        "    # Con cross_val_score, el pruning es menos directo de implementar aqu√≠.\n",
        "\n",
        "    return score_mean\n",
        "\n",
        "# --- Crear y Ejecutar el Estudio Optuna ---\n",
        "# Aseg√∫rate de que X_train_scaled e y_train est√©n definidos y preparados antes de esta l√≠nea\n",
        "\n",
        "study_knn = optuna.create_study(direction=\"maximize\", study_name=\"KNN_SMOTE_Optuna_Recall0\")\n",
        "\n",
        "    # n_trials controla cu√°ntas combinaciones diferentes de hiperpar√°metros probar\n",
        "    # Empieza con un n√∫mero menor (ej. 30-50) para ver si funciona y luego puedes aumentarlo.\n",
        "study_knn.optimize(objective_knn, n_trials=100, timeout=1800) # Ejemplo: 50 trials o 30 minutos\n",
        "\n",
        "print(\"\\n‚úÖ B√∫squeda de hiperpar√°metros para KNN completada.\")\n",
        "print(\"üéØ Mejores hiperpar√°metros KNN encontrados:\")\n",
        "print(study_knn.best_trial.params) # Muestra los nombres que usaste en trial.suggest_\n",
        "print(f\"üìà Mejor recall clase 0 (promedio CV): {study_knn.best_value:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CJqYFnTL9Ddf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "\n",
        "# --- Scorer Personalizado para Recall de Clase 0.0 ---\n",
        "# Es crucial definir que quieres el recall para la clase '0'\n",
        "scorer_recall_clase_0 = make_scorer(recall_score, pos_label=0, average='binary', zero_division=0)\n",
        "\n",
        "def objective_knn(trial):\n",
        "\n",
        "    n_neighbors = trial.suggest_int(\"n_neighbors\", 3, 16, step=2)\n",
        "\n",
        "    weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
        "\n",
        "    metric = trial.suggest_categorical(\"metric\", [\"euclidean\", \"manhattan\", \"minkowski\"])\n",
        "\n",
        "    smote_k = trial.suggest_int(\"smote_k\", 3, 7)\n",
        "\n",
        "\n",
        "\n",
        "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, metric=metric)\n",
        "\n",
        "    pipeline = ImbPipeline([('smote', smote), ('knn', knn)])\n",
        "\n",
        "\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    scores = cross_val_score(pipeline, X_train_scaled, y_train, scoring=scorer_recall_clase_0, cv=cv)\n",
        "\n",
        "\n",
        "\n",
        "    return scores.mean()\n",
        "\n",
        "\n",
        "\n",
        "study_knn = optuna.create_study(direction=\"maximize\", study_name=\"KNN_SMOTE_Optuna\")\n",
        "\n",
        "study_knn.optimize(objective_knn, n_trials=100)\n",
        "\n",
        "\n",
        "print(f\"‚úÖ Mejor recall promedio: {study_knn.best_value:.4f}\")"
      ],
      "metadata": {
        "id": "7AkoKn9bHoIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# üìå Extraer mejores hiperpar√°metros\n",
        "best_params = study_knn.best_params\n",
        "print(\"‚öô Usando hiperpar√°metros:\", best_params)\n",
        "\n",
        "# üîÅ Aplicar SMOTE y entrenar el modelo\n",
        "smote_final = SMOTE(k_neighbors=best_params[\"smote_k\"], random_state=42)\n",
        "knn_final = KNeighborsClassifier(\n",
        "    n_neighbors=best_params[\"n_neighbors\"],\n",
        "    weights=best_params[\"weights\"],\n",
        "    metric=best_params[\"metric\"]\n",
        ")\n",
        "\n",
        "pipeline_knn_final = Pipeline([(\"smote\", smote_final), (\"knn\", knn_final)])\n",
        "pipeline_knn_final.fit(X_train_scaled, y_train)\n",
        "\n",
        "# üß™ Predicciones probabil√≠sticas y etiquetas\n",
        "y_proba_knn = pipeline_knn_final.predict_proba(X_test_scaled)[:, 1]\n",
        "y_pred_knn = (y_proba_knn >= 0.5).astype(int)\n",
        "\n",
        "# üìä Evaluaci√≥n\n",
        "print(\"üìä Evaluaci√≥n del Modelo KNN Final (umbral 0.5)\")\n",
        "print(classification_report(y_test, y_pred_knn, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"]))\n",
        "print(\"‚úî AUC:\", roc_auc_score(y_test, y_proba_knn))\n",
        "\n",
        "print(\"üìå Matriz de Confusi√≥n:\")\n",
        "print(confusion_matrix(y_test, y_pred_knn))\n",
        "\n",
        "# üß™ Visualizaci√≥n opcional de curva precision-recall\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_proba_knn)\n",
        "plt.plot(thresholds, precision[:-1], label=\"Precision\")\n",
        "plt.plot(thresholds, recall[:-1], label=\"Recall\")\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Precision & Recall vs Threshold (KNN)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZnAUdlsw9fny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# üìå Extraer mejores hiperpar√°metros\n",
        "best_params = study_knn.best_params\n",
        "print(\"‚öô Usando hiperpar√°metros:\", best_params)\n",
        "\n",
        "# Pipeline con SMOTE y KNN usando los mejores hiperpar√°metros\n",
        "smote = SMOTE(k_neighbors=best_params[\"smote_k\"], random_state=42)\n",
        "knn = KNeighborsClassifier(\n",
        "    n_neighbors=best_params[\"n_neighbors\"],\n",
        "    weights=best_params[\"weights\"],\n",
        "    metric=best_params[\"metric\"]\n",
        ")\n",
        "\n",
        "pipeline_knn = Pipeline([(\"smote\", smote), (\"knn\", knn)])\n",
        "pipeline_knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Probabilidades\n",
        "y_pred_probs_knn = pipeline_knn.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Evaluaci√≥n por threshold (como antes, pero ahora s√≠ con SMOTE)\n",
        "thresholds = np.arange(0.50, 0.81, 0.02)\n",
        "for thresh in thresholds:\n",
        "    print(f\"\\nüî∏ Evaluaci√≥n con umbral = {thresh:.2f}\")\n",
        "    y_pred = (y_pred_probs_knn >= thresh).astype(int)\n",
        "    print(classification_report(y_test, y_pred, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"]))\n",
        "    print(\"üìå Matriz de Confusi√≥n:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "8i2WHjFebGUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardar hiperparametros"
      ],
      "metadata": {
        "id": "EFZcLSXFbDLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Ruta al directorio en tu Google Drive\n",
        "ruta_carpeta_drive_knn = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano - Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/Modelos/KNN/'\n",
        "\n",
        "# Crear la carpeta si no existe\n",
        "os.makedirs(ruta_carpeta_drive_knn, exist_ok=True)\n",
        "\n",
        "# Nombre del archivo\n",
        "nombre_archivo = 'modelo_knn_opt.pkl'\n",
        "\n",
        "# Construir la ruta completa uniendo la carpeta con el nombre del archivo\n",
        "# NOTA: El primer argumento de os.path.join debe ser la ruta del directorio, no el modelo.\n",
        "ruta_modelo_completa = os.path.join(ruta_carpeta_drive_knn, nombre_archivo)\n",
        "\n",
        "# Guardar el pipeline completo (modelo entrenado con SMOTE)\n",
        "# La funci√≥n joblib.dump toma el objeto a guardar como primer argumento\n",
        "joblib.dump(pipeline_knn_final, ruta_modelo_completa)\n",
        "\n",
        "print(f\"‚úÖ Modelo KNN completo guardado en: {ruta_modelo_completa}\")"
      ],
      "metadata": {
        "id": "iKfcSEPQfNOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Catboost"
      ],
      "metadata": {
        "id": "mxrUmAzPVK87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import make_scorer, recall_score\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n"
      ],
      "metadata": {
        "id": "GJ1N6RioVLzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "Hr22YQbobJWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import make_scorer, recall_score\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Scorer personalizado para Recall de la clase 0\n",
        "scorer_recall_clase_0 = make_scorer(recall_score, pos_label=0,average='binary',zero_division=0)\n",
        "# Funci√≥n objetivo para Optuna\n",
        "def objective_catboost(trial):\n",
        "    # Hiperpar√°metros para CatBoost\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True) # Rango ajustado, log scale puede ser mejor\n",
        "    depth = trial.suggest_int(\"depth\", 4, 10)\n",
        "    l2_leaf_reg = trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0, log=True) # log scale puede ser mejor\n",
        "    iterations = trial.suggest_int(\"iterations\", 100, 1000) # Aumentamos el m√°ximo, early_stopping se encargar√°\n",
        "\n",
        "    # Nuevos par√°metros\n",
        "    random_strength = trial.suggest_float(\"random_strength\", 0.1, 2.0, log=True)\n",
        "    border_count = trial.suggest_int(\"border_count\", 32, 255) # Comunes 128 o 254\n",
        "    subsample = trial.suggest_float(\"subsample\", 0.6, 1.0) # Fracci√≥n de filas para cada √°rbol\n",
        "    min_data_in_leaf = trial.suggest_int(\"min_data_in_leaf\", 1, 30)\n",
        "\n",
        "    # Par√°metro para Early Stopping\n",
        "    early_stopping_rounds = trial.suggest_int(\"early_stopping_rounds\", 10, 100) # Cu√°ntas rondas sin mejora\n",
        "\n",
        "\n",
        "    smote_k = trial.suggest_int(\"smote_k\", 3, 7)\n",
        "\n",
        "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "\n",
        "    catboost = CatBoostClassifier(\n",
        "        learning_rate=learning_rate,\n",
        "        depth=depth,\n",
        "        l2_leaf_reg=l2_leaf_reg,\n",
        "        iterations=iterations, #\n",
        "        random_strength=random_strength,\n",
        "        border_count=border_count,\n",
        "        subsample=subsample,\n",
        "        min_data_in_leaf=min_data_in_leaf,\n",
        "        verbose=0,\n",
        "        random_state=42,\n",
        "        # CatBoost permite la evaluaci√≥n de m√©tricas durante el entrenamiento para early stopping\n",
        "        # Es importante que la m√©trica de evaluaci√≥n sea consistente con lo que quieres optimizar\n",
        "        eval_metric='Recall', # CatBoost usa Recall por defecto, si tu clase positiva es la 1.\n",
        "                              # Para Recall de Clase 0, CatBoost tiene 'Recall:class=0' o puedes usar 'F1' para balance\n",
        "        # Si 'Recall:class=0' no es directamente soportado, usa una m√©trica general como 'F1'\n",
        "        # o conf√≠a en la evaluaci√≥n externa de cross_val_score con tu custom scorer.\n",
        "        # En este caso, como usamos cross_val_score con nuestro scorer, 'eval_metric' es menos cr√≠tico,\n",
        "        # pero es bueno para el early stopping interno de CatBoost. Si 'Recall:class=0' no funciona,\n",
        "        # puedes usar 'Logloss' o 'F1' para el early stopping.\n",
        "    )\n",
        "    pipeline  = ImbPipeline([\n",
        "        (\"smote\", smote),\n",
        "        (\"catboost\", catboost)\n",
        "    ])\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Lista para almacenar los scores de cada fold\n",
        "    fold_scores = []\n",
        "\n",
        "    # Iterar a trav√©s de los folds para la validaci√≥n cruzada\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_scaled, y_train)):\n",
        "        X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        try:\n",
        "            # Para usar early_stopping_rounds, CatBoost necesita un eval_set durante el fit.\n",
        "            # Fit del pipeline (SMOTE se aplica solo al X_train_fold)\n",
        "            # Luego, el CatBoost dentro del pipeline se entrena con su propio eval_set\n",
        "            # Note: Para CatBoost en un pipeline, el eval_set debe pasarse al .fit() del pipeline,\n",
        "            # y el pipeline lo pasa al .fit() del estimador final si este lo soporta.\n",
        "            # En imblearn.pipeline, el fit del estimador final recibe el eval_set.\n",
        "\n",
        "            # Entrenamos el pipeline con el eval_set para CatBoost.\n",
        "            # Aseg√∫rate que tu versi√≥n de imblearn y scikit-learn manejen bien eval_set en pipelines.\n",
        "            # Si hay problemas, una alternativa es entrenar el CatBoost fuera del pipeline para el early stopping\n",
        "            # y luego re-entrenar el pipeline completo con los par√°metros finales.\n",
        "\n",
        "            # M√©todo 1: Pasar eval_set directamente al pipeline (preferido si es compatible)\n",
        "            # Algunas versiones de imblearn.pipeline o sklearn.pipeline no pasan kwargs a sub-estimadores\n",
        "            # de forma directa. Si esto falla, usa el M√©todo 2.\n",
        "\n",
        "            pipeline.fit(X_train_fold, y_train_fold,\n",
        "                         catboost__eval_set=(X_val_fold, y_val_fold),\n",
        "                         catboost__early_stopping_rounds=early_stopping_rounds)\n",
        "\n",
        "            # Predicci√≥n en el conjunto de validaci√≥n del fold\n",
        "            y_pred_val = pipeline.predict(X_val_fold)\n",
        "\n",
        "            # Calcular el score con tu scorer personalizado\n",
        "            score = scorer_recall_clase_0(pipeline, X_val_fold, y_val_fold)\n",
        "            fold_scores.append(score)\n",
        "\n",
        "        except Exception as e: # Captura excepciones m√°s generales para debug\n",
        "            print(f\"Trial fallido en fold {fold} por error: {e}\")\n",
        "            return 0.0 # O np.nan si prefieres que Optuna ignore estos trials\n",
        "\n",
        "    # Si todos los folds fallan, devuelve 0.0, de lo contrario, el promedio\n",
        "    if not fold_scores: # Si la lista est√° vac√≠a (todos los trials fallaron)\n",
        "        return 0.0\n",
        "    return np.mean(fold_scores)\n",
        "\n",
        "\n",
        "\n",
        "# Ejecutar la b√∫squeda\n",
        "study_catboost = optuna.create_study(direction=\"maximize\", study_name=\"CatBoost_SMOTE_Optuna_Recall0\")\n",
        "study_catboost.optimize(objective_catboost, n_trials=100, timeout=1800)\n",
        "\n",
        "# Mostrar mejores resultados\n",
        "print(\"üéØ Mejores hiperpar√°metros CatBoost encontrados:\")\n",
        "print(study_catboost.best_trial.params)\n",
        "print(f\"üìà Mejor recall clase 0 (promedio CV): {study_catboost.best_value:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "WpSBO9eXQAfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
        "import numpy as np\n",
        "\n",
        "# Usar los mejores hiperpar√°metros encontrados\n",
        "best_params = study_catboost.best_trial.params\n",
        "\n",
        "#  Aseg√∫rate de que `X_test_scaled` e `y_test` est√©n definidos correctamente\n",
        "\n",
        "# Reconstruir SMOTE y modelo con los mejores par√°metros\n",
        "smote = SMOTE(k_neighbors=best_params['smote_k'], random_state=42)\n",
        "\n",
        "catboost_final = CatBoostClassifier(\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    depth=best_params['depth'],\n",
        "    l2_leaf_reg=best_params['l2_leaf_reg'],\n",
        "    iterations=best_params['iterations'],\n",
        "    random_strength=best_params['random_strength'],\n",
        "    border_count=best_params['border_count'],\n",
        "    subsample=best_params['subsample'],\n",
        "    min_data_in_leaf=best_params['min_data_in_leaf'],\n",
        "    verbose=0,\n",
        "    random_state=42,\n",
        "    eval_metric='Recall' # O 'F1', o 'Recall:class=0' si CatBoost lo soporta para la clase 0\n",
        ")\n",
        "\n",
        "# Construir el pipeline\n",
        "pipeline = ImbPipeline([\n",
        "    (\"smote\", smote),\n",
        "    (\"catboost\", catboost_final)\n",
        "])\n",
        "\n",
        "# Entrenar el pipeline completo con todos los datos de entrenamiento\n",
        "pipeline.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Obtener probabilidades de predicci√≥n\n",
        "y_proba = pipeline.predict_proba(X_test_scaled)[:, 1]  # Probabilidades clase 1\n",
        "\n",
        "# Bucle de evaluaci√≥n con distintos umbrales\n",
        "for threshold in np.arange(0.50, 0.81, 0.02):\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "\n",
        "    print(f\"\\n Evaluaci√≥n con umbral = {threshold:.2f}\")\n",
        "    print(classification_report(y_test, y_pred, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"]))\n",
        "    print(\" Matriz de Confusi√≥n:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# AUC general (sin importar el threshold)\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"\\n AUC: {auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "KllOsUIaVGhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold_final = 0.80\n",
        "y_pred_final = (y_proba >= threshold_final).astype(int)\n",
        "\n",
        "print(\"üîç Evaluaci√≥n Final del Modelo CatBoost con Threshold =\", threshold_final)\n",
        "print(classification_report(y_test, y_pred_final, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"], zero_division=0))\n",
        "print(\"üìå Matriz de Confusi√≥n:\")\n",
        "print(confusion_matrix(y_test, y_pred_final, labels=[0, 1]))"
      ],
      "metadata": {
        "id": "uPcCiwfuWSzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Ruta en tu Google Drive\n",
        "ruta_carpeta_drive_catboost = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano - Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/Modelos/CatBoost/'\n",
        "\n",
        "# Crear la carpeta si no existe\n",
        "os.makedirs(ruta_carpeta_drive_catboost, exist_ok=True)\n",
        "\n",
        "# Nombre de archivo\n",
        "nombre_archivo_catboost = 'modelo_catboost_opt.pkl'\n",
        "ruta_modelo_catboost = os.path.join(ruta_carpeta_drive_catboost, nombre_archivo_catboost)\n",
        "\n",
        "# Guardar el pipeline completo (incluye SMOTE y CatBoost ya entrenado)\n",
        "joblib.dump(pipeline, ruta_modelo_catboost)\n",
        "\n",
        "print(f\"‚úÖ Modelo CatBoost guardado en: {ruta_modelo_catboost}\")\n"
      ],
      "metadata": {
        "id": "mYY09C36ndDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine - SVM"
      ],
      "metadata": {
        "id": "Fts295zvSd25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.svm import SVC\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score # Importa cross_val_score\n",
        "from sklearn.metrics import recall_score, make_scorer\n",
        "import numpy as np\n",
        "\n",
        "# Aseg√∫rate de que X_train_scaled y y_train est√©n definidos y listos para usar\n",
        "\n",
        "# Scorer personalizado para Recall de la clase 0\n",
        "# A√±adimos zero_division=0 para manejar casos donde un fold podr√≠a no tener instancias de la clase 0,\n",
        "# evitando errores de divisi√≥n por cero y devolviendo 0.0 en esos casos.\n",
        "scorer_recall_clase_0 = make_scorer(recall_score, pos_label=0, average='binary', zero_division=0)\n",
        "\n",
        "def objective_svm(trial):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    # Hiperpar√°metros para el clasificador SVM (SVC)\n",
        "    # Se utiliza el prefijo 'svc__' para indicar que son par√°metros del paso 'svc' en el pipeline.\n",
        "    C = trial.suggest_float('svc__C', 1e-2, 100, log=True) # Par√°metro de regularizaci√≥n\n",
        "    gamma = trial.suggest_categorical('svc__gamma', ['scale', 'auto', 0.01, 0.1, 1]) # Coeficiente del kernel\n",
        "    kernel = trial.suggest_categorical('svc__kernel', ['linear', 'rbf', 'poly']) # Tipo de kernel\n",
        "\n",
        "    # El par√°metro 'degree' solo es relevante para el kernel 'poly'.\n",
        "    # Optuna permite sugerencias condicionales de hiperpar√°metros.\n",
        "    if kernel == 'poly':\n",
        "        degree = trial.suggest_int('svc__degree', 2, 5)\n",
        "    else:\n",
        "        # Si el kernel no es 'poly', el valor de 'degree' es irrelevante,\n",
        "        # pero debe ser proporcionado. SVC lo ignorar√°.\n",
        "        degree = 3\n",
        "\n",
        "    # Hiperpar√°metro para el oversampling SMOTE\n",
        "    # Se utiliza el prefijo 'smote__' para indicar que es un par√°metro del paso 'smote' en el pipeline.\n",
        "    smote_k = trial.suggest_int('smote__k_neighbors', 3, 7) # N√∫mero de vecinos para SMOTE\n",
        "\n",
        "    # 1. Definir la instancia de SMOTE con el k_neighbors sugerido\n",
        "    smote_instance = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "\n",
        "    # 2. Definir la instancia de SVC con los hiperpar√°metros sugeridos\n",
        "    # probability=True es necesario para obtener probabilidades y luego ajustar umbrales.\n",
        "    svm_classifier = SVC(probability=True, C=C, gamma=gamma, kernel=kernel,\n",
        "                         degree=degree, random_state=42)\n",
        "\n",
        "    # 3. Construir el pipeline usando ImbPipeline (de imblearn)\n",
        "    # ImbPipeline asegura que SMOTE se aplica solo al conjunto de entrenamiento dentro de cada fold de CV.\n",
        "    pipeline = ImbPipeline([\n",
        "        ('smote', smote_instance), # Paso de oversampling\n",
        "        ('svc', svm_classifier)    # Paso del clasificador SVM\n",
        "    ])\n",
        "\n",
        "    # 4. Configurar la validaci√≥n cruzada estratificada\n",
        "    # shuffle=True y random_state para reproducibilidad\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    try:\n",
        "        # 5. Evaluar el pipeline usando cross_val_score\n",
        "        # Esto automatiza el bucle de CV, el fitting y la puntuaci√≥n de cada fold.\n",
        "        # scoring=scorer_recall_clase_0 asegura que se maximice el recall de la clase 0.\n",
        "        # n_jobs=-1 para usar todos los n√∫cleos disponibles de la CPU, acelerando la CV.\n",
        "        scores = cross_val_score(pipeline, X_train_scaled, y_train,\n",
        "                                 scoring=scorer_recall_clase_0, cv=cv, n_jobs=-1)\n",
        "\n",
        "        # Devolver el promedio de los scores de los folds\n",
        "        return np.mean(scores)\n",
        "    except Exception as e:\n",
        "        # Capturar cualquier excepci√≥n y devolver 0.0 para penalizar el trial fallido.\n",
        "        print(f\"Trial fallido por error: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "# 6. Ejecutar la b√∫squeda de hiperpar√°metros con Optuna\n",
        "# direction=\"maximize\" porque queremos maximizar el recall.\n",
        "# n_trials: n√∫mero de combinaciones de hiperpar√°metros a probar.\n",
        "# timeout: tiempo m√°ximo en segundos para ejecutar la optimizaci√≥n.\n",
        "study_svm = optuna.create_study(direction=\"maximize\", study_name=\"SVM_SMOTE_Recall0\")\n",
        "study_svm.optimize(objective_svm, n_trials=100, timeout=1800)\n",
        "\n",
        "# 7. Mostrar los mejores resultados obtenidos\n",
        "print(\"üéØ Mejores hiperpar√°metros encontrados para SVM:\")\n",
        "print(study_svm.best_trial.params)\n",
        "print(f\"üìà Mejor recall clase 0 (promedio CV): {study_svm.best_value:.4f}\")\n"
      ],
      "metadata": {
        "id": "LdmqaQH3qXF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "#  Extraer mejores hiperpar√°metros\n",
        "best_params = study_svm.best_trial.params\n",
        "\n",
        "#  Configurar el modelo y SMOTE con los mejores par√°metros\n",
        "smote = SMOTE(k_neighbors=best_params['smote__k_neighbors'], random_state=42)\n",
        "\n",
        "svm_final = SVC(\n",
        "    C=best_params['svc__C'],\n",
        "    gamma=best_params['svc__gamma'],\n",
        "    kernel=best_params['svc__kernel'],\n",
        "    degree=best_params.get('svc__degree', 3),  # solo importa si kernel = poly\n",
        "    probability=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "#  Construir pipeline final\n",
        "pipeline = ImbPipeline([\n",
        "    (\"smote\", smote),\n",
        "    (\"svc\", svm_final)\n",
        "])\n",
        "\n",
        "#  Entrenar el modelo final con todo el conjunto de entrenamiento\n",
        "pipeline.fit(X_train_scaled, y_train)\n",
        "\n",
        "#  Predicciones probabil√≠sticas\n",
        "y_proba = pipeline.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "#  Evaluaci√≥n con distintos umbrales\n",
        "print(\" Evaluaci√≥n del Modelo SVM Final con distintos umbrales:\\n\")\n",
        "for threshold in np.arange(0.50, 0.81, 0.02):\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "    print(f\" Umbral = {threshold:.2f}\")\n",
        "    print(classification_report(y_test, y_pred, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"], zero_division=0))\n",
        "    print(\" Matriz de Confusi√≥n:\")\n",
        "    print(confusion_matrix(y_test, y_pred, labels=[0, 1]))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "#  AUC general\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"\\n‚úî AUC: {auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "qD67Ww-E69z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Ruta destino\n",
        "ruta_guardado_svm = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano - Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/Modelos/SVM'\n",
        "os.makedirs(ruta_guardado_svm, exist_ok=True)\n",
        "\n",
        "# Nombre del archivo\n",
        "nombre_archivo = 'modelo_svm_opt.pkl'\n",
        "ruta_modelo_completa = os.path.join(ruta_guardado_svm, nombre_archivo)\n",
        "\n",
        "# Guardar el pipeline completo\n",
        "joblib.dump(pipeline, ruta_modelo_completa)\n",
        "print(f\"‚úÖ Modelo SVM final guardado en: {ruta_modelo_completa}\")\n"
      ],
      "metadata": {
        "id": "yeezk9997oQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "_nCHqVTVscc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import make_scorer, recall_score\n",
        "import numpy as np\n",
        "\n",
        "# ‚öôÔ∏è Scorer personalizado: Recall para clase 0\n",
        "scorer_recall_clase_0 = make_scorer(recall_score, pos_label=0, average='binary', zero_division=0)\n",
        "\n",
        "def objective_rf(trial):\n",
        "    # Hiperpar√°metros para Random Forest\n",
        "    n_estimators = trial.suggest_int(\"rf__n_estimators\", 100, 1000)\n",
        "    max_depth = trial.suggest_int(\"rf__max_depth\", 3, 50)\n",
        "    min_samples_split = trial.suggest_int(\"rf__min_samples_split\", 2, 22)\n",
        "    min_samples_leaf = trial.suggest_int(\"rf__min_samples_leaf\", 1, 22)\n",
        "    max_features = trial.suggest_categorical(\"rf__max_features\", ['sqrt', 'log2',None])\n",
        "    criterion = trial.suggest_categorical(\"rf__criterion\", ['gini', 'entropy'])\n",
        "    class_weight = trial.suggest_categorical(\"rf__class_weight\", [None, 'balanced', 'balanced_subsample'])\n",
        "    smote_k = trial.suggest_int(\"smote__k_neighbors\", 3, 7)\n",
        "\n",
        "    # Instanciar componentes\n",
        "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        criterion=criterion,\n",
        "        class_weight=class_weight,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Crear pipeline\n",
        "    pipeline = ImbPipeline([\n",
        "        (\"smote\", smote),\n",
        "        (\"rf\", rf)\n",
        "    ])\n",
        "\n",
        "    # Validaci√≥n cruzada estratificada\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    try:\n",
        "        scores = cross_val_score(pipeline, X_train_scaled, y_train,\n",
        "                                 scoring=scorer_recall_clase_0, cv=cv, n_jobs=-1)\n",
        "        return np.mean(scores)\n",
        "    except Exception as e:\n",
        "        print(f\" Trial fallido: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "# Ejecutar optimizaci√≥n con Optuna\n",
        "study_rf = optuna.create_study(direction=\"maximize\", study_name=\"RandomForest_SMOTE_Recall0\")\n",
        "study_rf.optimize(objective_rf, n_trials=100, timeout=1800)\n",
        "\n",
        "# Resultados\n",
        "print(\" Mejores hiperpar√°metros encontrados:\")\n",
        "print(study_rf.best_trial.params)\n",
        "print(f\" Mejor recall clase 0 (promedio CV): {study_rf.best_value:.4f}\")\n"
      ],
      "metadata": {
        "id": "oxoanB7j5GT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisar dimensiones\n",
        "print(\" X_train_scaled shape:\", X_train_scaled.shape)\n",
        "print(\" y_train shape:\", y_train.shape)\n",
        "\n",
        "# üìä Recuento por clase en y_train\n",
        "import numpy as np\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "print(\"Distribuci√≥n de clases en y_train:\")\n",
        "for label, count in zip(unique, counts):\n",
        "    print(f\"Clase {label}: {count} ejemplos\")\n"
      ],
      "metadata": {
        "id": "9_VkYQ6I7xCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Reconstruir modelo con mejores hiperpar√°metros\n",
        "best_params = study_rf.best_trial.params\n",
        "\n",
        "# Instanciar SMOTE y RF con mejores par√°metros\n",
        "smote = SMOTE(k_neighbors=best_params[\"smote__k_neighbors\"], random_state=42)\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=best_params[\"rf__n_estimators\"],\n",
        "    max_depth=best_params[\"rf__max_depth\"],\n",
        "    min_samples_split=best_params[\"rf__min_samples_split\"],\n",
        "    min_samples_leaf=best_params[\"rf__min_samples_leaf\"],\n",
        "    max_features=best_params[\"rf__max_features\"],\n",
        "    criterion=best_params[\"rf__criterion\"],\n",
        "    class_weight=best_params[\"rf__class_weight\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Crear pipeline final\n",
        "pipeline_rf_final = ImbPipeline([\n",
        "    (\"smote\", smote),\n",
        "    (\"rf\", rf_model)\n",
        "])\n",
        "\n",
        "# Entrenar modelo con todo el set de entrenamiento\n",
        "pipeline_rf_final.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Probabilidades para clase 0\n",
        "y_proba = pipeline_rf_final.predict_proba(X_test_scaled)[:, 0]  # Probabilidad de clase 0 (reprobar)\n",
        "\n",
        "# AUC general\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"‚úî AUC: {auc:.4f}\\n\")\n",
        "\n",
        "# üìä Evaluaci√≥n con distintos thresholds\n",
        "thresholds = np.arange(0.50, 0.81, 0.02)\n",
        "\n",
        "for threshold in thresholds:\n",
        "    print(f\"üî∏ Umbral = {threshold:.2f}\")\n",
        "\n",
        "    # Convertir probabilidades a clases seg√∫n threshold\n",
        "    y_pred_thresh = (y_proba >= threshold).astype(int)\n",
        "\n",
        "    print(classification_report(y_test, y_pred_thresh, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"]))\n",
        "    print(\"üìå Matriz de Confusi√≥n:\")\n",
        "    print(confusion_matrix(y_test, y_pred_thresh))\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "ZTdb9S2dAdLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Ruta en tu Google Drive\n",
        "ruta_carpeta_rf = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano - Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/Modelos/RandomForest/'\n",
        "os.makedirs(ruta_carpeta_rf, exist_ok=True)\n",
        "\n",
        "# Ruta final del archivo\n",
        "ruta_guardado_rf = os.path.join(ruta_carpeta_rf, 'modelo_rf_optuna.pkl')\n",
        "\n",
        "# Guardar el pipeline entrenado\n",
        "joblib.dump(pipeline_rf_final, ruta_guardado_rf)\n",
        "\n",
        "print(f\"‚úÖ Modelo RF (Optuna) guardado en: {ruta_guardado_rf}\")\n"
      ],
      "metadata": {
        "id": "HWNaMhBtgt1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "id": "L44mi_8mDLuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP"
      ],
      "metadata": {
        "id": "0yYihGvYD2XQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input # Importar Input expl√≠citamente\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import recall_score, classification_report, confusion_matrix, roc_auc_score, accuracy_score, f1_score, make_scorer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "import traceback\n",
        "import os\n",
        "\n",
        "# --- Comprobaciones iniciales de datos y conversi√≥n a NumPy ---\n",
        "# Es preferible trabajar con arrays de NumPy para Keras, SMOTE y Sci-kit Learn\n",
        "# para evitar problemas de indexaci√≥n que pueden ocurrir con Pandas DataFrames/Series\n",
        "# si tienen √≠ndices no secuenciales.\n",
        "\n",
        "\n",
        "# Convertir X_train_scaled y y_train a arrays de NumPy\n",
        "X_train_scaled_np = X_train_scaled.values if hasattr(X_train_scaled, 'values') else X_train_scaled\n",
        "y_train_np = y_train.values if hasattr(y_train, 'values') else y_train\n",
        "\n",
        "# Asegurarse de que los datos de prueba tambi√©n est√©n en formato NumPy\n",
        "X_test_scaled_np = X_test_scaled.values if 'X_test_scaled' in locals() and hasattr(X_test_scaled, 'values') else X_test_scaled\n",
        "y_test_np = y_test.values if 'y_test' in locals() and hasattr(y_test, 'values') else y_test\n",
        "\n",
        "\n",
        "# --- PASO 1: Funci√≥n para Crear el Modelo Keras ---\n",
        "def create_simple_keras_model(input_dim, optimizer_config=('adam', 0.001), layer_config=((64, 0.2), (32, 0.2)), activation_config='relu'):\n",
        "    \"\"\"\n",
        "    Crea un modelo Keras secuencial con capas Dense y Dropout.\n",
        "    Acepta configuraci√≥n de optimizador, capas ocultas y activaci√≥n.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Se a√±ade la capa Input expl√≠citamente para evitar la UserWarning de Keras.\n",
        "    model.add(Input(shape=(input_dim,)))\n",
        "\n",
        "    # Primera capa Dense (ya no necesita input_shape/input_dim aqu√≠)\n",
        "    model.add(Dense(layer_config[0][0], activation=activation_config))\n",
        "    if layer_config[0][1] > 0: # Si hay dropout en la primera capa\n",
        "        model.add(Dropout(layer_config[0][1]))\n",
        "\n",
        "    # A√±adir capas ocultas adicionales (a partir de la segunda)\n",
        "    for neurons, dropout in layer_config[1:]: # Iterar desde el segundo elemento en adelante\n",
        "        if neurons > 0:\n",
        "            model.add(Dense(neurons, activation=activation_config))\n",
        "            if dropout > 0:\n",
        "                model.add(Dropout(dropout))\n",
        "\n",
        "    # Capa de salida para clasificaci√≥n binaria\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Configurar el optimizador\n",
        "    optimizer_name, learning_rate = optimizer_config\n",
        "    if optimizer_name.lower() == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name.lower() == 'rmsprop':\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "    else: # Por defecto a Adam\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # Definir m√©trica de recall para la clase 0\n",
        "    recall_metric_clase_0 = keras.metrics.Recall(class_id=0, name=\"recall_clase_0\")\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy', recall_metric_clase_0])\n",
        "    return model\n",
        "\n",
        "# --- PASO 2: Definir la Funci√≥n Objetivo para Optuna ---\n",
        "def objective_simple_mlp(trial):\n",
        "    K.clear_session() # Limpiar la sesi√≥n de Keras para cada trial\n",
        "\n",
        "    # Hiperpar√°metros para SMOTE\n",
        "    smote_k_val = trial.suggest_categorical('smote_k_neighbors', [3, 5, 7])\n",
        "\n",
        "    # Hiperpar√°metros para Keras MLP\n",
        "    n_hidden_layers = trial.suggest_int('n_hidden_layers', 1, 2)\n",
        "    layer_config_list = []\n",
        "    for i in range(n_hidden_layers):\n",
        "        neurons = trial.suggest_categorical(f'neurons_l{i}', [32, 64, 128])\n",
        "        dropout = trial.suggest_categorical(f'dropout_l{i}', [0.2, 0.3, 0.4, 0.5])\n",
        "        layer_config_list.append((neurons, dropout))\n",
        "    layer_config_val = tuple(layer_config_list)\n",
        "\n",
        "    activation_val = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
        "    optimizer_name_val = trial.suggest_categorical('optimizer_name', ['adam', 'rmsprop'])\n",
        "    learning_rate_val = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "\n",
        "    epochs_val = trial.suggest_categorical('epochs', [25, 50,75,100])\n",
        "    batch_size_val = trial.suggest_categorical('batch_size', [32, 64])\n",
        "\n",
        "    # Configuraci√≥n de Early Stopping\n",
        "    early_stopping_patience = trial.suggest_int('early_stopping_patience', 5, 15)\n",
        "\n",
        "    # Validaci√≥n cruzada estratificada\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_recall_scores_clase_0 = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_scaled_np, y_train_np)):\n",
        "        X_train_fold_clean, X_val_fold_clean = X_train_scaled_np[train_idx], X_train_scaled_np[val_idx]\n",
        "        y_train_fold_clean, y_val_fold_clean = y_train_np[train_idx], y_train_np[val_idx]\n",
        "\n",
        "        # Aplicar SMOTE solo al conjunto de entrenamiento del fold\n",
        "        sm = SMOTE(random_state=42, k_neighbors=smote_k_val)\n",
        "        try:\n",
        "            X_train_fold_res, y_train_fold_res = sm.fit_resample(X_train_fold_clean, y_train_fold_clean)\n",
        "        except ValueError as smote_err:\n",
        "            print(f\"‚ö†Ô∏è SMOTE fall√≥ en fold {fold} con k_neighbors={smote_k_val}: {smote_err}. Penalizando.\")\n",
        "            fold_recall_scores_clase_0.append(0.0)\n",
        "            continue\n",
        "\n",
        "        # Crear el modelo Keras con los hiperpar√°metros del trial\n",
        "        model = create_simple_keras_model(\n",
        "            input_dim=X_train_fold_res.shape[1],\n",
        "            optimizer_config=(optimizer_name_val, learning_rate_val),\n",
        "            layer_config=layer_config_val,\n",
        "            activation_config=activation_val\n",
        "        )\n",
        "\n",
        "        # Configurar el callback de Early Stopping\n",
        "        early_stopping_cb = EarlyStopping(\n",
        "            monitor='val_recall_clase_0',\n",
        "            patience=early_stopping_patience,\n",
        "            mode='max',\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Entrenar el modelo\n",
        "        history = model.fit(X_train_fold_res, y_train_fold_res,\n",
        "                            epochs=epochs_val,\n",
        "                            batch_size=batch_size_val,\n",
        "                            validation_data=(X_val_fold_clean, y_val_fold_clean),\n",
        "                            callbacks=[early_stopping_cb],\n",
        "                            verbose=0)\n",
        "\n",
        "        # Evaluar el modelo en el conjunto de validaci√≥n del fold\n",
        "        _, _, val_recall_clase_0_metric_value = model.evaluate(X_val_fold_clean, y_val_fold_clean, verbose=0)\n",
        "\n",
        "        fold_recall_scores_clase_0.append(val_recall_clase_0_metric_value)\n",
        "\n",
        "        # Reportar el score actual del fold a Optuna para poda\n",
        "        trial.report(val_recall_clase_0_metric_value, fold)\n",
        "\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    mean_recall_clase_0 = np.mean(fold_recall_scores_clase_0) if fold_recall_scores_clase_0 else 0.0\n",
        "    return mean_recall_clase_0\n",
        "\n",
        "# --- PASO 3: Crear un Estudio de Optuna y Ejecutar la Optimizaci√≥n ---\n",
        "study_name = \"MLP_Keras_Optuna_Recall0\"\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0, interval_steps=1)\n",
        "\n",
        "study_mlp_optuna = optuna.create_study(direction=\"maximize\", study_name=study_name, sampler=sampler, pruner=pruner)\n",
        "\n",
        "n_trials_optuna = 100\n",
        "timeout_seconds = 3600 # 1 hora de timeout\n",
        "\n",
        "print(f\" Iniciando optimizaci√≥n con Optuna para Keras MLP ({n_trials_optuna} trials, timeout: {timeout_seconds/60} min)...\")\n",
        "try:\n",
        "    study_mlp_optuna.optimize(objective_simple_mlp, n_trials=n_trials_optuna, timeout=timeout_seconds)\n",
        "    print(\" Optimizaci√≥n con Optuna para MLP Keras completada.\")\n",
        "\n",
        "    if study_mlp_optuna.best_trial:\n",
        "        print(f\"\\n Mejores hiperpar√°metros: {study_mlp_optuna.best_trial.params}\")\n",
        "        print(f\" Mejor Recall Clase 0.0 (promedio CV): {study_mlp_optuna.best_trial.value:.4f}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Optuna no encontr√≥ un 'best_trial' (posiblemente por timeout o todos los trials fallaron).\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Ocurri√≥ un error INESPERADO durante la optimizaci√≥n con Optuna:\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Oasjet27Gu3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 4: Crear y Entrenar el Modelo Final con los Mejores Hiperpar√°metros ---\n",
        "\n",
        "\n",
        "best_params = study_mlp_optuna.best_trial.params # Acceso directo a los mejores par√°metros\n",
        "\n",
        "# Preparar configuraci√≥n del modelo final\n",
        "input_dim = X_train_scaled_np.shape[1]\n",
        "\n",
        "# Reconstruir la configuraci√≥n de las capas din√°micamente a partir de best_params\n",
        "layer_config_list = []\n",
        "num_hidden_layers_optuna = best_params.get('n_hidden_layers', 1)\n",
        "for i in range(num_hidden_layers_optuna):\n",
        "    neurons = best_params.get(f'neurons_l{i}', 64)\n",
        "    dropout = best_params.get(f'dropout_l{i}', 0.5)\n",
        "    layer_config_list.append((neurons, dropout))\n",
        "layer_config_final = tuple(layer_config_list)\n",
        "\n",
        "# Crear la instancia del modelo Keras final\n",
        "final_model = create_simple_keras_model(\n",
        "    input_dim=input_dim,\n",
        "    optimizer_config=(best_params['optimizer_name'], best_params['learning_rate']),\n",
        "    layer_config=layer_config_final,\n",
        "    activation_config=best_params['activation']\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento completo (usando los k_neighbors √≥ptimos)\n",
        "smote_final_k = best_params.get('smote_k_neighbors', 5)\n",
        "smote_final = SMOTE(k_neighbors=smote_final_k, random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote_final.fit_resample(X_train_scaled_np, y_train_np.astype(int))\n",
        "\n",
        "# Configurar Early Stopping para el entrenamiento final\n",
        "early_stopping_final_cb = EarlyStopping(\n",
        "    monitor='val_recall_clase_0', # Monitorea el recall de la clase 0 en el conjunto de validaci√≥n\n",
        "    patience=6, #best_params['early_stopping_patience'], # Paciencia √≥ptima\n",
        "    min_delta=0.02,\n",
        "    mode='max', # Buscamos maximizar el recall\n",
        "    restore_best_weights=True, # Restaurar los pesos del mejor epoch\n",
        "    verbose=1 # Mostrar el progreso del early stopping\n",
        ")\n",
        "\n",
        "print(f\"\\n Entrenando el modelo final con los mejores hiperpar√°metros: {best_params}\")\n",
        "# Entrenar el modelo final y CAPTURAR EL HISTORIAL en la variable 'history' (global)\n",
        "history = final_model.fit( # <-- LA VARIABLE 'history' GLOBAL SE DEFINE AQU√ç\n",
        "    X_train_resampled, y_train_resampled,\n",
        "    epochs=best_params['epochs'], # √âpocas m√°ximas √≥ptimas\n",
        "    batch_size=best_params['batch_size'], # Batch size √≥ptimo\n",
        "    validation_split=0.25, # Usar una parte de los datos remuestreados para validaci√≥n\n",
        "    callbacks=[early_stopping_final_cb], # Aplicar Early Stopping\n",
        "    verbose=1 # Mostrar el progreso del entrenamiento\n",
        ")\n"
      ],
      "metadata": {
        "id": "enAZ-n0cmTSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 5: Evaluaci√≥n en el conjunto de prueba ---\n",
        "\n",
        "print(\" Bloque 5: Modelo final entrenado y historial capturado.\")\n",
        "y_pred_probs = final_model.predict(X_test_scaled_np, verbose=0)\n",
        "y_pred_probs_flat = y_pred_probs.flatten() # Asegura que sea un array 1D\n",
        "\n",
        "print(\"\\nüìä Bloque 6: Evaluaci√≥n del Modelo MLP Final con distintos umbrales en el conjunto de prueba:\\n\")\n",
        "for threshold in np.arange(0.50, 0.81, 0.02): # Rango de umbrales para explorar\n",
        "    y_pred_thresholded = (y_pred_probs_flat >= threshold).astype(int) # Predicciones binarizadas\n",
        "\n",
        "    print(f\" Umbral = {threshold:.2f}\")\n",
        "    print(classification_report(y_test_np, y_pred_thresholded, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"], zero_division=0))\n",
        "    print(\" Matriz de Confusi√≥n:\")\n",
        "    print(confusion_matrix(y_test_np, y_pred_thresholded, labels=[0, 1]))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# M√©tricas Globales (independientes del umbral, eval√∫an la capacidad de ranking)\n",
        "print(\"\\n--- M√©tricas Globales del Modelo (independientes del umbral) ---\")\n",
        "auc = roc_auc_score(y_test_np, y_pred_probs_flat)\n",
        "print(f\"‚úî ROC AUC: {auc:.4f}\")\n",
        "print(\" Bloque 6: Evaluaci√≥n en el conjunto de prueba completada.\")"
      ],
      "metadata": {
        "id": "ScKIlt91l08Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 6: Graficar el Historial de Entrenamiento para Diagn√≥stico de Overfitting/Underfitting ---\n",
        "\n",
        "print(\"\\n Bloque 7: Generando gr√°ficos del historial de entrenamiento para diagnosticar Overfitting/Underfitting\")\n",
        "\n",
        "# Define la ruta para guardar los gr√°ficos\n",
        "#ruta_carpeta_graficos = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano - Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/Modelos/MLP/graficos/'\n",
        "#os.makedirs(ruta_carpeta_graficos, exist_ok=True) # Crea la carpeta si no existe\n",
        "\n",
        "# Gr√°fico de la p√©rdida (Loss)\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='P√©rdida de Entrenamiento')\n",
        "if 'val_loss' in history.history:\n",
        "    plt.plot(history.history['val_loss'], label='P√©rdida de Validaci√≥n')\n",
        "plt.title('P√©rdida (Loss) durante el Entrenamiento')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.ylabel('P√©rdida')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "#plt.savefig(os.path.join(ruta_carpeta_graficos, 'loss_plot.png')) # Guardar gr√°fico de p√©rdida\n",
        "\n",
        "# Gr√°fico del Recall de la Clase 0\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['recall_clase_0'], label='Recall Clase 0 (Entrenamiento)')\n",
        "if 'val_recall_clase_0' in history.history:\n",
        "    plt.plot(history.history['val_recall_clase_0'], label='Recall Clase 0 (Validaci√≥n)')\n",
        "plt.title('Recall de la Clase 0 durante el Entrenamiento')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "#plt.savefig(os.path.join(ruta_carpeta_graficos, 'recall_clase0_plot.png')) # Guardar gr√°fico de recall\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show() # Muestra los gr√°ficos en la salida del Notebook/Canvas\n",
        "\n",
        "print(\"‚úÖ Bloque 7: Gr√°ficos generados y guardados en Google Drive.\")"
      ],
      "metadata": {
        "id": "nCLKtKMBnTB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 7: Guardar el Modelo Final ---\n",
        "\n",
        "\n",
        "ruta_carpeta_modelo = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano - Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/Modelos/MLP/'\n",
        "os.makedirs(ruta_carpeta_modelo, exist_ok=True)\n",
        "\n",
        "nombre_archivo_modelo_mlp = 'modelo_optuna_final.keras' # Nombre que ya decidimos\n",
        "ruta_modelo_completa_mlp = os.path.join(ruta_carpeta_modelo, nombre_archivo_modelo_mlp)\n",
        "\n",
        "final_model.save(ruta_modelo_completa_mlp)\n",
        "print(f\"‚úÖ Bloque 8: Modelo MLP entrenado y guardado en: {ruta_modelo_completa_mlp}\")"
      ],
      "metadata": {
        "id": "hq1s-21RnRmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNN OPTUNA"
      ],
      "metadata": {
        "id": "Cq0B_Q0nb9Nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Par√°metros:\n",
        "    - input_dim: Dimensi√≥n de entrada (n√∫mero de features).\n",
        "    - n_hidden_layers: N√∫mero de capas ocultas.\n",
        "    - neurons_list: Lista con la cantidad de neuronas por capa.\n",
        "    - dropout_list: Lista con valores de dropout por capa (opcional).\n",
        "    - activation: Funci√≥n de activaci√≥n (por defecto 'relu')."
      ],
      "metadata": {
        "id": "OvbwVyleconR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import recall_score,f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from optuna.exceptions import TrialPruned\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "QqpB1AyooqNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dnn_model(input_dim, optimizer_config=('adam', 0.001), layer_config=((128, 0.3),), activation_config='relu'):\n",
        "    \"\"\"\n",
        "    Crea y compila un modelo de red neuronal densa (DNN).\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))\n",
        "    for neurons, dr_rate in layer_config:\n",
        "        if neurons > 0:\n",
        "            model.add(Dense(neurons, activation=activation_config))\n",
        "            if dr_rate > 0:\n",
        "                model.add(Dropout(dr_rate))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    optimizer_name, learning_rate = optimizer_config\n",
        "    opt = Adam(learning_rate=learning_rate) if optimizer_name == 'adam' else RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    # Se mantiene el recall para poder graficarlo, pero no ser√° el objetivo principal\n",
        "    recall_c0 = keras.metrics.Recall(class_id=0, name=\"recall_clase_0\")\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', recall_c0])\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# Funcion objetivo\n",
        "\n",
        "def objective_dnn_optuna_f1(trial):\n",
        "\n",
        "    K.clear_session()\n",
        "\n",
        "    # --- CAMBIO: Espacio de b√∫squeda ajustado para modelos m√°s robustos ---\n",
        "    smote_k = trial.suggest_categorical('smote_k', [3, 5, 7])\n",
        "    n_layers = trial.suggest_int('n_hidden_layers', 1, 3) # Menos capas para evitar sobreajuste\n",
        "\n",
        "    layer_conf = []\n",
        "    for i in range(n_layers):\n",
        "        neurons = trial.suggest_categorical(f'n_units_l{i}', [32, 64, 96]) # Menos neuronas\n",
        "        dropout = trial.suggest_float(f'dropout_l{i}', 0.2, 0.6, step=0.1) # Mayor rango de dropout\n",
        "        layer_conf.append((neurons, dropout))\n",
        "\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])\n",
        "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
        "    epochs = trial.suggest_categorical('epochs', [25, 50, 75, 100])\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "    f1_scores = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
        "        X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        try:\n",
        "            sm = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "            X_train_res, y_train_res = sm.fit_resample(X_train_fold, y_train_fold)\n",
        "        except ValueError:\n",
        "            raise TrialPruned()\n",
        "\n",
        "        model = create_dnn_model(input_dim=X_train_res.shape[1], optimizer_config=(optimizer_name, lr), layer_config=tuple(layer_conf), activation_config=activation)\n",
        "\n",
        "        # El EarlyStopping ahora puede monitorear la p√©rdida de validaci√≥n, que es m√°s estable\n",
        "        cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True, verbose=0)\n",
        "\n",
        "        model.fit(X_train_res, y_train_res, epochs=epochs, batch_size=batch_size, validation_data=(X_val_fold, y_val_fold), callbacks=[cb], verbose=0)\n",
        "\n",
        "        # --- CAMBIO: Evaluaci√≥n basada en F1-Score ---\n",
        "        y_pred_proba = model.predict(X_val_fold, verbose=0)\n",
        "        y_pred_classes = (y_pred_proba > 0.5).astype(\"int32\")\n",
        "\n",
        "        # Se calcula el F1 para la clase 0 (reprobados). zero_division=0 evita errores si no hay predicciones.\n",
        "        score = f1_score(y_val_fold, y_pred_classes, pos_label=0, zero_division=0)\n",
        "        f1_scores.append(score)\n",
        "\n",
        "    return np.mean(f1_scores)\n",
        "\n",
        "\n",
        "\n",
        "study_dnn = optuna.create_study(direction=\"maximize\", study_name=\"DNN_F1_Score_Optuna\")\n",
        "# CAMBIO: La funci√≥n objetivo ahora es la que optimiza el F1\n",
        "study_dnn.optimize(objective_dnn_optuna_f1, n_trials=100, timeout=3600)\n",
        "\n",
        "print(\"\\n Mejor Trial (Optimizado para F1-Score):\")\n",
        "# CAMBIO: El valor mostrado ahora es el F1-Score\n",
        "print(f\"   Valor (F1-Score clase 0): {study_dnn.best_value:.4f}\")\n",
        "best_params = study_dnn.best_trial.params\n",
        "print(\"  Hiperpar√°metros:\")\n",
        "for key, value in best_params.items():\n",
        "    print(f\"    - {key}: {value}\")\n"
      ],
      "metadata": {
        "id": "cjuj7iHTtmLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import Callback # Import the Callback class\n",
        "from imblearn.over_sampling import SMOTE # Ensure SMOTE is imported\n",
        "\n",
        "\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(self, validation_data, pos_label=0):\n",
        "        super().__init__()\n",
        "        self.X_val, self.y_val = validation_data\n",
        "        self.pos_label = pos_label\n",
        "        self.train_f1s = [] # List to store F1-scores for training set\n",
        "        self.val_f1s = []   # List to store F1-scores for validation set\n",
        "        # Store the validation data to be used directly\n",
        "        self.val_data_for_pred = validation_data # Store explicitly for prediction\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # Predict on the validation data passed to the callback\n",
        "        y_val_pred_proba = self.model.predict(self.val_data_for_pred[0], verbose=0)[:, 0]\n",
        "\n",
        "        # Using the default 0.5 threshold for F1 calculation in the callback\n",
        "        # y_train_pred_classes = (y_train_pred_proba >= 0.5).astype(int) # This was the line causing error\n",
        "        y_val_pred_classes = (y_val_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "        val_f1 = f1_score(self.val_data_for_pred[1], y_val_pred_classes, pos_label=self.pos_label, zero_division=0)\n",
        "        self.val_f1s.append(val_f1)     # Append F1 of the explicit validation_data set\n",
        "\n",
        "        # Optionally print the F1 score at the end of the epoch\n",
        "        # logs['f1_class_0_explicit'] = val_f1 # You could add this to logs if needed, but logging requires specific Keras versions/methods\n",
        "        print(f\" - explicit_val_f1_class_0: {val_f1:.4f}\") # Print explicitly\n",
        "\n",
        "\n",
        "def plot_learning_curves(history, f1_callback):\n",
        "    \"\"\"\n",
        "    Genera gr√°ficos para las curvas de aprendizaje (p√©rdida y F1-Score).\n",
        "    \"\"\"\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Gr√°fico de P√©rdida (Loss)\n",
        "    ax1.plot(history.history['loss'], label='P√©rdida de Entrenamiento', color='dodgerblue')\n",
        "    # Check if validation loss is available (it should be with validation_data)\n",
        "    if 'val_loss' in history.history:\n",
        "        ax1.plot(history.history['val_loss'], label='P√©rdida de Validaci√≥n', color='darkorange', linestyle='--')\n",
        "    ax1.set_title('Curva de P√©rdida del Modelo', fontsize=16)\n",
        "    ax1.set_xlabel('√âpoca'); ax1.set_ylabel('Loss'); ax1.legend()\n",
        "\n",
        "    # Gr√°fico de F1-Score\n",
        "    # Use the recorded F1 scores from the callback for the explicit validation set\n",
        "    # We are no longer calculating/plotting a separate 'training' F1 in the callback for simplicity/speed\n",
        "    ax2.plot(f1_callback.val_f1s, label='F1-Score Validaci√≥n (Clase 0 - Explicit)', color='darkorange', linestyle='--') # Clarify label\n",
        "    ax2.set_title('Curva de F1-Score del Modelo (Clase 0) en Validaci√≥n', fontsize=16)\n",
        "    ax2.set_xlabel('√âpoca'); ax2.set_ylabel('F1-Score'); ax2.legend()\n",
        "\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- Entrenando modelo final con los mejores hiperpar√°metros para visualizaci√≥n ---\")\n",
        "\n",
        "# 1. Extraer los mejores hiperpar√°metros\n",
        "# Assume best_params, X_train_scaled, y_train are defined from previous cells\n",
        "\n",
        "best_params = study_dnn.best_trial.params\n",
        "final_layer_conf = []\n",
        "for i in range(best_params['n_hidden_layers']):\n",
        "    final_layer_conf.append((best_params[f'n_units_l{i}'], best_params[f'dropout_l{i}']))\n",
        "\n",
        "# 2. Aplicar SMOTE al conjunto de entrenamiento completo\n",
        "sm = SMOTE(k_neighbors=best_params['smote_k'], random_state=42)\n",
        "X_train_final_res, y_train_final_res = sm.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Crear y compilar el modelo final\n",
        "\n",
        "final_model = create_dnn_model(\n",
        "    input_dim=X_train_final_res.shape[1],\n",
        "    optimizer_config=(best_params['optimizer'], best_params['lr']),\n",
        "    layer_config=tuple(final_layer_conf),\n",
        "    activation_config=best_params['activation']\n",
        ")\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_fit, X_val_fit, y_train_fit, y_val_fit = train_test_split(\n",
        "    X_train_final_res, y_train_final_res,\n",
        "    test_size=0.2, # Use 20% of the resampled data for validation\n",
        "    random_state=42,\n",
        "    stratify=y_train_final_res # Maintain class distribution\n",
        ")\n",
        "\n",
        "f1_callback_instance = F1ScoreCallback(validation_data=(X_val_fit, y_val_fit), pos_label=0)\n",
        "\n",
        "\n",
        "\n",
        "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=10, mode='min',\n",
        "    restore_best_weights=True, verbose=1 # Set verbose to 1 to see when it stops\n",
        ")\n",
        "\n",
        "\n",
        "# 4. Entrenar el modelo final, passing the callbacks\n",
        "history_final = final_model.fit(\n",
        "    X_train_fit, # Use the training split of the resampled data\n",
        "    y_train_fit, # Use the training split of the resampled data\n",
        "    epochs= best_params['epochs'],\n",
        "    batch_size=best_params['batch_size'],\n",
        "    validation_data=(X_val_fit, y_val_fit), # Use the validation split of the resampled data\n",
        "    callbacks=[f1_callback_instance, early_stopping_callback], # Pass the callback instance\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 5. Generar y mostrar los gr√°ficos, passing both history and the callback instance\n",
        "plot_learning_curves(history_final, f1_callback_instance)"
      ],
      "metadata": {
        "id": "qYL2yZx4qFvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Predecir probabilidades\n",
        "y_pred_proba_dnn = final_model.predict(X_test_scaled).ravel()\n",
        "\n",
        "# 2. Usar threshold est√°ndar (0.5)\n",
        "y_pred_dnn = (y_pred_proba_dnn >= 0.5).astype(int)\n",
        "\n",
        "# 3. Reporte de clasificaci√≥n\n",
        "print(\"üìä Reporte de Clasificaci√≥n (Prueba):\")\n",
        "print(classification_report(y_test, y_pred_dnn, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"], zero_division=0))\n",
        "\n",
        "# 4. M√©tricas generales\n",
        "print(f\" Accuracy: {accuracy_score(y_test, y_pred_dnn):.4f}\")\n",
        "print(f\"F1 Macro: {f1_score(y_test, y_pred_dnn, average='macro'):.4f}\")\n",
        "print(f\" AUC-ROC: {roc_auc_score(y_test, y_pred_proba_dnn):.4f}\")\n",
        "\n",
        "# 5. Matriz de Confusi√≥n\n",
        "cm = confusion_matrix(y_test, y_pred_dnn)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Reprobar (0)\", \"Aprobar (1)\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Matriz de Confusi√≥n - DNN\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RTJcsCFrb92v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# 1. Obtener las probabilidades para la clase positiva (Aprobar)\n",
        "y_pred_proba_test_dnn = final_model.predict(X_test_scaled).ravel()  # aplanar por si devuelve (N, 1)\n",
        "\n",
        "# 2. Definir thresholds a evaluar\n",
        "thresholds = np.arange(0.50, 0.81, 0.02)\n",
        "\n",
        "# 3. Evaluar para cada threshold\n",
        "for thresh in thresholds:\n",
        "    print(f\"\\n--- Threshold: {thresh:.2f} ---\")\n",
        "\n",
        "    # Convertir probabilidades en clases seg√∫n el threshold\n",
        "    y_pred_thresh = (y_pred_proba_test_dnn >= thresh).astype(int)\n",
        "\n",
        "    # Matriz de confusi√≥n\n",
        "    cm = confusion_matrix(y_test, y_pred_thresh)\n",
        "\n",
        "    # Reporte de clasificaci√≥n\n",
        "    print(\"Matriz de Confusi√≥n:\")\n",
        "    print(cm)\n",
        "    print(\"Reporte de Clasificaci√≥n:\")\n",
        "    print(classification_report(y_test, y_pred_thresh, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"]))\n",
        "\n",
        "    # Accuracy y F1 Macro\n",
        "    acc = accuracy_score(y_test, y_pred_thresh)\n",
        "    f1 = f1_score(y_test, y_pred_thresh, average='macro')\n",
        "\n",
        "    # AUC (aunque no depende del threshold)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba_test_dnn)\n",
        "\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1 Macro: {f1:.4f}\")\n",
        "    print(f\"AUC-ROC (fijo): {auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "CQ_TUJDE4oLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ruta donde guardar en tu Google Drive (ajusta si es necesario)\n",
        "ruta_modelo_dnn = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano - Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/Modelos/DNN'\n",
        "os.makedirs(ruta_modelo_dnn, exist_ok=True)\n",
        "\n",
        "# Guardar el modelo en formato .h5\n",
        "ruta_modelo_h5 = os.path.join(ruta_modelo_dnn, 'modelo_dnn_optuna_068.h5')\n",
        "final_model.save(ruta_modelo_h5)\n",
        "\n",
        "print(f\"‚úÖ Modelo DNN guardado exitosamente en: {ruta_modelo_h5}\")\n"
      ],
      "metadata": {
        "id": "s1A7pRUt6W3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.plot_param_importances(study_dnn)\n"
      ],
      "metadata": {
        "id": "HjwjfavrhHzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "F4RDUNhIrPWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input, Conv1D, MaxPooling1D, Flatten, Reshape\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import recall_score, classification_report, confusion_matrix, roc_auc_score, accuracy_score, f1_score, make_scorer\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "import traceback # Para imprimir el stack trace de errores\n",
        "import os # Para guardar gr√°ficos y modelos\n",
        "\n",
        "\n",
        "# Convertir X_train_scaled y y_train a arrays de NumPy\n",
        "# Esto es CRUCIAL para evitar KeyErrors con Pandas DataFrames/Series y asegurar compatibilidad\n",
        "X_train_scaled_np = X_train_scaled.values if hasattr(X_train_scaled, 'values') else X_train_scaled\n",
        "y_train_np = y_train.values if hasattr(y_train, 'values') else y_train\n",
        "\n",
        "# Asegurarse de que los datos de prueba tambi√©n est√©n en formato NumPy\n",
        "X_test_scaled_np = X_test_scaled.values if 'X_test_scaled' in locals() and hasattr(X_test_scaled, 'values') else X_test_scaled\n",
        "y_test_np = y_test.values if 'y_test' in locals() and hasattr(y_test, 'values') else y_test\n",
        "\n",
        "\n",
        "# Validaci√≥n de formas b√°sicas\n",
        "if X_train_scaled_np.ndim != 2:\n",
        "    print(f\"ERROR: X_train_scaled debe ser 2D. Forma actual: {X_train_scaled_np.shape}\")\n",
        "    exit()\n",
        "if y_train_np.ndim != 1:\n",
        "    print(f\"ERROR: y_train debe ser 1D. Forma actual: {y_train_np.shape}\")\n",
        "    exit()\n",
        "\n",
        "print(\"Bloque 1: Importaciones y preparaci√≥n de datos completados.\")"
      ],
      "metadata": {
        "id": "12CFqhwzsorf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 1: Funci√≥n para Crear el Modelo CNN ---\n",
        "def create_cnn_model_v2(input_shape_features, layer_config_cnn, dense_units_post_cnn,\n",
        "                        activation_cnn, dropout_rate_dense, optimizer_config):\n",
        "    \"\"\"\n",
        "    Crea y compila un modelo de red neuronal convolucional (CNN) adaptado para datos tabulares.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Input(shape=(input_shape_features,)))\n",
        "    model.add(Reshape((input_shape_features, 1)))\n",
        "\n",
        "    for i, (filters, kernel_size, dropout_conv) in enumerate(layer_config_cnn):\n",
        "        if filters > 0:\n",
        "            model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation_cnn, padding='same'))\n",
        "            if dropout_conv > 0:\n",
        "                model.add(Dropout(dropout_conv))\n",
        "            model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    if dense_units_post_cnn > 0:\n",
        "        model.add(Dense(dense_units_post_cnn, activation=activation_cnn))\n",
        "        if dropout_rate_dense > 0:\n",
        "            model.add(Dropout(dropout_rate_dense))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer_name, learning_rate = optimizer_config\n",
        "    if optimizer_name.lower() == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name.lower() == 'rmsprop':\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "    else:\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # Para el entrenamiento de la CNN, Keras no tiene F1-Score nativo por class_id.\n",
        "    # Monitorearemos 'recall' o 'accuracy' o 'loss' y calcularemos F1 externamente.\n",
        "    # Si quieres monitorear recall_clase_0, ser√≠a:\n",
        "    # recall_clase_0_metric = keras.metrics.Recall(class_id=0, name=\"recall_clase_0\")\n",
        "    # metrics=['accuracy', recall_clase_0_metric]\n",
        "    # En este caso, para optimizar F1-Score en Optuna, lo calculamos manualmente en objective_cnn.\n",
        "    # Las m√©tricas en compile son solo para el monitoreo interno del modelo durante fit().\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy']) # Solo accuracy para simplificar las m√©tricas internas de Keras\n",
        "    return model\n",
        "\n",
        "# --- Callback personalizado para calcular F1-Score en cada √©poca ---\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(self, X_val, y_val, pos_label=0):\n",
        "        super().__init__()\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.pos_label = pos_label\n",
        "        self.train_f1s = []\n",
        "        self.val_f1s = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Calcular F1-Score para el conjunto de validaci√≥n\n",
        "        y_pred_val_proba = self.model.predict(self.X_val, verbose=0)\n",
        "        y_pred_val_classes = (y_pred_val_proba > 0.5).astype(\"int32\")\n",
        "        val_f1 = f1_score(self.y_val, y_pred_val_classes, pos_label=self.pos_label, zero_division=0)\n",
        "        self.val_f1s.append(val_f1)\n",
        "\n",
        "        # NOTE: Calculating training F1-score here is complex and slow if not passed through metrics.\n",
        "        # For simplicity in plotting, we'll append a placeholder or 0.0 for training F1.\n",
        "        self.train_f1s.append(0.0) # Placeholder, as it's hard to get true training F1 efficiently here\n",
        "\n",
        "        # Puedes imprimir para ver el progreso si verbose>0 en fit()\n",
        "        # print(f\" - epoch {epoch+1}: val_f1_clase_0: {val_f1:.4f}\")\n",
        "\n",
        "print(\"Bloque 2: Funci√≥n 'create_cnn_model_v2' y 'F1ScoreCallback' definidas.\")"
      ],
      "metadata": {
        "id": "wItwr29g2z24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 2: Definir la Funci√≥n Objetivo para Optuna (Optimizando para F1-Score Clase 0) ---\n",
        "def objective_cnn(trial):\n",
        "    K.clear_session()\n",
        "\n",
        "    smote_k = trial.suggest_categorical('smote_k', [3, 5, 7, 9])\n",
        "\n",
        "    n_conv_layers = trial.suggest_int('n_conv_layers', 1, 2)\n",
        "    # Lista para construir la configuraci√≥n de las capas convolucionales\n",
        "    layer_config_cnn_list = []\n",
        "    for i in range(n_conv_layers):\n",
        "        filters = trial.suggest_categorical(f'filters_l{i}', [16, 32, 64, 128])\n",
        "        kernel_size = trial.suggest_categorical(f'kernel_size_l{i}', [2, 3])\n",
        "        dropout_conv = trial.suggest_float(f'dropout_conv_l{i}', 0.2, 0.5, step=0.1)\n",
        "        layer_config_cnn_list.append((filters, kernel_size, dropout_conv))\n",
        "\n",
        "    dense_units = trial.suggest_categorical('dense_units', [16, 32, 64])\n",
        "    dropout_rate_dense = trial.suggest_float('dropout_dense', 0.1, 0.3, step=0.1)\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop'])\n",
        "    lr = trial.suggest_float('lr', 1e-4, 5e-3, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
        "\n",
        "    epochs_max = trial.suggest_categorical('epochs_max', [50, 100, 150, 200])\n",
        "    early_stopping_patience = trial.suggest_int('early_stopping_patience', 10, 25)\n",
        "\n",
        "    sm = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "    X_res, y_res = sm.fit_resample(X_train_scaled_np, y_train_np)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores_fold = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_res, y_res)):\n",
        "        X_tr_fold, X_val_fold = X_res[train_idx], X_res[val_idx]\n",
        "        y_tr_fold, y_val_fold = y_res[train_idx], y_res[val_idx]\n",
        "\n",
        "        # --- ¬°CORRECCI√ìN AQU√ç! Pasar los par√°metros directamente, no como un diccionario 'config' ---\n",
        "        model = create_cnn_model_v2(\n",
        "            input_shape_features=X_tr_fold.shape[1],\n",
        "            layer_config_cnn=tuple(layer_config_cnn_list), # Asegurar que sea tupla\n",
        "            dense_units_post_cnn=dense_units,\n",
        "            activation_cnn=activation,\n",
        "            dropout_rate_dense=dropout_rate_dense,\n",
        "            optimizer_config=(optimizer_name, lr)\n",
        "        )\n",
        "\n",
        "        early_stopping_cb_trial = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=early_stopping_patience,\n",
        "            mode='min',\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        model.fit(X_tr_fold, y_tr_fold,\n",
        "                  epochs=epochs_max,\n",
        "                  batch_size=batch_size,\n",
        "                  validation_data=(X_val_fold, y_val_fold),\n",
        "                  callbacks=[early_stopping_cb_trial],\n",
        "                  verbose=0)\n",
        "\n",
        "        y_val_pred_proba = model.predict(X_val_fold, verbose=0)\n",
        "        y_val_pred_classes = (y_val_pred_proba > 0.5).astype(\"int32\")\n",
        "\n",
        "        f1 = f1_score(y_val_fold, y_val_pred_classes, pos_label=0, zero_division=0)\n",
        "        f1_scores_fold.append(f1)\n",
        "\n",
        "        trial.report(f1, fold_idx)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(f1_scores_fold)\n",
        "\n",
        "print(\" Bloque 3: Funci√≥n 'objective_cnn' definida.\")"
      ],
      "metadata": {
        "id": "qVbSsMdWzK75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 3: Crear un Estudio de Optuna y Ejecutar la Optimizaci√≥n ---\n",
        "study_name = \"CNN_F1_Score_Optuna\" # Nombre del estudio enfocado en F1-Score Clase 0\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0, interval_steps=1)\n",
        "\n",
        "study_cnn_optuna = optuna.create_study(direction=\"maximize\", study_name=study_name, sampler=sampler, pruner=pruner)\n",
        "\n",
        "n_trials_optuna = 100 # N√∫mero de trials\n",
        "timeout_seconds = 3600 # 2 horas de timeout (ajusta seg√∫n tus recursos y paciencia)\n",
        "\n",
        "print(f\"üîÅ Iniciando optimizaci√≥n con Optuna para Keras CNN ({n_trials_optuna} trials, timeout: {timeout_seconds/60} min)...\")\n",
        "try:\n",
        "    study_cnn_optuna.optimize(objective_cnn, n_trials=n_trials_optuna, timeout=timeout_seconds)\n",
        "    print(\"Optimizaci√≥n con Optuna para CNN Keras completada.\")\n",
        "\n",
        "    print(f\"\\n Mejores hiperpar√°metros CNN (Optimizados para F1-Score Clase 0): {study_cnn_optuna.best_trial.params}\")\n",
        "    print(f\" Mejor F1-Score clase 0 (promedio CV): {study_cnn_optuna.best_value:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Ocurri√≥ un error INESPERADO durante la optimizaci√≥n con Optuna:\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n Bloque 4: Optimizaci√≥n Optuna completada.\")"
      ],
      "metadata": {
        "id": "sEAH9WjgzNmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 4: Crear y Entrenar el Modelo Final con los Mejores Hiperpar√°metros ---\n",
        "# Este bloque asume que 'study_cnn_optuna' est√° definido y que se encontr√≥ un best_trial.\n",
        "\n",
        "best_params = study_cnn_optuna.best_trial.params\n",
        "\n",
        "# Preparar configuraci√≥n de capas CNN\n",
        "layer_config_cnn_final = []\n",
        "n_conv_layers_final = best_params.get('n_conv_layers', 1)\n",
        "for i in range(n_conv_layers_final):\n",
        "    filters = best_params.get(f'filters_l{i}', 32)\n",
        "    kernel_size = best_params.get(f'kernel_size_l{i}', 2)\n",
        "    dropout_conv = best_params.get(f'dropout_conv_l{i}', 0.2)\n",
        "    layer_config_cnn_final.append((filters, kernel_size, dropout_conv))\n",
        "\n",
        "# Crear la instancia del modelo CNN final\n",
        "final_model = create_cnn_model_v2(\n",
        "    input_shape_features=X_train_scaled_np.shape[1],\n",
        "    layer_config_cnn=tuple(layer_config_cnn_final),\n",
        "    dense_units_post_cnn=best_params['dense_units'],\n",
        "    activation_cnn=best_params['activation'],\n",
        "    dropout_rate_dense=best_params['dropout_dense'],\n",
        "    optimizer_config=(best_params['optimizer'], best_params['lr']) # Usar 'optimizer' y 'lr' de los HPs\n",
        ")\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento completo\n",
        "smote_final_k = best_params.get('smote_k', 5) # Usar 'smote_k' que es el nombre del par√°metro\n",
        "smote_final = SMOTE(k_neighbors=smote_final_k, random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote_final.fit_resample(X_train_scaled_np, y_train_np.astype(int))\n",
        "\n",
        "# Configurar Early Stopping para el entrenamiento final\n",
        "early_stopping_final_cb = EarlyStopping(\n",
        "    monitor='val_loss', # Monitorear val_loss, ya que F1-Score no est√° directamente en metrics\n",
        "    patience=10, #best_params['early_stopping_patience'],\n",
        "    mode='min', # Minimizamos la p√©rdida\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Dividir para la validaci√≥n interna del modelo final (para EarlyStopping y F1 callback)\n",
        "X_train_fit, X_val_fit, y_train_fit, y_val_fit = train_test_split(\n",
        "    X_train_resampled, y_train_resampled, test_size=0.2, random_state=42, stratify=y_train_resampled\n",
        ")\n",
        "\n",
        "# Instanciar el F1ScoreCallback para el entrenamiento final\n",
        "f1_callback_instance = F1ScoreCallback(X_val=X_val_fit, y_val=y_val_fit, pos_label=0)\n",
        "\n",
        "print(f\"\\nEntrenando el modelo final CNN con los mejores hiperpar√°metros (optimizados para F1-Score): {best_params}\")\n",
        "# Entrenar el modelo final y CAPTURAR EL HISTORIAL en la variable 'history'\n",
        "history = final_model.fit(\n",
        "    X_train_fit,\n",
        "    y_train_fit,\n",
        "    epochs=best_params['epochs_max'], # Usar el m√°ximo de epochs\n",
        "    batch_size=best_params['batch_size'],\n",
        "    validation_data=(X_val_fit, y_val_fit), # Usar el conjunto de validaci√≥n expl√≠cito\n",
        "    callbacks=[early_stopping_final_cb, f1_callback_instance], # Ambos callbacks\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Bloque 5: Modelo CNN final entrenado y historial capturado.\")"
      ],
      "metadata": {
        "id": "N-awFOwzBSd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 5: Evaluaci√≥n en el conjunto de prueba ---\n",
        "# Este bloque asume que 'final_model' est√° definido y entrenado del Bloque 5.\n",
        "\n",
        "y_pred_probs = final_model.predict(X_test_scaled_np, verbose=0)\n",
        "y_pred_probs_flat = y_pred_probs.flatten()\n",
        "\n",
        "print(\"\\nüìä Bloque 6: Evaluaci√≥n del Modelo CNN Final con distintos umbrales en el conjunto de prueba:\\n\")\n",
        "for threshold in np.arange(0.50, 0.81, 0.02):\n",
        "    y_pred_thresholded = (y_pred_probs_flat >= threshold).astype(int)\n",
        "\n",
        "    print(f\" Umbral = {threshold:.2f}\")\n",
        "    print(classification_report(y_test_np, y_pred_thresholded, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"], zero_division=0))\n",
        "    print(\" Matriz de Confusi√≥n:\")\n",
        "    print(confusion_matrix(y_test_np, y_pred_thresholded, labels=[0, 1]))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# M√©tricas Globales (independientes del umbral, eval√∫an la capacidad de ranking)\n",
        "print(\"\\n--- M√©tricas Globales del Modelo (independientes del umbral) ---\")\n",
        "auc = roc_auc_score(y_test_np, y_pred_probs_flat)\n",
        "print(f\" ROC AUC: {auc:.4f}\")\n",
        "print(\" Bloque 6: Evaluaci√≥n en el conjunto de prueba completada.\")"
      ],
      "metadata": {
        "id": "U2dt5ER5zPqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 6: Graficar el Historial de Entrenamiento para Diagn√≥stico de Overfitting/Underfitting ---\n",
        "# Este bloque asume que 'history' y 'f1_callback_instance' est√°n definidos del Bloque 5.\n",
        "\n",
        "print(\"\\n Bloque 7: Generando gr√°ficos del historial de entrenamiento para diagnosticar Overfitting/Underfitting...\")\n",
        "\n",
        "# Define la ruta para guardar los gr√°ficos\n",
        "ruta_carpeta_graficos = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano - Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/Modelos/CNN/graficos/'\n",
        "os.makedirs(ruta_carpeta_graficos, exist_ok=True)\n",
        "\n",
        "# Funci√≥n para graficar las curvas de aprendizaje (adaptada para F1-Score)\n",
        "def plot_learning_curves(history_obj, f1_callback_obj, best_params_for_title=None, save_path=None):\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    if best_params_for_title:\n",
        "        title_str = \"Curvas de Aprendizaje CNN (F1-Score Clase 0)\\n\"\n",
        "        title_str += f\"LR: {best_params_for_title.get('lr'):.1e}, Opt: {best_params_for_title.get('optimizer')}, \"\n",
        "        title_str += f\"ConvLayers: {best_params_for_title.get('n_conv_layers')}, SMOTE_k: {best_params_for_title.get('smote_k')}\"\n",
        "        fig.suptitle(title_str, fontsize=18, y=1.03)\n",
        "\n",
        "    # Gr√°fico de P√©rdida (Loss)\n",
        "    ax1.plot(history_obj.history['loss'], label='P√©rdida de Entrenamiento', color='dodgerblue')\n",
        "    if 'val_loss' in history_obj.history:\n",
        "        ax1.plot(history_obj.history['val_loss'], label='P√©rdida de Validaci√≥n', color='darkorange', linestyle='--')\n",
        "    ax1.set_title('Curva de P√©rdida del Modelo', fontsize=16)\n",
        "    ax1.set_xlabel('√âpoca'); ax1.set_ylabel('Loss'); ax1.legend()\n",
        "\n",
        "    # Gr√°fico de F1-Score (desde el callback personalizado)\n",
        "    # Nota: f1_callback_obj.train_f1s en este caso es un placeholder (0.0)\n",
        "    # Si quieres el F1 de entrenamiento real, necesitar√≠as una implementaci√≥n m√°s compleja.\n",
        "    ax2.plot(f1_callback_obj.train_f1s, label='F1-Score Entrenamiento (Clase 0 - Placeholder)', color='lightgray', linestyle=':')\n",
        "    ax2.plot(f1_callback_obj.val_f1s, label='F1-Score Validaci√≥n (Clase 0)', color='darkorange', linestyle='--')\n",
        "    ax2.set_title('Curva de F1-Score del Modelo (Clase 0)', fontsize=16)\n",
        "    ax2.set_xlabel('√âpoca'); ax2.set_ylabel('F1-Score'); ax2.legend()\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]);\n",
        "    plt.show()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close(fig)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Llamada a la funci√≥n de graficaci√≥n\n",
        "plot_learning_curves(history, f1_callback_instance, best_params_for_title=best_params, save_path=os.path.join(ruta_carpeta_graficos, 'cnn_f1_learning_curves.png'))\n",
        "\n",
        "print(\"‚úÖ Bloque 7: Gr√°ficos generados y guardados en Google Drive.\")"
      ],
      "metadata": {
        "id": "4O0x3uDjCqmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 7: Guardar el Modelo Final ---\n",
        "# Este bloque asume que 'final_model' est√° definido y entrenado del Bloque 5.\n",
        "\n",
        "ruta_carpeta_modelo = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano - Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/Modelos/CNN/'\n",
        "os.makedirs(ruta_carpeta_modelo, exist_ok=True)\n",
        "\n",
        "nombre_archivo_modelo_cnn = 'modelo_cnn_f1_optuna_final_0.7.keras' # Nombre del modelo CNN optimizado para F1-Score\n",
        "ruta_modelo_completa_cnn = os.path.join(ruta_carpeta_modelo, nombre_archivo_modelo_cnn)\n",
        "\n",
        "final_model.save(ruta_modelo_completa_cnn)\n",
        "print(f\"Bloque 8: Modelo CNN entrenado y guardado en: {ruta_modelo_completa_cnn}\")"
      ],
      "metadata": {
        "id": "i0203iGwGv_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "OXdtcK3g_Hdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_consolidado2\n",
        "\n"
      ],
      "metadata": {
        "id": "nsB4OzoYG9zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_consolidado.columns"
      ],
      "metadata": {
        "id": "yBZesO0UHmD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_log_cols = [\n",
        "    'max_days_with_access', 'max_days_without_access', 'first_last_log_diff',\n",
        "    'logs', 'week_logs', 'daily_avg', 'weekly_avg', 'days_with_logs',\n",
        "    'days_with_logs_avg', 'days_with_logs_week', 'days_with_logs_avg_week',\n",
        "    'activity_total', 'content_total', 'other_total','report_total',\n",
        "    'system_total', 'activity_week', 'content_week', 'other_week',\n",
        "    'report_week', 'system_week', 'total_weeks', 'course_progress',\n",
        "    'longest_streak', 'proyeccion_nota_final', 'aprobando'\n",
        "]\n",
        "\n",
        "target_col = 'aprobo_semestre_real'\n"
      ],
      "metadata": {
        "id": "LlDMYyV7QWLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar columnas necesarias\n",
        "columnas_lstm = ['Nombre completo del usuario', 'semana_semestre', 'Semestre'] + features_log_cols + [target_col]\n",
        "df_lstm = df_consolidado[columnas_lstm].dropna(subset=[target_col])\n"
      ],
      "metadata": {
        "id": "IMhHnUynRt_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_lstm\n"
      ],
      "metadata": {
        "id": "1eiFzXUGVG46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the counts of unique values in the 'aprobo_semestre_real' column\n",
        "print(df_lstm['aprobo_semestre_real'].value_counts())\n",
        "\n",
        "# Optional: Check the normalized counts (proportions)\n",
        "print(\"\\nNormalized counts:\")\n",
        "print(df_lstm['aprobo_semestre_real'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "NLt_NVReViqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_lstm_grouped = df_lstm.groupby(['Nombre completo del usuario','Semestre','semana_semestre'])[features_log_cols].mean().reset_index()\n"
      ],
      "metadata": {
        "id": "y32UdxSdTM6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input, LSTM, Reshape # Agregada capa LSTM\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import recall_score, classification_report, confusion_matrix, roc_auc_score, accuracy_score, f1_score\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import tensorflow.keras.backend as K\n"
      ],
      "metadata": {
        "id": "T8JeS510do3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import StandardScaler # Importar StandardScaler\n",
        "\n",
        "# --- Definir las columnas clave para la creaci√≥n de secuencias ---\n",
        "ID_COLS = ['Nombre completo del usuario', 'Semestre']\n",
        "TIME_COL = 'semana_semestre'\n",
        "TARGET_COL = 'aprobo_semestre_real'\n",
        "FEATURES_LOG_COLS = [\n",
        "    'max_days_with_access', 'max_days_without_access', 'first_last_log_diff',\n",
        "    'logs', 'week_logs', 'daily_avg', 'weekly_avg', 'days_with_logs',\n",
        "    'days_with_logs_avg', 'days_with_logs_week', 'days_with_logs_avg_week',\n",
        "    'activity_total', 'content_total', 'other_total','report_total',\n",
        "    'system_total', 'activity_week', 'content_week', 'other_week',\n",
        "    'report_week', 'system_week', 'total_weeks', 'course_progress',\n",
        "    'longest_streak', 'proyeccion_nota_final', 'aprobando'\n",
        "]\n",
        "\n",
        "# --- ASEGURAR QUE df_consolidado EST√â CARGADO Y DISPONIBLE ---\n",
        "\n",
        "\n",
        "print(f\"DEBUG: df_consolidado cargado. Filas iniciales: {len(df_consolidado)}\")\n",
        "print(f\"DEBUG: Columnas en df_consolidado: {df_consolidado.columns.tolist()}\")\n",
        "\n",
        "for col in ID_COLS + [TIME_COL] + [TARGET_COL]:\n",
        "    if col not in df_consolidado.columns:\n",
        "        print(f\" ERROR CR√çTICO: La columna '{col}' (ID/Tiempo/Target) NO se encontr√≥ en df_consolidado. ¬°Revisa el nombre!\")\n",
        "        exit()\n",
        "\n",
        "all_cols_to_process_for_numeric = FEATURES_LOG_COLS + [TARGET_COL] + [TIME_COL]\n",
        "\n",
        "for col in all_cols_to_process_for_numeric:\n",
        "    if col in df_consolidado.columns:\n",
        "        original_dtype = df_consolidado[col].dtype\n",
        "        df_consolidado[col] = pd.to_numeric(df_consolidado[col], errors='coerce')\n",
        "        if df_consolidado[col].isnull().any() and not (pd.api.types.is_float_dtype(original_dtype) and df_consolidado[col].isnull().all()):\n",
        "            print(f\"DEBUG: Columna '{col}' ahora tiene {df_consolidado[col].isnull().sum()} NaNs despu√©s de la conversi√≥n a num√©rico. Posibles valores no num√©ricos originales.\")\n",
        "    elif col in FEATURES_LOG_COLS:\n",
        "        print(f\"Advertencia: La columna de caracter√≠stica '{col}' NO se encontr√≥ en df_consolidado. ¬°Ser√° ignorada!\")\n",
        "\n",
        "print(\" Bloque 1: Importaciones y definici√≥n de columnas completadas.\")\n",
        "print(f\"N√∫mero de filas en df_consolidado despu√©s de verificaci√≥n y conversi√≥n num√©rica: {len(df_consolidado)}\")"
      ],
      "metadata": {
        "id": "qIZga007t8gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Ordenar y Manejar NaN's dentro de las Secuencias (¬°CR√çTICO ANTES DEL ESCALADO!) ---\n",
        "print(\"DEBUG: Iniciando ordenamiento y manejo de NaNs...\")\n",
        "df_consolidado_original_len_b2 = len(df_consolidado)\n",
        "\n",
        "df_consolidado = df_consolidado.sort_values(by=ID_COLS + [TIME_COL]).reset_index(drop=True)\n",
        "\n",
        "cols_for_ffill = [\n",
        "    'max_days_with_access', 'max_days_without_access', 'longest_streak',\n",
        "    'promedio_ponderado', 'proyeccion_nota_final', 'aprobando'\n",
        "]\n",
        "\n",
        "for col in FEATURES_LOG_COLS:\n",
        "    if col not in df_consolidado.columns:\n",
        "        continue\n",
        "\n",
        "    if col in cols_for_ffill:\n",
        "        nan_before_ffill = df_consolidado[col].isnull().sum()\n",
        "        df_consolidado[col] = df_consolidado.groupby(ID_COLS)[col].ffill()\n",
        "        nan_after_ffill = df_consolidado[col].isnull().sum()\n",
        "        if nan_before_ffill > nan_after_ffill:\n",
        "            print(f\"DEBUG: ffill aplicado a '{col}'. NaNs restantes: {nan_after_ffill}\")\n",
        "\n",
        "    nan_before_fillna = df_consolidado[col].isnull().sum()\n",
        "    df_consolidado[col] = df_consolidado[col].fillna(0)\n",
        "    if nan_before_fillna > df_consolidado[col].isnull().sum():\n",
        "        print(f\"DEBUG: '{col}' rellenado con 0. NaNs restantes: {df_consolidado[col].isnull().sum()}\")\n",
        "\n",
        "target_nan_count = df_consolidado[TARGET_COL].isnull().sum()\n",
        "df_consolidado[TARGET_COL] = df_consolidado[TARGET_COL].fillna(0).astype(int)\n",
        "if target_nan_count > 0:\n",
        "    print(f\"DEBUG: {target_nan_count} NaNs en columna target '{TARGET_COL}' rellenados con 0 y convertidos a int.\")\n",
        "\n",
        "print(\" Bloque 2: Preprocesamiento de datos temporales y manejo de NaN's completado.\")\n",
        "print(f\"N√∫mero de filas en df_consolidado despu√©s de preprocesamiento: {len(df_consolidado)}\")\n",
        "\n",
        "if df_consolidado.empty:\n",
        "    print(\" ERROR CR√çTICO: df_consolidado est√° VAC√çO despu√©s del preprocesamiento. No se pueden crear secuencias.\")"
      ],
      "metadata": {
        "id": "QlYEiOOjt-6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Aplicaci√≥n de Escalado a las Caracter√≠sticas ---\n",
        "print(\"DEBUG: Iniciando escalado de FEATURES_LOG_COLS...\")\n",
        "\n",
        "# Seleccionar solo las columnas de features_log_cols que realmente existen en df_consolidado\n",
        "actual_features_to_scale = [f for f in FEATURES_LOG_COLS if f in df_consolidado.columns]\n",
        "\n",
        "if not actual_features_to_scale:\n",
        "    print(\" ERROR: No hay caracter√≠sticas v√°lidas para escalar en df_consolidado.\")\n",
        "    # No se puede escalar, y las variables escaladas estar√°n vac√≠as.\n",
        "else:\n",
        "    scaler_lstm = StandardScaler() # Se crea el escalador\n",
        "\n",
        "    # Aplicar fit_transform SOLO a las filas y columnas relevantes para escalado\n",
        "    # Aseg√∫rate de no escalar la columna del target o las IDs\n",
        "    df_consolidado[actual_features_to_scale] = scaler_lstm.fit_transform(df_consolidado[actual_features_to_scale])\n",
        "\n",
        "    print(f\"DEBUG: Caracter√≠sticas escaladas: {actual_features_to_scale}\")\n",
        "    print(\"Bloque 3: Escalado de caracter√≠sticas completado.\")\n",
        "\n",
        "print(f\"N√∫mero de filas en df_consolidado despu√©s de escalado: {len(df_consolidado)}\")"
      ],
      "metadata": {
        "id": "T6_JPtZPuA2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Creaci√≥n de Secuencias y Aplicaci√≥n de Padding ---\n",
        "print(\"DEBUG: Iniciando creaci√≥n de secuencias con agrupaci√≥n por usuario Y semestre...\")\n",
        "X_sequences = []\n",
        "y_labels = []\n",
        "sequence_ids_list = [] # Opcional: para depuraci√≥n o trazabilidad\n",
        "\n",
        "# Puedes definir una longitud m√°xima fija si sabes que tus semestres tienen una duraci√≥n m√°xima\n",
        "# De lo contrario, pad_sequences usar√° la longitud de la secuencia m√°s larga encontrada.\n",
        "MAX_WEEKS_FOR_LSTM = 15 # Ejemplo: Puedes ajustar esto a 15, 20 o el m√°ximo real de semanas por semestre.\n",
        "\n",
        "# Reinicializar variables para el caso de no encontrar secuencias\n",
        "LSTM_MAX_SEQUENCE_LENGTH = MAX_WEEKS_FOR_LSTM # Usamos la longitud fija\n",
        "LSTM_NUM_FEATURES_PER_TIMESTEP = 0\n",
        "X_padded_sequences = np.array([])\n",
        "y_labels_np = np.array([])\n",
        "\n",
        "if df_consolidado.empty:\n",
        "    print(\"ERROR CR√çTICO: df_consolidado est√° VAC√çO. No se pueden crear secuencias.\")\n",
        "else:\n",
        "    # Usar la lista de features que realmente se escalaron y existen\n",
        "    current_features_for_lstm = [f for f in FEATURES_LOG_COLS if f in df_consolidado.columns]\n",
        "    if not current_features_for_lstm:\n",
        "        print(\" ERROR CR√çTICO: Ninguna de las columnas en FEATURES_LOG_COLS se encontr√≥ en df_consolidado. No se pueden crear secuencias.\")\n",
        "    else:\n",
        "        # ¬°¬°¬°CORRECCI√ìN CLAVE: Agrupar por ID_COLS (Nombre completo del usuario, Semestre)!!!\n",
        "        grouped_sequences = df_consolidado.groupby(ID_COLS, dropna=False)\n",
        "\n",
        "        print(f\"DEBUG: N√∫mero de grupos (secuencias √∫nicas por usuario-semestre) identificados: {len(grouped_sequences)}\")\n",
        "\n",
        "        if len(grouped_sequences) == 0:\n",
        "            print(\" ERROR CR√çTICO: No se encontraron grupos v√°lidos al agrupar df_consolidado. Revisa ID_COLS y los datos.\")\n",
        "        else:\n",
        "            for name, group in grouped_sequences:\n",
        "                if group.empty or TIME_COL not in group.columns or not current_features_for_lstm:\n",
        "                    print(f\"DEBUG: Grupo vac√≠o o sin columnas esenciales para la secuencia: {name}. Saltando.\")\n",
        "                    continue\n",
        "\n",
        "                group_sorted = group.sort_values(by=TIME_COL)\n",
        "                features_data = group_sorted[current_features_for_lstm].values\n",
        "\n",
        "                if len(features_data) >= 3:\n",
        "                    final_label = group_sorted[TARGET_COL].iloc[-1]\n",
        "                    X_sequences.append(features_data)\n",
        "                    y_labels.append(final_label)\n",
        "                    sequence_ids_list.append(name)\n",
        "\n",
        "            if not X_sequences:\n",
        "                print(\"ERROR CR√çTICO: X_sequences est√° VAC√çA despu√©s de procesar los grupos y aplicar el filtro de longitud. No se pudo crear ninguna secuencia v√°lida.\")\n",
        "            else:\n",
        "                # --- Padding de las Secuencias (Usando tu MAX_WEEKS_FOR_LSTM fijo) ---\n",
        "                print(f\"DEBUG: Aplicando padding a una longitud fija de {MAX_WEEKS_FOR_LSTM} semanas.\")\n",
        "\n",
        "                X_padded_sequences = pad_sequences(X_sequences,\n",
        "                                                   maxlen=MAX_WEEKS_FOR_LSTM,\n",
        "                                                   dtype='float32',\n",
        "                                                   padding='post',\n",
        "                                                   truncating='post',\n",
        "                                                   value=0.0)\n",
        "\n",
        "                y_labels_np = np.array(y_labels)\n",
        "\n",
        "                # El n√∫mero de caracter√≠sticas por paso de tiempo se toma de la secuencia ya rellenada\n",
        "                LSTM_NUM_FEATURES_PER_TIMESTEP = X_padded_sequences.shape[2]\n",
        "\n",
        "                print(f\"Forma de X_padded_sequences para LSTM: {X_padded_sequences.shape}\")\n",
        "                print(f\"Forma de y_labels (NumPy array) para LSTM: {y_labels_np.shape}\")\n",
        "\n",
        "\n",
        "print(\"Bloque 4: Creaci√≥n de secuencias y aplicaci√≥n de padding completados.\")"
      ],
      "metadata": {
        "id": "4vhbqs1buFoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Divisi√≥n del Dataset para LSTM ---\n",
        "if X_padded_sequences.size == 0 or y_labels_np.size == 0:\n",
        "    print(\" ERROR CR√çTICO: Los datos para LSTM est√°n vac√≠os. No se puede realizar la divisi√≥n.\")\n",
        "    X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = (np.array([]), np.array([]), np.array([]), np.array([]))\n",
        "else:\n",
        "    if len(np.unique(y_labels_np)) < 2:\n",
        "        print(\" ERROR: y_labels_np tiene menos de 2 clases √∫nicas despu√©s del preprocesamiento. No se puede estratificar. Dividiendo sin estratificar.\")\n",
        "        X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(\n",
        "            X_padded_sequences, y_labels_np, test_size=0.2, random_state=42\n",
        "        )\n",
        "    else:\n",
        "        X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(\n",
        "            X_padded_sequences, y_labels_np, test_size=0.2, random_state=42, stratify=y_labels_np\n",
        "        )\n",
        "\n",
        "    print(f\"\\nForma de X_train_lstm: {X_train_lstm.shape}\")\n",
        "    print(f\"Forma de X_test_lstm: {X_test_lstm.shape}\")\n",
        "    print(f\"Forma de y_train_lstm: {y_train_lstm.shape}\")\n",
        "    print(f\"Forma de y_test_lstm: {y_test_lstm.shape}\")\n",
        "\n",
        "print(\"Bloque 5: Divisi√≥n del dataset para LSTM completada.\")"
      ],
      "metadata": {
        "id": "ukYTC5Ok8IO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 1: Funci√≥n para Crear el Modelo LSTM ---\n",
        "def create_lstm_model(input_shape, config):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Input(shape=input_shape))\n",
        "\n",
        "    for i in range(config['n_lstm_layers']):\n",
        "        model.add(LSTM(config[f'lstm_units_l{i}'], return_sequences=(i < config['n_lstm_layers'] - 1)))\n",
        "        if config[f'dropout_lstm_l{i}'] > 0:\n",
        "            model.add(Dropout(config[f'dropout_lstm_l{i}']))\n",
        "\n",
        "    if config['n_dense_layers'] > 0:\n",
        "        for i in range(config['n_dense_layers']):\n",
        "            model.add(Dense(config[f'dense_units_l{i}'], activation=config['activation']))\n",
        "            if config[f'dropout_dense_l{i}'] > 0:\n",
        "                model.add(Dropout(config[f'dropout_dense_l{i}']))\n",
        "    else:\n",
        "        model.add(Dense(32, activation=config['activation']))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer_name, learning_rate = config['optimizer_name'], config['lr']\n",
        "    if optimizer_name.lower() == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name.lower() == 'rmsprop':\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "    else:\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# --- Callback personalizado para calcular F1-Score en cada √©poca ---\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(self, X_val, y_val, pos_label=0):\n",
        "        super().__init__()\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.pos_label = pos_label\n",
        "        self.train_f1s = []\n",
        "        self.val_f1s = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        y_pred_val_proba = self.model.predict(self.X_val, verbose=0)\n",
        "        y_pred_val_classes = (y_pred_val_proba > 0.5).astype(\"int32\")\n",
        "        val_f1 = f1_score(self.y_val, y_pred_val_classes, pos_label=self.pos_label, zero_division=0)\n",
        "        self.val_f1s.append(val_f1)\n",
        "        self.train_f1s.append(0.0)\n",
        "\n",
        "print(\"Bloque 6: Funci√≥n 'create_lstm_model' y 'F1ScoreCallback' definidas.\")"
      ],
      "metadata": {
        "id": "hJy7tBOdum2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 2: Definir la Funci√≥n Objetivo para Optuna (Optimizando para F1-Score Clase 0) ---\n",
        "# Esta funci√≥n ahora recibe X_train_data y y_train_data como argumentos.\n",
        "def objective_lstm(trial, X_train_data, y_train_data):\n",
        "    K.clear_session()\n",
        "\n",
        "    smote_k = trial.suggest_categorical('smote_k', [3, 5, 7, 9])\n",
        "\n",
        "    n_lstm_layers = trial.suggest_int('n_lstm_layers', 1, 2)\n",
        "    lstm_layer_configs = {}\n",
        "    for i in range(n_lstm_layers):\n",
        "        lstm_layer_configs[f'lstm_units_l{i}'] = trial.suggest_categorical(f'lstm_units_l{i}', [32, 64, 96, 128])\n",
        "        lstm_layer_configs[f'dropout_lstm_l{i}'] = trial.suggest_float(f'dropout_lstm_l{i}', 0.2, 0.5, step=0.1)\n",
        "\n",
        "    n_dense_layers = trial.suggest_int('n_dense_layers', 0, 1)\n",
        "    dense_layer_configs = {}\n",
        "    if n_dense_layers > 0:\n",
        "        for i in range(n_dense_layers):\n",
        "            dense_layer_configs[f'dense_units_l{i}'] = trial.suggest_categorical(f'dense_units_l{i}', [32, 64, 128])\n",
        "            dense_layer_configs[f'dropout_dense_l{i}'] = trial.suggest_float(f'dropout_dense_l{i}', 0.2, 0.5, step=0.1)\n",
        "\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
        "    optimizer_name = trial.suggest_categorical('optimizer_name', ['Adam', 'RMSprop'])\n",
        "    lr = trial.suggest_float('lr', 1e-4, 5e-3, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
        "\n",
        "    epochs_max = trial.suggest_categorical('epochs_max', [50, 100, 150])\n",
        "    early_stopping_patience = trial.suggest_int('early_stopping_patience', 10, 30)\n",
        "\n",
        "    sm = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores_fold = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train_data, y_train_data)):\n",
        "        X_tr_fold, X_val_fold = X_train_data[train_idx], X_train_data[val_idx]\n",
        "        y_tr_fold, y_val_fold = y_train_data[train_idx], y_train_data[val_idx]\n",
        "\n",
        "        # Aplicar SMOTE al conjunto de entrenamiento del fold (aplanando temporalmente)\n",
        "        original_shape_X_tr_fold = X_tr_fold.shape\n",
        "        X_tr_fold_flattened = X_tr_fold.reshape(original_shape_X_tr_fold[0], -1)\n",
        "\n",
        "        try:\n",
        "            X_tr_fold_res_flat, y_tr_fold_res = sm.fit_resample(X_tr_fold_flattened, y_tr_fold)\n",
        "        except ValueError as e:\n",
        "            print(f\"Trial {trial.number}, Fold {fold_idx}: SMOTE fall√≥ ({e}). Retornando 0.0.\")\n",
        "            return 0.0\n",
        "\n",
        "        # Volver a dar forma 3D a los datos despu√©s de SMOTE para el modelo LSTM\n",
        "        X_tr_fold_res = X_tr_fold_res_flat.reshape(y_tr_fold_res.shape[0], original_shape_X_tr_fold[1], original_shape_X_tr_fold[2])\n",
        "\n",
        "        model_config = {\n",
        "            'n_lstm_layers': n_lstm_layers,\n",
        "            **lstm_layer_configs,\n",
        "            'n_dense_layers': n_dense_layers,\n",
        "            **dense_layer_configs,\n",
        "            'activation': activation,\n",
        "            'optimizer_name': optimizer_name,\n",
        "            'lr': lr\n",
        "        }\n",
        "\n",
        "        # Crear la instancia del modelo LSTM\n",
        "        # input_shape para LSTM es (max_sequence_length, num_features_per_timestep)\n",
        "        model = create_lstm_model(input_shape=(X_train_data.shape[1], X_train_data.shape[2]), config=model_config)\n",
        "\n",
        "        early_stopping_cb_trial = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=early_stopping_patience,\n",
        "            mode='min',\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        model.fit(X_tr_fold_res, y_tr_fold_res,\n",
        "                  epochs=epochs_max,\n",
        "                  batch_size=batch_size,\n",
        "                  validation_data=(X_val_fold, y_val_fold),\n",
        "                  callbacks=[early_stopping_cb_trial],\n",
        "                  verbose=0)\n",
        "\n",
        "        y_val_pred_proba = model.predict(X_val_fold, verbose=0)\n",
        "        y_val_pred_classes = (y_val_pred_proba > 0.5).astype(\"int32\")\n",
        "\n",
        "        f1 = f1_score(y_val_fold, y_val_pred_classes, pos_label=0, zero_division=0)\n",
        "        f1_scores_fold.append(f1)\n",
        "\n",
        "        trial.report(f1, fold_idx)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(f1_scores_fold)\n",
        "\n",
        "print(\"Bloque 7: Funci√≥n 'objective_lstm' definida.\")"
      ],
      "metadata": {
        "id": "IRjdwD0lvJzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 3: Crear un Estudio de Optuna y Ejecutar la Optimizaci√≥n ---\n",
        "study_name = \"LSTM_F1_Score_Optuna\"\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0, interval_steps=1)\n",
        "\n",
        "study_lstm_optuna = optuna.create_study(direction=\"maximize\", study_name=study_name, sampler=sampler, pruner=pruner)\n",
        "\n",
        "n_trials_optuna = 60\n",
        "timeout_seconds = 7200\n",
        "\n",
        "print(f\"üîÅ Iniciando optimizaci√≥n con Optuna para Keras LSTM ({n_trials_optuna} trials, timeout: {timeout_seconds/60} min)...\")\n",
        "try:\n",
        "    # ¬°¬°¬°CORRECCI√ìN AQU√ç!!! Se pasan X_train_lstm y y_train_lstm como argumentos\n",
        "    # Esto asume que X_train_lstm y y_train_lstm se definieron en el Bloque 5.\n",
        "    study_lstm_optuna.optimize(lambda trial: objective_lstm(trial, X_train_lstm, y_train_lstm), n_trials=n_trials_optuna, timeout=timeout_seconds)\n",
        "    print(\"‚úÖ Optimizaci√≥n con Optuna para LSTM Keras completada.\")\n",
        "\n",
        "    print(f\"\\nMejores hiperpar√°metros LSTM (Optimizados para F1-Score Clase 0): {study_lstm_optuna.best_trial.params}\")\n",
        "    print(f\" Mejor F1-Score clase 0 (promedio CV): {study_lstm_optuna.best_value:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Ocurri√≥ un error INESPERADO durante la optimizaci√≥n con Optuna:\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n Bloque 8: Optimizaci√≥n Optuna completada.\")"
      ],
      "metadata": {
        "id": "Y3WsHbBxvM6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 4: Crear y Entrenar el Modelo Final con los Mejores Hiperpar√°metros ---\n",
        "# Este bloque asume que 'study_lstm_optuna' est√° definido y que se encontr√≥ un best_trial.\n",
        "\n",
        "best_params = study_lstm_optuna.best_trial.params\n",
        "\n",
        "# Reconstruir la configuraci√≥n de las capas LSTM\n",
        "lstm_layer_configs_final = []\n",
        "n_lstm_layers_final = best_params.get('n_lstm_layers', 1)\n",
        "for i in range(n_lstm_layers_final):\n",
        "    lstm_layer_configs_final.append({\n",
        "        'units': best_params[f'lstm_units_l{i}'],\n",
        "        'dropout': best_params[f'dropout_lstm_l{i}']\n",
        "    })\n",
        "\n",
        "# Reconstruir la configuraci√≥n de las capas Densas\n",
        "dense_layer_configs_final = []\n",
        "n_dense_layers_final = best_params.get('n_dense_layers', 0)\n",
        "if n_dense_layers_final > 0:\n",
        "    for i in range(n_dense_layers_final):\n",
        "        dense_layer_configs_final.append({\n",
        "            'units': best_params[f'dense_units_l{i}'],\n",
        "            'dropout': best_params[f'dropout_dense_l{i}']\n",
        "        })\n",
        "\n",
        "# Crear la instancia del modelo LSTM final\n",
        "final_model = Sequential()\n",
        "# Input shape para LSTM es (max_sequence_length, num_features_per_timestep)\n",
        "# Lo obtenemos de X_train_lstm (que es de la divisi√≥n del Bloque 5).\n",
        "final_model.add(Masking(mask_value=0.0, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]))) # Capa Masking para ceros de padding\n",
        "\n",
        "for i, layer_config in enumerate(lstm_layer_configs_final):\n",
        "    final_model.add(LSTM(layer_config['units'], return_sequences=(i < n_lstm_layers_final - 1)))\n",
        "    if layer_config['dropout'] > 0:\n",
        "        final_model.add(Dropout(layer_config['dropout']))\n",
        "\n",
        "if n_dense_layers_final > 0:\n",
        "    for i, layer_config in enumerate(dense_layer_configs_final):\n",
        "        final_model.add(Dense(layer_config['units'], activation=best_params['activation']))\n",
        "        if layer_config['dropout'] > 0:\n",
        "            final_model.add(Dropout(layer_config['dropout']))\n",
        "else:\n",
        "    final_model.add(Dense(32, activation=best_params['activation']))\n",
        "    final_model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "final_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer_cls_final = getattr(keras.optimizers, best_params['optimizer_name'])\n",
        "optimizer_final = optimizer_cls_final(learning_rate=best_params['lr'])\n",
        "\n",
        "final_model.compile(loss='binary_crossentropy',\n",
        "                    optimizer=optimizer_final,\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento completo (X_train_lstm)\n",
        "smote_final_k = best_params.get('smote_k', 5)\n",
        "smote_final = SMOTE(k_neighbors=smote_final_k, random_state=42)\n",
        "\n",
        "# SMOTE necesita un array 2D. Aplanamos X_train_lstm.\n",
        "original_shape_X_train_lstm = X_train_lstm.shape\n",
        "X_train_lstm_flattened = X_train_lstm.reshape(original_shape_X_train_lstm[0], -1)\n",
        "\n",
        "X_train_resampled_flat, y_train_resampled = smote_final.fit_resample(X_train_lstm_flattened, y_train_lstm.astype(int))\n",
        "\n",
        "# Volver a dar forma 3D a los datos remuestreados para el entrenamiento del modelo LSTM\n",
        "X_train_resampled = X_train_resampled_flat.reshape(y_train_resampled.shape[0], original_shape_X_train_lstm[1], original_shape_X_train_lstm[2])\n",
        "\n",
        "\n",
        "# Configurar Early Stopping para el entrenamiento final\n",
        "early_stopping_final_cb = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=best_params['early_stopping_patience'],\n",
        "    mode='min',\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Dividir para la validaci√≥n interna del modelo final (para EarlyStopping y F1 callback)\n",
        "X_train_fit, X_val_fit, y_train_fit, y_val_fit = train_test_split(\n",
        "    X_train_resampled, y_train_resampled, test_size=0.2, random_state=42, stratify=y_train_resampled\n",
        ")\n",
        "\n",
        "# Instanciar el F1ScoreCallback para el entrenamiento final\n",
        "f1_callback_instance = F1ScoreCallback(X_val=X_val_fit, y_val=y_val_fit, pos_label=0)\n",
        "\n",
        "print(f\"\\n Entrenando el modelo final LSTM con los mejores hiperpar√°metros (optimizados para F1-Score): {best_params}\")\n",
        "# Entrenar el modelo final y CAPTURAR EL HISTORIAL en la variable 'history'\n",
        "history = final_model.fit(\n",
        "    X_train_fit,\n",
        "    y_train_fit,\n",
        "    epochs=best_params['epochs_max'],\n",
        "    batch_size=best_params['batch_size'],\n",
        "    validation_data=(X_val_fit, y_val_fit),\n",
        "    callbacks=[early_stopping_final_cb, f1_callback_instance],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Bloque 9: Modelo LSTM final entrenado y historial capturado.\")"
      ],
      "metadata": {
        "id": "nKAcw2lu8T0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 5: Evaluaci√≥n en el conjunto de prueba ---\n",
        "# Este bloque asume que 'final_model' est√° definido y entrenado del Bloque 9.\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
        "# X_test_lstm y y_test_lstm son los conjuntos de prueba definidos en el Bloque 5.\n",
        "y_pred_probs = final_model.predict(X_test_lstm, verbose=0)\n",
        "y_pred_probs_flat = y_pred_probs.flatten()\n",
        "\n",
        "print(\"\\n Bloque 10: Evaluaci√≥n del Modelo LSTM Final con distintos umbrales en el conjunto de prueba:\\n\")\n",
        "for threshold in np.arange(0.50, 0.81, 0.02):\n",
        "    y_pred_thresholded = (y_pred_probs_flat >= threshold).astype(int)\n",
        "\n",
        "    print(f\" Umbral = {threshold:.2f}\")\n",
        "    print(classification_report(y_test_lstm, y_pred_thresholded, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"], zero_division=0))\n",
        "    print(\" Matriz de Confusi√≥n:\")\n",
        "    print(confusion_matrix(y_test_lstm, y_pred_thresholded, labels=[0, 1]))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# M√©tricas Globales (independientes del umbral, eval√∫an la capacidad de ranking)\n",
        "print(\"\\n--- M√©tricas Globales del Modelo (independientes del umbral) ---\")\n",
        "auc = roc_auc_score(y_test_lstm, y_pred_probs_flat)\n",
        "print(f\" ROC AUC: {auc:.4f}\")\n",
        "print(\"Bloque 10: Evaluaci√≥n en el conjunto de prueba completada.\")"
      ],
      "metadata": {
        "id": "NqqBNKY-J0ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 6: Graficar el Historial de Entrenamiento para Diagn√≥stico de Overfitting/Underfitting ---\n",
        "# Este bloque asume que 'history' y 'f1_callback_instance' est√°n definidos del Bloque 9.\n",
        "import os\n",
        "print(\"\\nüìà Bloque 11: Generando gr√°ficos del historial de entrenamiento para diagnosticar Overfitting/Underfitting...\")\n",
        "\n",
        "# Define la ruta para guardar los gr√°ficos\n",
        "ruta_carpeta_graficos = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano - Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/Modelos/LSTM/graficos/'\n",
        "os.makedirs(ruta_carpeta_graficos, exist_ok=True)\n",
        "\n",
        "# Funci√≥n para graficar las curvas de aprendizaje (adaptada para F1-Score)\n",
        "def plot_learning_curves(history_obj, f1_callback_obj, best_params_for_title=None, save_path=None):\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    if best_params_for_title:\n",
        "        title_str = \"Curvas de Aprendizaje LSTM (F1-Score Clase 0)\\n\"\n",
        "        title_str += f\"LR: {best_params_for_title.get('lr'):.1e}, Opt: {best_params_for_title.get('optimizer_name')}, \"\n",
        "        title_str += f\"LSTM_Layers: {best_params_for_title.get('n_lstm_layers')}, SMOTE_k: {best_params_for_title.get('smote_k')}\"\n",
        "        fig.suptitle(title_str, fontsize=18, y=1.03)\n",
        "\n",
        "    # Gr√°fico de P√©rdida (Loss)\n",
        "    ax1.plot(history_obj.history['loss'], label='P√©rdida de Entrenamiento', color='dodgerblue')\n",
        "    if 'val_loss' in history_obj.history:\n",
        "        ax1.plot(history_obj.history['val_loss'], label='P√©rdida de Validaci√≥n', color='darkorange', linestyle='--')\n",
        "    ax1.set_title('Curva de P√©rdida del Modelo', fontsize=16)\n",
        "    ax1.set_xlabel('√âpoca'); ax1.set_ylabel('Loss'); ax1.legend()\n",
        "\n",
        "    # Gr√°fico de F1-Score (desde el callback personalizado)\n",
        "    ax2.plot(f1_callback_obj.train_f1s, label='F1-Score Entrenamiento (Clase 0 - Placeholder)', color='lightgray', linestyle=':')\n",
        "    ax2.plot(f1_callback_obj.val_f1s, label='F1-Score Validaci√≥n (Clase 0)', color='darkorange', linestyle='--')\n",
        "    ax2.set_title('Curva de F1-Score del Modelo (Clase 0)', fontsize=16)\n",
        "    ax2.set_xlabel('√âpoca'); ax2.set_ylabel('F1-Score'); ax2.legend()\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]);\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close(fig)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Llamada a la funci√≥n de graficaci√≥n\n",
        "plot_learning_curves(history, f1_callback_instance, best_params_for_title=best_params, save_path=os.path.join(ruta_carpeta_graficos, 'lstm_f1_learning_curves.png'))\n",
        "\n",
        "print(\"‚úÖ Bloque 11: Gr√°ficos generados y guardados en Google Drive.\")"
      ],
      "metadata": {
        "id": "466MnEgmL4zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 7: Guardar el Modelo Final ---\n",
        "# Este bloque asume que 'final_model' est√° definido y entrenado del Bloque 9.\n",
        "\n",
        "ruta_carpeta_modelo = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano - Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/Modelos/LSTM/'\n",
        "os.makedirs(ruta_carpeta_modelo, exist_ok=True)\n",
        "\n",
        "nombre_archivo_modelo_lstm = 'modelo_lstm_f1_optuna_final.keras' # Nombre del modelo LSTM\n",
        "ruta_modelo_completa_lstm = os.path.join(ruta_carpeta_modelo, nombre_archivo_modelo_lstm)\n",
        "\n",
        "final_model.save(ruta_modelo_completa_lstm)\n",
        "print(f\"Bloque 12: Modelo LSTM entrenado y guardado en: {ruta_modelo_completa_lstm}\")"
      ],
      "metadata": {
        "id": "9-NcYpasMbUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hibrido CNN - LSTM"
      ],
      "metadata": {
        "id": "JJXJG6ynZQjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define las columnas clave (aseg√∫rate que existan en df_consolidado)\n",
        "ID_COLS = ['Nombre completo del usuario', 'Semestre']\n",
        "TIME_COL = 'semana_semestre'\n",
        "TARGET_COL = 'aprobo_semestre_real'\n",
        "FEATURES_LOG_COLS = [\n",
        "    'max_days_with_access', 'max_days_without_access', 'first_last_log_diff',\n",
        "    'logs', 'week_logs', 'daily_avg', 'weekly_avg', 'days_with_logs',\n",
        "    'days_with_logs_avg', 'days_with_logs_week', 'days_with_logs_avg_week', #\n",
        "    'activity_total', 'content_total', 'other_total', 'report_total',\n",
        "    'system_total', 'activity_week', 'content_week', 'other_week',\n",
        "    'report_week', 'system_week', 'total_weeks', 'course_progress',\n",
        "    'longest_streak', 'proyeccion_nota_final', 'aprobando'\n",
        "]\n",
        "\n",
        "# Convertir df_consolidado a NumPy arrays y manejar NaNs (replicando la l√≥gica de preparaci√≥n de LSTM)\n",
        "# Esto es para asegurar que los datos X_train_lstm, y_train_lstm est√©n bien preparados.\n",
        "\n",
        "# Validar que las columnas CR√çTICAS existan\n",
        "for col in ID_COLS + [TIME_COL] + [TARGET_COL]:\n",
        "    if col not in df_consolidado.columns:\n",
        "        print(f\" ERROR CR√çTICO: La columna '{col}' (ID/Tiempo/Target) NO se encontr√≥ en df_consolidado. ¬°Revisa el nombre!\")\n",
        "        exit()\n",
        "\n",
        "# Convertir a num√©rico (manejar posibles cadenas vac√≠as o no num√©ricas)\n",
        "all_cols_to_process_for_numeric = FEATURES_LOG_COLS + [TARGET_COL] + [TIME_COL]\n",
        "for col in all_cols_to_process_for_numeric:\n",
        "    if col in df_consolidado.columns:\n",
        "        df_consolidado[col] = pd.to_numeric(df_consolidado[col], errors='coerce')\n",
        "    elif col in FEATURES_LOG_COLS:\n",
        "        print(f\"‚ö†Ô∏è Advertencia: La columna de caracter√≠stica '{col}' NO se encontr√≥ en df_consolidado. ¬°Ser√° ignorada!\")\n",
        "\n",
        "# Ordenar y Manejar NaN's (ffill y fillna(0))\n",
        "df_consolidado = df_consolidado.sort_values(by=ID_COLS + [TIME_COL]).reset_index(drop=True)\n",
        "cols_for_ffill = ['max_days_with_access', 'max_days_without_access', 'longest_streak', 'promedio_ponderado', 'proyeccion_nota_final', 'aprobando']\n",
        "for col in FEATURES_LOG_COLS:\n",
        "    if col not in df_consolidado.columns: continue\n",
        "    if col in cols_for_ffill: df_consolidado[col] = df_consolidado.groupby(ID_COLS)[col].ffill()\n",
        "    df_consolidado[col] = df_consolidado[col].fillna(0)\n",
        "df_consolidado[TARGET_COL] = df_consolidado[TARGET_COL].fillna(0).astype(int)\n",
        "\n",
        "# Aplicar Escalado (StandardScaler)\n",
        "actual_features_to_scale = [f for f in FEATURES_LOG_COLS if f in df_consolidado.columns]\n",
        "if not actual_features_to_scale:\n",
        "    print(\" ERROR: No hay caracter√≠sticas v√°lidas para escalar en df_consolidado.\")\n",
        "    exit()\n",
        "scaler_lstm_cnn = StandardScaler()\n",
        "df_consolidado[actual_features_to_scale] = scaler_lstm_cnn.fit_transform(df_consolidado[actual_features_to_scale])\n",
        "\n",
        "# Crear Secuencias y Padding (usando tu MAX_WEEKS_FOR_LSTM fijo)\n",
        "X_sequences = [] ; y_labels = [] ; sequence_ids_list = []\n",
        "MAX_WEEKS_FOR_LSTM = 30 # Longitud fija\n",
        "current_features_for_lstm_cnn = [f for f in FEATURES_LOG_COLS if f in df_consolidado.columns]\n",
        "\n",
        "if df_consolidado.empty: print(\" ERROR: df_consolidado est√° VAC√çO.\") ; exit()\n",
        "if not current_features_for_lstm_cnn: print(\" ERROR: No hay features para LSTM-CNN.\") ; exit()\n",
        "\n",
        "grouped_sequences = df_consolidado.groupby(ID_COLS, dropna=False)\n",
        "for name, group in grouped_sequences:\n",
        "    if group.empty or TIME_COL not in group.columns or not current_features_for_lstm_cnn: continue\n",
        "    group_sorted = group.sort_values(by=TIME_COL)\n",
        "    features_data = group_sorted[current_features_for_lstm_cnn].values\n",
        "    if len(features_data) >= 3: # Filtrar por longitud m√≠nima\n",
        "        X_sequences.append(features_data)\n",
        "        y_labels.append(group_sorted[TARGET_COL].iloc[-1])\n",
        "\n",
        "if not X_sequences: print(\" ERROR: X_sequences est√° VAC√çA.\") ; exit()\n",
        "\n",
        "X_padded_sequences_lstm_cnn = pad_sequences(X_sequences, maxlen=MAX_WEEKS_FOR_LSTM, dtype='float32', padding='post', truncating='post', value=0.0)\n",
        "y_labels_np_lstm_cnn = np.array(y_labels)\n",
        "\n",
        "# Division del Dataset para LSTM-CNN\n",
        "if X_padded_sequences_lstm_cnn.size == 0 or y_labels_np_lstm_cnn.size == 0:\n",
        "    print(\" ERROR: Los datos para LSTM-CNN est√°n vac√≠os.\") ; exit()\n",
        "\n",
        "if len(np.unique(y_labels_np_lstm_cnn)) < 2:\n",
        "    print(\" ERROR: y_labels_np_lstm_cnn tiene menos de 2 clases. No se puede estratificar.\")\n",
        "    X_train_lstm_cnn, X_test_lstm_cnn, y_train_lstm_cnn, y_test_lstm_cnn = train_test_split(X_padded_sequences_lstm_cnn, y_labels_np_lstm_cnn, test_size=0.2, random_state=42)\n",
        "else:\n",
        "    X_train_lstm_cnn, X_test_lstm_cnn, y_train_lstm_cnn, y_test_lstm_cnn = train_test_split(X_padded_sequences_lstm_cnn, y_labels_np_lstm_cnn, test_size=0.2, random_state=42, stratify=y_labels_np_lstm_cnn)\n",
        "\n",
        "print(\" Bloque 1: Importaciones y preparaci√≥n de datos para LSTM-CNN completados.\")\n",
        "print(f\"Forma de X_train_lstm_cnn: {X_train_lstm_cnn.shape}\")\n",
        "print(f\"Forma de y_train_lstm_cnn: {y_train_lstm_cnn.shape}\")"
      ],
      "metadata": {
        "id": "B4i9uRMCZRkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "# ¬°¬°¬°CORRECCI√ìN AQUI!!! Importar layers expl√≠citamente para usar layers.Conv1D, etc.\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Dropout, Input, LSTM, Conv1D, MaxPooling1D, Flatten, Masking # Todas estas son parte de layers\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "\n",
        "from sklearn.metrics import f1_score # f1_score ya importado\n",
        "\n",
        "# --- PASO 1: Funci√≥n para Crear el Modelo LSTM-CNN ---\n",
        "def create_lstm_cnn_model(input_shape, config):\n",
        "    \"\"\"\n",
        "    Crea y compila un modelo de red neuronal h√≠brida CNN-LSTM.\n",
        "    input_shape debe ser (max_sequence_length, num_features_per_timestep).\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Capa de entrada que recibe las secuencias, incluyendo Masking para los ceros de padding\n",
        "    model.add(layers.Masking(mask_value=0.0, input_shape=input_shape)) # Usar layers.Masking\n",
        "\n",
        "    # --- Parte CNN (extraer patrones locales a lo largo del tiempo) ---\n",
        "    for i in range(config['n_conv_layers']):\n",
        "        model.add(layers.Conv1D(filters=config[f'filters_l{i}'], # Usar layers.Conv1D\n",
        "                                kernel_size=config[f'kernel_size_l{i}'],\n",
        "                                activation=config['activation'],\n",
        "                                padding='same'))\n",
        "        if config[f'dropout_conv_l{i}'] > 0:\n",
        "            model.add(layers.Dropout(config[f'dropout_conv_l{i}'])) # Usar layers.Dropout\n",
        "        model.add(layers.MaxPooling1D(pool_size=2)) # Usar layers.MaxPooling1D\n",
        "\n",
        "    # --- Parte LSTM (aprender dependencias de las secuencias reducidas por CNN) ---\n",
        "    for i in range(config['n_lstm_layers']):\n",
        "        model.add(layers.LSTM(config[f'lstm_units_l{i}'], return_sequences=(i < config['n_lstm_layers'] - 1))) # Usar layers.LSTM\n",
        "        if config[f'dropout_lstm_l{i}'] > 0:\n",
        "            model.add(layers.Dropout(config[f'dropout_lstm_l{i}']))\n",
        "\n",
        "    # --- Capas densas post-recurrente ---\n",
        "    if config['n_dense_layers'] > 0:\n",
        "        for i in range(config['n_dense_layers']):\n",
        "            model.add(layers.Dense(config[f'dense_units_l{i}'], activation=config['activation'])) # Usar layers.Dense\n",
        "            if config[f'dropout_dense_l{i}'] > 0:\n",
        "                model.add(layers.Dropout(config[f'dropout_dense_l{i}']))\n",
        "    else:\n",
        "        model.add(layers.Dense(32, activation=config['activation'])) # Usar layers.Dense\n",
        "        model.add(layers.Dropout(0.2)) # Usar layers.Dropout\n",
        "\n",
        "    # Capa de salida para clasificaci√≥n binaria\n",
        "    model.add(layers.Dense(1, activation='sigmoid')) # Usar layers.Dense\n",
        "\n",
        "    # Configurar el optimizador\n",
        "    optimizer_cls = getattr(tf.keras.optimizers, config['optimizer_name']) # tf.keras.optimizers es correcto\n",
        "    optimizer = optimizer_cls(learning_rate=config['lr'])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# --- Callback personalizado para calcular F1-Score en cada √©poca ---\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(self, X_val, y_val, pos_label=0):\n",
        "        super().__init__()\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.pos_label = pos_label\n",
        "        self.train_f1s = []\n",
        "        self.val_f1s = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        y_pred_val_proba = self.model.predict(self.X_val, verbose=0)\n",
        "        y_pred_val_classes = (y_pred_val_proba > 0.5).astype(\"int32\")\n",
        "        val_f1 = f1_score(self.y_val, y_pred_val_classes, pos_label=self.pos_label, zero_division=0)\n",
        "        self.val_f1s.append(val_f1)\n",
        "        self.train_f1s.append(0.0) # Placeholder\n",
        "\n",
        "print(\" Bloque 2: Funci√≥n 'create_lstm_cnn_model' y 'F1ScoreCallback' definidas.\")"
      ],
      "metadata": {
        "id": "-PexigSlaUHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K # Para K.clear_session()\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import optuna\n",
        "import traceback\n",
        "\n",
        "# --- PASO 2: Definir la Funci√≥n Objetivo para Optuna (Optimizando para Clase 0) ---\n",
        "def objective_lstm_cnn(trial, X_train_data, y_train_data):\n",
        "    K.clear_session()\n",
        "\n",
        "    # --- Hiperpar√°metros para SMOTE ---\n",
        "    smote_k = trial.suggest_categorical('smote_k', [3, 5, 7, 9])\n",
        "\n",
        "    # --- Hiperpar√°metros para Capas CONVOLUCIONALES ---\n",
        "    n_conv_layers = trial.suggest_int('n_conv_layers', 1, 2)\n",
        "    conv_layer_configs = {}\n",
        "    for i in range(n_conv_layers):\n",
        "        conv_layer_configs[f'filters_l{i}'] = trial.suggest_categorical(f'filters_l{i}', [16, 32, 64, 128])\n",
        "        conv_layer_configs[f'kernel_size_l{i}'] = trial.suggest_categorical(f'kernel_size_l{i}', [2, 3])\n",
        "        conv_layer_configs[f'dropout_conv_l{i}'] = trial.suggest_float(f'dropout_conv_l{i}', 0.2, 0.5, step=0.1)\n",
        "\n",
        "    # --- Hiperpar√°metros para Capas LSTM ---\n",
        "    n_lstm_layers = trial.suggest_int('n_lstm_layers', 1, 2)\n",
        "    lstm_layer_configs = {}\n",
        "    for i in range(n_lstm_layers):\n",
        "        lstm_layer_configs[f'lstm_units_l{i}'] = trial.suggest_categorical(f'lstm_units_l{i}', [32, 64, 96, 128])\n",
        "        lstm_layer_configs[f'dropout_lstm_l{i}'] = trial.suggest_float(f'dropout_lstm_l{i}', 0.2, 0.5, step=0.1)\n",
        "\n",
        "    # --- Hiperpar√°metros para Capas DENSAS post-recurrente ---\n",
        "    n_dense_layers = trial.suggest_int('n_dense_layers', 0, 1) # 0 o 1 capa densa oculta\n",
        "    dense_layer_configs = {}\n",
        "    if n_dense_layers > 0:\n",
        "        for i in range(n_dense_layers):\n",
        "            dense_layer_configs[f'dense_units_l{i}'] = trial.suggest_categorical(f'dense_units_l{i}', [32, 64, 128])\n",
        "            dense_layer_configs[f'dropout_dense_l{i}'] = trial.suggest_float(f'dropout_dense_l{i}', 0.2, 0.5, step=0.1)\n",
        "\n",
        "    # --- Hiperpar√°metros Generales ---\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
        "    optimizer_name = trial.suggest_categorical('optimizer_name', ['Adam', 'RMSprop'])\n",
        "    lr = trial.suggest_float('lr', 1e-4, 5e-3, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
        "\n",
        "    epochs_max = trial.suggest_categorical('epochs_max', [50, 100, 150])\n",
        "    early_stopping_patience = trial.suggest_int('early_stopping_patience', 10, 30)\n",
        "\n",
        "    # --- Balanceo con SMOTE ---\n",
        "    sm = SMOTE(k_neighbors=smote_k, random_state=42)\n",
        "\n",
        "    # --- Validaci√≥n Cruzada Estratificada (5 splits) ---\n",
        "    # Usar X_train_data y y_train_data que se pasaron como argumentos\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores_fold = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train_data, y_train_data)):\n",
        "        X_tr_fold, X_val_fold = X_train_data[train_idx], X_train_data[val_idx]\n",
        "        y_tr_fold, y_val_fold = y_train_data[train_idx], y_train_data[val_idx]\n",
        "\n",
        "        # Aplicar SMOTE al conjunto de entrenamiento del fold (aplanando temporalmente)\n",
        "        original_shape_X_tr_fold = X_tr_fold.shape\n",
        "        X_tr_fold_flattened = X_tr_fold.reshape(original_shape_X_tr_fold[0], -1)\n",
        "\n",
        "        try:\n",
        "            X_tr_fold_res_flat, y_tr_fold_res = sm.fit_resample(X_tr_fold_flattened, y_tr_fold)\n",
        "        except ValueError as e:\n",
        "            print(f\"Trial {trial.number}, Fold {fold_idx}: SMOTE fall√≥ ({e}). Retornando 0.0.\")\n",
        "            return 0.0\n",
        "\n",
        "        # Volver a dar forma 3D a los datos despu√©s de SMOTE para el modelo LSTM-CNN\n",
        "        X_tr_fold_res = X_tr_fold_res_flat.reshape(y_tr_fold_res.shape[0], original_shape_X_tr_fold[1], original_shape_X_tr_fold[2])\n",
        "\n",
        "        # --- Configuraci√≥n del Modelo para este Trial ---\n",
        "        model_config = {\n",
        "            'n_conv_layers': n_conv_layers,\n",
        "            **conv_layer_configs, # Desempaquetar las configs de CNN\n",
        "            'n_lstm_layers': n_lstm_layers,\n",
        "            **lstm_layer_configs, # Desempaquetar las configs de LSTM\n",
        "            'n_dense_layers': n_dense_layers,\n",
        "            **dense_layer_configs, # Desempaquetar las configs de Dense\n",
        "            'activation': activation,\n",
        "            'optimizer_name': optimizer_name,\n",
        "            'lr': lr\n",
        "        }\n",
        "\n",
        "        # Crear la instancia del modelo LSTM-CNN\n",
        "        # input_shape es (max_sequence_length, num_features_per_timestep)\n",
        "        model = create_lstm_cnn_model(input_shape=(X_train_data.shape[1], X_train_data.shape[2]), config=model_config) !\n",
        "\n",
        "        # Callbacks de EarlyStopping\n",
        "        early_stopping_cb_trial = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=early_stopping_patience,\n",
        "            mode='min',\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        model.fit(X_tr_fold_res, y_tr_fold_res,\n",
        "                  epochs=epochs_max,\n",
        "                  batch_size=batch_size,\n",
        "                  validation_data=(X_val_fold, y_val_fold),\n",
        "                  callbacks=[early_stopping_cb_trial],\n",
        "                  verbose=0)\n",
        "\n",
        "        y_val_pred_proba = model.predict(X_val_fold, verbose=0)\n",
        "        y_val_pred_classes = (y_val_pred_proba > 0.5).astype(\"int32\")\n",
        "\n",
        "        f1 = f1_score(y_val_fold, y_val_pred_classes, pos_label=0, zero_division=0)\n",
        "        f1_scores_fold.append(f1)\n",
        "\n",
        "        trial.report(f1, fold_idx)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(f1_scores_fold)\n",
        "\n",
        "print(\"Bloque 3: Funci√≥n 'objective_lstm_cnn' definida.\")"
      ],
      "metadata": {
        "id": "GldjaxedZy6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna # Necesario si este bloque se ejecuta solo\n",
        "\n",
        "# --- PASO 3: Crear un Estudio de Optuna y Ejecutar la Optimizaci√≥n ---\n",
        "study_name = \"LSTM_CNN_F1_Score_Optuna\" # Nuevo nombre para el estudio\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0, interval_steps=1)\n",
        "\n",
        "study_lstm_cnn_optuna = optuna.create_study(direction=\"maximize\", study_name=study_name, sampler=sampler, pruner=pruner)\n",
        "\n",
        "n_trials_optuna = 60 # N√∫mero de trials\n",
        "timeout_seconds = 7200 # 2 horas de timeout (ajusta seg√∫n tus recursos y paciencia)\n",
        "\n",
        "print(f\" Iniciando optimizaci√≥n con Optuna para Keras LSTM-CNN ({n_trials_optuna} trials, timeout: {timeout_seconds/60} min)...\")\n",
        "try:\n",
        "    # Se pasan X_train_lstm_cnn y y_train_lstm_cnn como argumentos a la funci√≥n objective\n",
        "    study_lstm_cnn_optuna.optimize(lambda trial: objective_lstm_cnn(trial, X_train_lstm_cnn, y_train_lstm_cnn), n_trials=n_trials_optuna, timeout=timeout_seconds)\n",
        "    print(\" Optimizaci√≥n con Optuna para LSTM-CNN Keras completada.\")\n",
        "\n",
        "    print(f\"\\n Mejores hiperpar√°metros LSTM-CNN (Optimizados para F1-Score Clase 0): {study_lstm_cnn_optuna.best_trial.params}\")\n",
        "    print(f\" Mejor F1-Score clase 0 (promedio CV): {study_lstm_cnn_optuna.best_value:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Ocurri√≥ un error INESPERADO durante la optimizaci√≥n con Optuna:\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n‚úÖ Bloque 4: Optimizaci√≥n Optuna completada.\")"
      ],
      "metadata": {
        "id": "w3JXu776Z0DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 4: Crear y Entrenar el Modelo Final con los Mejores Hiperpar√°metros ---\n",
        "# Este bloque asume que 'study_lstm_cnn_optuna' est√° definido y que se encontr√≥ un best_trial.\n",
        "\n",
        "best_params = study_lstm_cnn_optuna.best_trial.params\n",
        "\n",
        "# Reconstruir la configuraci√≥n de las capas CNN\n",
        "conv_layer_configs_final = []\n",
        "n_conv_layers_final = best_params.get('n_conv_layers', 1)\n",
        "for i in range(n_conv_layers_final):\n",
        "    conv_layer_configs_final.append({\n",
        "        'filters': best_params[f'filters_l{i}'],\n",
        "        'kernel_size': best_params[f'kernel_size_l{i}'],\n",
        "        'dropout_conv': best_params[f'dropout_conv_l{i}']\n",
        "    })\n",
        "\n",
        "# Reconstruir la configuraci√≥n de las capas LSTM\n",
        "lstm_layer_configs_final = []\n",
        "n_lstm_layers_final = best_params.get('n_lstm_layers', 1)\n",
        "for i in range(n_lstm_layers_final):\n",
        "    lstm_layer_configs_final.append({\n",
        "        'units': best_params[f'lstm_units_l{i}'],\n",
        "        'dropout': best_params[f'dropout_lstm_l{i}']\n",
        "    })\n",
        "\n",
        "# Reconstruir la configuraci√≥n de las capas Densas\n",
        "dense_layer_configs_final = []\n",
        "n_dense_layers_final = best_params.get('n_dense_layers', 0)\n",
        "if n_dense_layers_final > 0:\n",
        "    for i in range(n_dense_layers_final):\n",
        "        dense_layer_configs_final.append({\n",
        "            'units': best_params[f'dense_units_l{i}'],\n",
        "            'dropout': best_params[f'dropout_dense_l{i}']\n",
        "        })\n",
        "\n",
        "# Crear la instancia del modelo LSTM-CNN final\n",
        "final_model_lstm_cnn = Sequential()\n",
        "final_model_lstm_cnn.add(Input(shape=(X_train_lstm_cnn.shape[1], X_train_lstm_cnn.shape[2]))) # Input shape para LSTM-CNN\n",
        "\n",
        "# Capa Masking para ceros de padding\n",
        "final_model_lstm_cnn.add(Masking(mask_value=0.0, input_shape=(X_train_lstm_cnn.shape[1], X_train_lstm_cnn.shape[2])))\n",
        "\n",
        "# Parte CNN\n",
        "for i, layer_config in enumerate(conv_layer_configs_final):\n",
        "    final_model_lstm_cnn.add(Conv1D(filters=layer_config['filters'],\n",
        "                                    kernel_size=layer_config['kernel_size'],\n",
        "                                    activation=best_params['activation'],\n",
        "                                    padding='same'))\n",
        "    if layer_config['dropout_conv'] > 0:\n",
        "        final_model_lstm_cnn.add(Dropout(layer_config['dropout_conv']))\n",
        "    final_model_lstm_cnn.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Parte LSTM\n",
        "for i, layer_config in enumerate(lstm_layer_configs_final):\n",
        "    final_model_lstm_cnn.add(LSTM(layer_config['units'], return_sequences=(i < n_lstm_layers_final - 1)))\n",
        "    if layer_config['dropout'] > 0:\n",
        "        final_model_lstm_cnn.add(Dropout(layer_config['dropout']))\n",
        "\n",
        "# Capas densas post-recurrente\n",
        "if n_dense_layers_final > 0:\n",
        "    for i, layer_config in enumerate(dense_layer_configs_final):\n",
        "        final_model_lstm_cnn.add(Dense(layer_config['units'], activation=best_params['activation']))\n",
        "        if layer_config['dropout'] > 0:\n",
        "            final_model_lstm_cnn.add(Dropout(layer_config['dropout']))\n",
        "else: # Si n_dense_layers fue 0, a√±adir una capa densa por defecto\n",
        "    final_model_lstm_cnn.add(Dense(32, activation=best_params['activation']))\n",
        "    final_model_lstm_cnn.add(Dropout(0.2))\n",
        "\n",
        "final_model_lstm_cnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer_cls_final = getattr(tf.keras.optimizers, best_params['optimizer_name'])\n",
        "optimizer_final = optimizer_cls_final(learning_rate=best_params['lr'])\n",
        "\n",
        "final_model_lstm_cnn.compile(loss='binary_crossentropy',\n",
        "                    optimizer=optimizer_final,\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento completo (X_train_lstm_cnn)\n",
        "smote_final_k = best_params.get('smote_k', 5)\n",
        "smote_final = SMOTE(k_neighbors=smote_final_k, random_state=42)\n",
        "\n",
        "# SMOTE necesita un array 2D. Aplanamos X_train_lstm_cnn.\n",
        "original_shape_X_train_lstm_cnn = X_train_lstm_cnn.shape\n",
        "X_train_lstm_cnn_flattened = X_train_lstm_cnn.reshape(original_shape_X_train_lstm_cnn[0], -1)\n",
        "\n",
        "X_train_resampled_flat, y_train_resampled = smote_final.fit_resample(X_train_lstm_cnn_flattened, y_train_lstm_cnn.astype(int))\n",
        "\n",
        "# Volver a dar forma 3D a los datos remuestreados\n",
        "X_train_resampled = X_train_resampled_flat.reshape(y_train_resampled.shape[0], original_shape_X_train_lstm_cnn[1], original_shape_X_train_lstm_cnn[2])\n",
        "\n",
        "# Configurar Early Stopping para el entrenamiento final\n",
        "early_stopping_final_cb = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=best_params['early_stopping_patience'],\n",
        "    mode='min',\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Dividir para la validaci√≥n interna del modelo final\n",
        "X_train_fit, X_val_fit, y_train_fit, y_val_fit = train_test_split(\n",
        "    X_train_resampled, y_train_resampled, test_size=0.2, random_state=42, stratify=y_train_resampled\n",
        ")\n",
        "\n",
        "# Instanciar el F1ScoreCallback para el entrenamiento final\n",
        "f1_callback_instance_lstm_cnn = F1ScoreCallback(X_val=X_val_fit, y_val=y_val_fit, pos_label=0)\n",
        "\n",
        "print(f\"\\n Entrenando el modelo final LSTM-CNN con los mejores hiperpar√°metros (optimizados para F1-Score): {best_params}\")\n",
        "# Entrenar el modelo final y CAPTURAR EL HISTORIAL\n",
        "history_lstm_cnn = final_model_lstm_cnn.fit(\n",
        "    X_train_fit,\n",
        "    y_train_fit,\n",
        "    epochs=best_params['epochs_max'],\n",
        "    batch_size=best_params['batch_size'],\n",
        "    validation_data=(X_val_fit, y_val_fit),\n",
        "    callbacks=[early_stopping_final_cb, f1_callback_instance_lstm_cnn],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Bloque 5: Modelo LSTM-CNN final entrenado y historial capturado.\")"
      ],
      "metadata": {
        "id": "JIsw5Qlpan5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 5: Evaluaci√≥n en el conjunto de prueba ---\n",
        "# Este bloque asume que 'final_model_lstm_cnn' est√° definido y entrenado del Bloque 5.\n",
        "\n",
        "y_pred_probs_lstm_cnn = final_model_lstm_cnn.predict(X_test_lstm_cnn, verbose=0)\n",
        "y_pred_probs_flat_lstm_cnn = y_pred_probs_lstm_cnn.flatten()\n",
        "\n",
        "print(\"\\nBloque 6: Evaluaci√≥n del Modelo LSTM-CNN Final con distintos umbrales en el conjunto de prueba:\\n\")\n",
        "for threshold in np.arange(0.50, 0.81, 0.02):\n",
        "    y_pred_thresholded_lstm_cnn = (y_pred_probs_flat_lstm_cnn >= threshold).astype(int)\n",
        "\n",
        "    print(f\" Umbral = {threshold:.2f}\")\n",
        "    print(classification_report(y_test_lstm_cnn, y_pred_thresholded_lstm_cnn, target_names=[\"Reprobar (0)\", \"Aprobar (1)\"], zero_division=0))\n",
        "    print(\" Matriz de Confusi√≥n:\")\n",
        "    print(confusion_matrix(y_test_lstm_cnn, y_pred_thresholded_lstm_cnn, labels=[0, 1]))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# M√©tricas Globales (independientes del umbral, eval√∫an la capacidad de ranking)\n",
        "print(\"\\n--- M√©tricas Globales del Modelo (independientes del umbral) ---\")\n",
        "auc_lstm_cnn = roc_auc_score(y_test_lstm_cnn, y_pred_probs_flat_lstm_cnn)\n",
        "print(f\" ROC AUC: {auc_lstm_cnn:.4f}\")\n",
        "print(\"Bloque 6: Evaluaci√≥n en el conjunto de prueba completada.\")"
      ],
      "metadata": {
        "id": "C7ykeoj6qWT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 6: Graficar el Historial de Entrenamiento para Diagn√≥stico de Overfitting/Underfitting ---\n",
        "# Este bloque asume que 'history_lstm_cnn' y 'f1_callback_instance_lstm_cnn' est√°n definidos del Bloque 5.\n",
        "\n",
        "print(\"\\nüìà Bloque 7: Generando gr√°ficos del historial de entrenamiento para diagnosticar Overfitting/Underfitting...\")\n",
        "\n",
        "# Define la ruta para guardar los gr√°ficos\n",
        "ruta_carpeta_graficos_lstm_cnn = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano - Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/Modelos/LSTM_CNN/graficos/'\n",
        "os.makedirs(ruta_carpeta_graficos_lstm_cnn, exist_ok=True)\n",
        "\n",
        "# Funci√≥n para graficar las curvas de aprendizaje (adaptada para F1-Score)\n",
        "def plot_learning_curves(history_obj, f1_callback_obj, best_params_for_title=None, save_path=None):\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    if best_params_for_title:\n",
        "        title_str = \"Curvas de Aprendizaje LSTM-CNN (F1-Score Clase 0)\\n\"\n",
        "        title_str += f\"LR: {best_params_for_title.get('lr'):.1e}, Opt: {best_params_for_title.get('optimizer_name')}, \"\n",
        "        title_str += f\"Conv_L: {best_params_for_title.get('n_conv_layers')}, LSTM_L: {best_params_for_title.get('n_lstm_layers')}, SMOTE_k: {best_params_for_title.get('smote_k')}\"\n",
        "        fig.suptitle(title_str, fontsize=18, y=1.03)\n",
        "\n",
        "    # Gr√°fico de P√©rdida (Loss)\n",
        "    ax1.plot(history_obj.history['loss'], label='P√©rdida de Entrenamiento', color='dodgerblue')\n",
        "    if 'val_loss' in history_obj.history:\n",
        "        ax1.plot(history_obj.history['val_loss'], label='P√©rdida de Validaci√≥n', color='darkorange', linestyle='--')\n",
        "    ax1.set_title('Curva de P√©rdida del Modelo', fontsize=16)\n",
        "    ax1.set_xlabel('√âpoca'); ax1.set_ylabel('Loss'); ax1.legend()\n",
        "\n",
        "    # Gr√°fico de F1-Score (desde el callback personalizado)\n",
        "    ax2.plot(f1_callback_obj.train_f1s, label='F1-Score Entrenamiento (Clase 0 - Placeholder)', color='lightgray', linestyle=':')\n",
        "    ax2.plot(f1_callback_obj.val_f1s, label='F1-Score Validaci√≥n (Clase 0)', color='darkorange', linestyle='--')\n",
        "    ax2.set_title('Curva de F1-Score del Modelo (Clase 0)', fontsize=16)\n",
        "    ax2.set_xlabel('√âpoca'); ax2.set_ylabel('F1-Score'); ax2.legend()\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]);\n",
        "    plt.show()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close(fig)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Llamada a la funci√≥n de graficaci√≥n\n",
        "plot_learning_curves(history_lstm_cnn, f1_callback_instance_lstm_cnn, best_params_for_title=best_params, save_path=os.path.join(ruta_carpeta_graficos_lstm_cnn, 'lstm_cnn_f1_learning_curves.png'))\n",
        "\n",
        "print(\"Bloque 7: Gr√°ficos generados y guardados en Google Drive.\")"
      ],
      "metadata": {
        "id": "JjbYFBW1qbQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASO 7: Guardar el Modelo Final ---\n",
        "# Este bloque asume que 'final_model_lstm_cnn' est√° definido y entrenado del Bloque 5.\n",
        "\n",
        "ruta_carpeta_modelo_lstm_cnn = '/content/drive/MyDrive/Seminario de Titulo - Prof Roberto Mu√±oz/Maximiliano - Generaci√≥n y An√°lisis de Secuencias de Actividad/logs/Modelos/LSTM_CNN/'\n",
        "os.makedirs(ruta_carpeta_modelo_lstm_cnn, exist_ok=True)\n",
        "\n",
        "nombre_archivo_modelo_lstm_cnn = 'modelo_lstm_cnn_f1_optuna_final.keras' # Nombre del modelo LSTM-CNN\n",
        "ruta_modelo_completa_lstm_cnn = os.path.join(ruta_carpeta_modelo_lstm_cnn, nombre_archivo_modelo_lstm_cnn)\n",
        "\n",
        "final_model_lstm_cnn.save(ruta_modelo_completa_lstm_cnn)\n",
        "print(f\" Bloque 8: Modelo LSTM-CNN entrenado y guardado en: {ruta_modelo_completa_lstm_cnn}\")"
      ],
      "metadata": {
        "id": "AUxNx7O-wpZV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}